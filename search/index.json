[{"content":"Welcome—— 欢迎来到我的技术\u0026amp;生活博客！本站点建于2024/11/23，本篇文章写于2024/11/24，最后一次更新于2025/7/15。\n想写的东西有很多，但到真正要落笔时却有些踌躇不前。\n关于Kambri 我是一名就读于国内计算机科学与技术专业的大三本科生，在本科的前两年半学习并研究高性能计算（HPC），并在大三下开始探索高效人工智能（Efficient AI）和机器学习系统（MLSys），如推理优化等，希望能在推研后攻读相关领域的硕士学位。\n爱好旅行，希望能在学生时代结束前环游中国；音乐，喜欢梶浦由记（和她的乐队，Kalafina和Fiction Junction）、kokia、aimer、reol的歌曲；运动，一周会有几次健身和羽毛球。\n关于站点 该站点由Hugo构建，使用Stack主题并进行修改，并托管于GitHub Pages。\n建设站点计划TODOLIST：\n配置CDN并绑定域名（可以通过www.kambri.top访问，但未配置SSL证书）\n添加评论系统（使用插件utterances搭建）\n配置搜索引擎站点地图（目前已经能被bing、google搜索到）\nLast but not least，更多contributory文章\n博文更新计划TODOLIST：\n旅行回忆录，以往的旅行经历就靠零星的回忆和些许照片还原罢，这或许也不错 如果你喜欢我的博客，欢迎来到站点仓库点一个star，谢谢！\n寄语 从我上踏入计算机科学与技术这个专业开始，便一直想拥有一个技术博客。但由于种种原因，比较流行的博客平台如CSDN、博客园、知乎都没有让我特别感兴趣，我也够懒，时至今日才动手搭建属于自己的博客站点。所幸大多精彩的技术知识没有因此错过，它们大多被我记录于OneNote或是Obsidian中，在未来我也会视情况将这些留存的技术文章或是学习笔记迁移到这个站点。\n随着大学生活的推移，我有了很多足够精彩的经历，但很少在社交媒体发文，因此也愈发希望能够在某个地方记录这些转瞬即逝的想法。在起初的构想中，技术博文和生活随笔放在不同的站点或许会更有利于读者，但想来还是过于折腾，就索性放在一起罢。\n总之，在这个大雪纷飞的夜晚，Kambri\u0026rsquo;s Blog正式诞生了！\n——Kambri 2024.11.24\n","date":"2025-07-15T23:43:00+08:00","image":"https://kaigezheng.github.io/p/hello-world/img/cover_hu_211c3ad2b5e596a3.jpg","permalink":"https://kaigezheng.github.io/p/hello-world/","title":"Hello World"},{"content":"计划在这篇博客里调研并粗略地学习一下到目前为止比较有影响力的AI Infra工作（类似Survey），并慢慢补充丰富。Anyway，迈出行动的第一步最难。\nModel Parameters Parameter Estimation 1B = 1 Billion = 十亿\n假设模型层数为$N$，隐藏层维度为$H$，接下来考虑一层Transformer层的参数量估算：\n自注意力层：（不需要考虑MHA的情况，因为多头concat起来的为度就等于隐藏层维度）需要注意的是这里包括一次注意力计算和一次线性映射，因此涉及四个可训练参数$W_Q$、$W_K$、$W_V$和$W_O$，因此注意力层的可训练参数量为$4H^2 + 4H$。 $$Attention(Q, K, V)=softmax(\\frac{QK^{T}}{\\sqrt[]{d_{k}} })V \\newline Q=W_{Q}X+b_{Q}, K=W_{K}X+b_{K}, V=W_{V}X+b_{V}, O = W_{O}Attention_{O}+b_{O}$$ 前馈网络层：FFN层包括一次线性升维和一次线性降维，设计两个可训练参数$W_{1}$和$W_{2}$，可训练参数量为$(H\\times 4H + 4H) + (4H \\times H + H) = 8H^{2} + 5H$。 $$FFN(x) = GeLU(xW_{1}+b_{1})W_{2}+b_{2}$$ 残差连接和层归一化：Add \u0026amp; Norm层（主要是LN层）涉及两个可训练向量参数$\\alpha$和$b$，即2H。 $$Y = Attention(X) + X \\newline LN(Y) = \\alpha \\frac{Y - \\mu}{\\sigma} + b$$综上，一层Transformer层由一层Attention层、一层FFN层和两层Add \u0026amp; Norm层组成，可训练参数量为$12H^{2} + 13H$，可以近似为$12H^{2}$。\nComputation Estimation AxB和BxC的矩阵相乘，每个输出元素需要进行$n$次乘法和$n-1$次加法，$\\approx 2n FLOPs$；整个矩阵共有AxC个输出元素，因此总$FLOPs \\approx 2ABC$，可以近似为$ABC$。因此估算参数时，主要关注矩阵乘或向量矩阵乘的维度即可。注意$W_{Q/K/V}$的维度是$HxH$，而$Q/K/V$的维度是$LxH$。\n接下来估算计算量，由于LayerNorm、Dropout等计算量较小，暂时不考虑。设模型层数为$N$，隐藏层维度为$H$，批量大小为$B$，序列长度为$L$：\n自注意力层 （LHxHH=LH）线性投影QKV：每个投影是$H\\times H$，应用于每个token就是$BLH^{2}$，总共3个矩阵，因此$FLOPs=3BLH^{2}$\n（LHxHL=LL）Attention Score ($QK^{T}$)：每个token对应一个$L\\times L$的注意力矩阵，需要做$H$次乘加，约为$BHL^{2}$\n（——————）Softmax和Scaling：Softmax涉及取指、求和、逐元素除和、数值稳定的计算，这里只能估计为$xBL^{2}$，相比SDPA可忽略不计\n（LLxLH=LH）Attention Output与V相乘：显然是$BHL^{2}$\n（LHxHH=LH）输出线性层$W_{O}$：显然是$BLH^{2}$\n因此，自注意力层的总FLOPs：\n$$\\approx (3BLH^{2})+(2BHL^{2})+(BLH^{2})=4BLH^{2}+2BHL^{2}$$ 前馈网络层： 升维（$H$-\u0026gt;$4H$）：$FLOPs=BLH(4H)$\n降维（$4H$-\u0026gt;$H$）：$FLOPs=BL(4H)H$\n（当然还有GeLU激活，不过是线性的计算量，可以忽略不计）因此，FFN层的总Flops：\n$$\\approx 8BLH^{2}$$综上，$Total\\ FLOPs \\approx 12BLH^{2} + 2BHL^{2}$$。（理论上应该再乘以2）\nMemory Estimation Inference Optimization KV Cache OPtimization KV Cache KV (Key-Value) Cache是一种在自回归模型（如Decoder of Transformer）中常用的推理加速技术，通过在推理的注意力机制计算过程中缓存已计算过的$Key$和$Value$，减少重复的$K$、$V$与权重矩阵的projection计算。\n$$Attention(Q, K, V)=softmax(\\frac{QK^{T}}{\\sqrt[]{d_{k}} })V$$为什么可以缓存$K$和$V$？由于Casual Mask机制，当模型推理时当前token不需要与之后的token进行Attention计算，因此在计算第$t$个token的$Attention_{t}$时，只需要$Q_{0:t}$、$K_{0:t}$和$V_{0:t}$。而Decoder中的$Q$需要token在embedding后通过$W_q$投影，但$K_{0:t-1}$与$V_{0:t-1}$来自Encoder中，且在计算$Attention_{0:t-1}$时已被计算过，因此可以通过缓存已被计算过的历史$K$与$V$来节省这部分计算。\n接下来参考知乎@看图学的公式推导，\n计算第一个token时的Attention：\n$$ Attention(Q, K, V) = softmax(\\frac{Q_{1}K_{1}^{T}}{\\sqrt[]{d}})V_{1} $$计算第二个token时的Attention（矩阵第二行对应$Attention_{2}$），$softmax(\\frac{Q_{1}K_{2}}{\\sqrt d})$项被mask掉了：\n$$ Attention(Q, K, V) = softmax(\\frac{Q_{2}[K_{1}, K_{2}]^{T}}{\\sqrt[]{d}})[V_{1}, V_{2}] \\newline = \\begin{pmatrix} softmax(\\frac{Q_{1}K_{1}^{T}}{\\sqrt d}) \u0026 softmax(-\\infty )\\\\ softmax(\\frac{Q_{2}K_{1}^{T}}{\\sqrt d}) \u0026 softmax(\\frac{Q_{2}K_{2}^{T}}{\\sqrt d}) \\end{pmatrix}[V_{1}, V_{2}] \\newline =\\begin{pmatrix} softmax(\\frac{Q_{1}K_{1}^{T}}{\\sqrt d})V_{1} + 0 \\times V_{2} \\\\ softmax(\\frac{Q_{2}K_{1}^{T}}{\\sqrt d})V_{1} + softmax(\\frac{Q_{2}K_{2}^{T}}{\\sqrt d})V_{2} \\end{pmatrix} $$以此类推，Attention矩阵的严格上三角部分都被mask掉了，因此计算第$t$个token的$Attention_{t}$时与$Q_{1:t-1}$无关：\n$$ Attention_{1} = softmax(\\frac{Q_{1}K_{1}^{T}}{\\sqrt[]{d}})V_{1} \\newline Attention_{2} = softmax(\\frac{Q_{1}[K_{1}, K_{2}]^{T}}{\\sqrt[]{d}})[V_{1}, V_{2}] \\newline ... \\newline Attention_{t} = softmax(\\frac{Q_{t}K_{1:t}^{T}}{\\sqrt[]{d}})V_{1:t} $$源码实现参考Huggingface的GPT2推理实现，KV Cache的逻辑核心思路如下：\n对于Cross Attention，$Q$来自decoder的当前token，$KV$来自encoder的全部输出。因此$KV$通常不变，只需生成一次并缓存。\n对于Self Attention，$QKV$都来自decoder的当前token，因为decoder需要看过去所有的token，因此前面token的$KV$都需要缓存\n看源码好难——\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def forward( self, hidden_states: Optional[tuple[torch.FloatTensor]], past_key_value: Optional[Cache] = None, cache_position: Optional[torch.LongTensor] = None, attention_mask: Optional[torch.FloatTensor] = None, head_mask: Optional[torch.FloatTensor] = None, encoder_hidden_states: Optional[torch.Tensor] = None, encoder_attention_mask: Optional[torch.FloatTensor] = None, output_attentions: Optional[bool] = False, **kwargs, ) -\u0026gt; tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]: # 判断是否是Cross Attention is_cross_attention = encoder_hidden_states is not None # Cross Attention使用cross_attention_cache # Self Attention使用self_attention_cache # 用is_updated表示当前层的KV是否已缓存 (用于Cross Attention) if past_key_value is not None: if isinstance(past_key_value, EncoderDecoderCache): is_updated = past_key_value.is_updated.get(self.layer_idx) if is_cross_attention: curr_past_key_value = past_key_value.cross_attention_cache else: curr_past_key_value = past_key_value.self_attention_cache else: curr_past_key_value = past_key_value if is_cross_attention: # Cross Attention query_states = self.q_attn(hidden_states) attention_mask = encoder_attention_mask # 尝试获取KV Cache if past_key_value is not None and is_updated: key_states = curr_past_key_value.layers[self.layer_idx].keys value_states = curr_past_key_value.layers[self.layer_idx].values else: key_states, value_states = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2) # 变换成MHA的shape shape_kv = (*key_states.shape[:-1], -1, self.head_dim) key_states = key_states.view(shape_kv).transpose(1, 2) value_states = value_states.view(shape_kv).transpose(1, 2) else: # Self Attention query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) shape_kv = (*key_states.shape[:-1], -1, self.head_dim) key_states = key_states.view(shape_kv).transpose(1, 2) value_states = value_states.view(shape_kv).transpose(1, 2) shape_q = (*query_states.shape[:-1], -1, self.head_dim) query_states = query_states.view(shape_q).transpose(1, 2) # 更新缓存: 启动KV Cache，且是Self Attention，或Cross Attention没有缓存过的情况 if (past_key_value is not None and not is_cross_attention) or ( past_key_value is not None and is_cross_attention and not is_updated ): cache_position = cache_position if not is_cross_attention else None key_states, value_states = curr_past_key_value.update( key_states, value_states, self.layer_idx, {\u0026#34;cache_position\u0026#34;: cache_position} ) if is_cross_attention: past_key_value.is_updated[self.layer_idx] = True # 判断是否是因果注意力 (Casual) is_causal = attention_mask is None and query_states.shape[-2] \u0026gt; 1 and not is_cross_attention # 选择注意力实现方式 # [eager/flash_attention_2/sdpa/triton/xformers] using_eager = self.config._attn_implementation == \u0026#34;eager\u0026#34; attention_interface: Callable = eager_attention_forward if self.config._attn_implementation != \u0026#34;eager\u0026#34;: attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation] # 选择精度提升(upcast)和重排(reorder) if using_eager and self.reorder_and_upcast_attn: attn_output, attn_weights = self._upcast_and_reordered_attn( query_states, key_states, value_states, attention_mask, head_mask ) else: # 调用注意力计算函数 attn_output, attn_weights = attention_interface( self, query_states, key_states, value_states, attention_mask, head_mask=head_mask, dropout=self.attn_dropout.p if self.training else 0.0, is_causal=is_causal, **kwargs, ) # 将Attention结果用线性层c_proj投影回原始维度 attn_output = attn_output.reshape(*attn_output.shape[:-2], -1).contiguous() attn_output = self.c_proj(attn_output) attn_output = self.resid_dropout(attn_output) return attn_output, attn_weights 同时，KV Cache在减少重复$KV$计算的同时会引入大量的Memory开销，可以粗略计算一下KV Cache的显存占用：\n$$ Memory = 2 \\times batch\\_size \\times seq\\_len \\times num\\_layers \\times num\\_heads \\times head\\_dims \\times dtype\\_size $$MQA Multi-Query Attention (MQA)是Google在2019年于《Fast Transformer Decoding: One Write-Head is All You Need》提出的一种高效注意力机制，旨在减少推理过程中的计算和内存开销。与传统MHA不同，MQA保留多个Query头，但所有注意力头共享同一组Key和Value，这种结构显著减少了KV Cache的Memory开销，同时保持了MHA相近的性能表现。\n下面是基于PyTorch的一个简单实现（还没实现Casual Mask）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 import torch import torch.nn as nn import torch.nn.functional as F class MultiQueryAttention(nn.Module): def __init__(self, embed_dim, num_heads): super().__init__() self.embed_dim = embed_dim self.num_heads = num_heads self.head_dim = embed_dim // num_heads # 多个Query头 self.q_proj = nn.Linear(embed_dim, embed_dim) # 共享的Key和Value self.k_proj = nn.Linear(embed_dim, self.head_dim) self.v_proj = nn.Linear(embed_dim, self.head_dim) self.out_proj = nn.Linear(embed_dim, embed_dim) def forward(self, x, mask=None): B, T, C = x.size() # Q: (B, T, num_heads, head_dim) q = self.q_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2) # (B, num_heads, T, head_dim) # K, V: shared (B, T, head_dim)-\u0026gt;(B, 1, T, head_dim) k = self.k_proj(x).unsqueeze(1) v = self.v_proj(x).unsqueeze(1) # Attention Score: (B, num_heads, T, T) attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5) if mask is not None: attn_scores = attn_scores.masked_fill(mask == 0, float(\u0026#39;-inf\u0026#39;)) attn_weights = F.softmax(attn_scores, dim=-1) # Attention output: (B, num_heads, T, head_dim) attn_output = torch.matmul(attn_weights, v) # 合并头-\u0026gt;(B, T, embed_dim) attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C) return self.out_proj(attn_output) GQA Grouped-Query Attention (GQA)Google与在023年于GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints提出的一种介于MHA和MQA之间的注意力机制，让多个Query头共享同一组Key和Value，旨在保留部分表达能力的同时大幅减少计算和内存开销。\nOverview of grouped-query method 源码上，只在Huggingface的仓库里找到了sdpa_attention_paged_forward的实现，看上去挺GQA的。\n核心思路是：\n先用repeat_kv将KV head复制num_attention_heads // num_key_value_heads次（从(B, num_key_value_heads, L, D)到(B, num_attention_heads, L, D)）\n支持KV Cache的SDPA\nPreliminaries (FlashAttention) FlashAttention由Tri Dao等在2022年于《FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness》提出，并在2023年于《FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning 》提出v2版本，2024年于《FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision》提出v3版本。\nOnline Softmax Naive Softmax涉及两次read（遍历求sum和逐元素除sum(exp)）和一次write（结果写回），数学公式如下：\n$$softmax(x_{i}) = \\frac{e^{x_{i}}}{\\Sigma _{j=1}^{n}e^{x_{j}}}$$如果$x$太大，$e^x$会上溢，而safe softmax解决了这一问题。\nSafe Softmax涉及三次read（还需要遍历一次减去max）和一次write，目的是为了避免数值溢出，数学公式如下：\n$$softmax(x_{i}) = \\frac{e^{x_{i} - max(x)}}{\\Sigma _{j=1}^{n}e^{x_{j} - max(x)}}$$但是需要多次遍历数据，性能较差，而online softmax解决了这一问题。\nOnline Softmax只需要两次read（遍历一次x并维护最大值和归一化因子）和一次write，核心思路如下：\n在线维护变量（$m_t$，当前前$t$个元素的最大值；$d_t$，当前前$t$个元素的归一化因子）\n初始化\n$m_0=-\\infty, d_0=0$\n遍历并维护变量\n更新最大值：\n$m_t=max(m_{t-1}, x_t)$\n更新归一化因子（递推）【难点】：\n$d_t=d_{t-1}\\cdot e^{m_{t-1}-m_t}+e^{x_t - m_t}(=\\Sigma_{j=1}^{t-1}e^{x_j-m_{t-1}}\\cdot e^{m_{t-1}-m_t}+e^{x_t - m_t})$\n这里的公式推导非常巧妙，应用了同底指数相乘等于两个指数幂相加，论文的推导如下，$d_{t}$代表前$t$个数与最大值（局部，即$m_t$）之差的指数和：\n$$d_t = d_{t-1}\\times e^{m_{t-1}-m_t} + e^{x_t-m_t} \\newline =(\\Sigma_{j=1}^{t-1}e^{x_j-m_{t-1}}) \\times e^{m_{t-1}-m_t} + e^{x_t-m_t} \\newline = \\Sigma_{j=1}^{t-1}e^{x_j-m_t}+e^{x_t-m_t} \\newline = \\Sigma_{j=1}^{t}e^{x_j-m_t}$$ 可以这么理解：每次更新归一化因子时，都乘以了$e^{m_{t-1} - m_{t}}$，那么最后这个因子会是$e^{0-m_{global}}$，正是分母$e^{x_{t}-m_{global}}$的一部分，如此巧妙地将全局最大值保留到了遍历结束，而且在递推中的每一步都纠正了之前的局部最大值\nonline softmax的伪代码如下，实现上还是比较简单的：\nPseudocode of Online Softmax 参考@TaurusMoon的实现写了C++的online softmax kernel：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 using namespace std; template\u0026lt;typename T\u0026gt; void OnlineSoftmax(T* dst, const T* src, int n) { T m = -numeric_limits\u0026lt;T\u0026gt;::infinity(); T d = 0.0f; for (int i = 0; i \u0026lt; n; +i) { T m_update = max(m, src[i]); d = d * exp(m - m_update) + exp(src[i] - m_update); m = m_update; } for (int i = 0; i \u0026lt; n; ++i) { dst[i] = exp(src[i] - m) / d; } } GPU Memory Architecture 显存/高带宽内存（HBM, High Bandwidth Memory）是封装在GPU Core外的DRAM（动态存储，需要周期性刷新），通过超宽总线连接GPU Core，大容量的同时延迟也相对较大。\n静态内存（SRAM, Static Random Access Memory）**是封装在GPU Core内部的SRAM（静态存储），如Register、Shared Memory、L1/L2 Cache。\nMemory/Bandwidth Architcture of A100 Operator 算子主要可以分为两类：\n计算受限型：如GEMM等\n内存受限型：主要是element-wise类（如Activation、Dropout、Maskibg等）和reduction类（如Sum、Softmax、LayerNorm等）\nFlashAttention FlashAttention-v1 Transformer的核心计算是Attention，朴素的Attention计算步骤如下，其中一般$N \\gg d$，复杂度是$N^2$，在长序列频繁读写（5read \u0026amp; 3 write）大矩阵时非常依赖HBM：\nStandard Attention Implementation FlashAttention的核心思路就是提高Attention算子的SRAM利用率（将输入的QKV矩阵从HBM加载到SRAM中计算），减少HBM访存。\nTiling 常规的row-wise softmax不适合分块的算法，因此这里需要使用online softmax，在分块后的范围内，片上计算max和rowsum，并在通信后计算全局的max和scale factor。\nRecomputation 在反向传播的优化，计算梯度需要用到QK计算的attention score ($S$)和softmax后的attention score ($P$)。FlashAttention通过存储Attention的输出结果($O$)和归一化统计量$(m, l)$来快速计算$S$和$P$，避免了用$QKV$的重复计算。\nKernel Fusion 很常见的优化，减少了多余的HBM写回和重新加载。\nPyTorch vs. FlashAttention on GPT-2 总结一下，FlashAttention可以让计算提速2-4倍，节约10倍以上内存（主要是边存变算，不用存储复杂度为$N^2$的$QKV$，转而存储复杂度为$Nd$的输出结果和统计量）。\nTraining Optimization Parallel Computting (on Data) DP 数据并行（DP, Data Parallel）：模型副本在每个GPU上各自独立地前向传播，梯度会聚合（AllReduce）到主GPU进行参数更新。缺点是非跨进程，只支持单机多卡；梯度聚合会发生在主设备，导致通信瓶颈和负载不均衡。\n实现上较为简单:\n1 2 model = ...() model = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3]) 不过PyTorch建议多卡并行的时候使用DPP，即使只有一个节点（DP的性能较DDP更差，因为主卡负载很不均衡，单进程多线程环境下设计GIL竞争，且可扩展性不如DDP），源码实现见这里，更加底层的Operator在torch.nn.parallel.scatter_gather/_functions/comm下（如scatter、gather等）。\nDDP 分布式数据并行（DDP, Distributed Data Parallel）：每个进程对应一个GPU，每个GPU上都有模型副本，梯度通过AllReduce同步，每个金层都参与参数更新（每个GPU独立进行前向、计算loss、计算梯度，并在AllReduce后通过平均梯度更新）。\n实现上可以通过手动设置并行（多个terminal设置RANK、WORLD_SIZE、MASTER_ADDR、MASTER_PORT等环境变量并启动脚本）或用torchrun自动管理环境变量（torchrun --nproc_per_node=... \u0026lt;script\u0026gt;）：\n1 2 3 4 5 6 7 8 9 10 11 12 import os import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP os.environ[\u0026#39;MASTER_ADDR\u0026#39;] = \u0026#39;localhost\u0026#39; os.environ[\u0026#39;MASTER_PORT\u0026#39;] = \u0026#39;...\u0026#39; dist.init_process_group(backend=\u0026#39;nccl\u0026#39;, rank=rank, world_size=world_size) model = ...().to(rank) model = DDP(model, device_ids=[rank]) ... dist.destroy_process_group() 源码参考这里。DDP可以使用高效的通信后端（如NCCL），没有主从瓶颈（支持单机多卡/多机多卡），还是非常实用的。\nFSDP 全分片数据并行（FSDP, Fully Sharded Data Parallel）：模型权重按参数维度切分到多个GPU上（shard），前向传播时重新聚合参数（gather），反向传播后再切分（reshard），大幅减少显存占用，主要通过torch.nn.distributed.fsdp.FullyShardedDataParallel来实现，源码参考这里。\n本质上FSDP还是数据并行，知识参数分别有点模型并行的味道\nParallel Computting (on Model) Existing parallelism for distributed training (sorry我没找到图片来源) TP 张量并行（TP, Tensor Parallel）：是一种层内并行（Intra-Layer Parallelism）策略，将模型中的一个层（如MLP层、Attention层）的内部计算划分到多个设备上，多个设备共同完成该层前向和反向传播。这么做可以突破显存的限制，但是会对延迟较敏感。\nColumn-wise Parallelism (切列，维度完整) $$W = [W_{1}, W_{2}] \\newline Y_{i}=XW_{i}^{T}\\ (on\\ each\\ GPU) \\newline Y = Y_{1} + Y_{2}\\ (AllReduce)$$ Row-wise Parallelism (切行) $$W=\\begin{bmatrix}W_{1} \\\\W_{2}\\end{bmatrix} \\newline Y_{i}=XW_{i}^{T}\\ (on\\ each \\ GPU) \\newline Y=concat(Y_{1}, Y_{2})\\ (AllGather)$$PP 流水线并行（PP, Pipeline Parallel）：是一种层间并行（Inter-Layer Parallelism）策略，将模型按顺序划分为多个stage，不同GPU执行不同的stage，多个micro-batch以流水线方式通过模型。\n支持极大模型（层数多），显存需求分布在各stage，且跨GPU通信压力小；但是存在pipeline bubble（起始阶段GPU空闲，影响吞吐）\nQuantization Precision Formats TF32 strikes a balance that delivers performance with range and accuracy IEEE 754标准中浮点数由三部分组成：S符号位、E指数位、M尾数位，接下来介绍各种精度格式：\nFP32 标准的IEEE 754单精度浮点格式，1位符号位+8位指数位+23位位数（下文用[S, E, M]来表示），精度较高，适用于所有主流硬件（CPU、GPU、TPU等）\nTP32 NVIDIA在Ampere架构引入的混合格式，[1, 8, 10]，截断了尾数位（减少乘加复杂度），支持Tensor Core优化，精度介于FP32和FP16之间，常在训练时作为FP32替换\nFP16 16-bit半精度浮点数，[1, 5, 10]\nBF16 Google TPU推出的Brain Float 16，[1, 8, 7]，常用于混合精度训练\nFP8 [1, 4, 3]或[1, 5, 2]，需要Hopper架构GPU支持\nINT8 8-bit整型\nFP4 [1, 2, 1]\nReference 大模型推理加速：看图学KV Cache\nLM(20)：漫谈 KV Cache 优化方法，深度理解 StreamingLLM\nFast Transformer Decoding: One Write-Head is All You Need\nGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\nPyTorch 分布式概览\nFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\nFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\nFlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision\nOnline normalizer calculation for softmax\n一心二用的Online Softmax\n","date":"2025-08-15T16:32:00+08:00","image":"https://kaigezheng.github.io/p/llm3/img/cover_hu_6ed020187f170be6.jpg","permalink":"https://kaigezheng.github.io/p/llm3/","title":"Infra入门——An Overview of AI Infra"},{"content":"前言 上个月（2025/05/22），微软终于把WSL在github上开源了。作为WSL2的深度长期用户，自然是对这个消息感到兴奋的，这意味着WSL未来能够得到更好的社区支持和维护。srds，在天灾（打翻水到笔记本上导致和外壳一体的键盘部分短路）和人祸（国补下的mac价格太香了，一直也很想玩玩macOS）的双重影响下，我购入了一台macbook air m4作为开发的主力机，替换了原来的Windows笔记本。这意味着我无法使用WSL了（尽管后来我了解到有类似Lima这样的开源替代方案）。\n虽然平时有大量的开发是在三江源数据分析中心的远程计算集群（Linux）中完成的，但远程终归是远程，没有本地的开发环境终于不是特别安全的（尤其是那个集群经常在客观和主观因素下都不太稳定）。我自己想到两个方案，一个是用VMware，另一个是Docker，采取了后者。放弃VMware的原因是我以为VMware Fusion仍旧处于付费状态，最近才发现其实已经在2024/05/15转为对个人用户免费了，那就留到以后再折腾罢。\n最终我选择了用Docker跑一个Ubuntu容器，再使用VSCode“远程”连接，在使用体验上和原生WSL加VSCode连接上几乎没有什么差异，当然也还有许多可以优化的工作。这何尝不是一种MSL（MacOS Subsystem for Linux）呢？\nMacOS Subsystem for Linux Docker Method 特别感谢一下@YYmicro学长在我调试Docker容器时给予的帮助和建议。\nPreliminary 安装Docker Desktop (汉化可以参考这个仓库，确认版本并覆盖asar即可)\n安装Visual Studio Code (with RemoteSSH Extension)\nNormal 在Docker Hub找到心仪的OS容器，这里以Ubuntu为例：\nubuntu docker container ubuntu docker container 之后在terminal操作，这里直接拉取默认的ubuntu，由于我的mac是arm64架构的M系列芯片（Apple Silicon），因此默认拉取的是arm架构的ubuntu(v24.04)：\n1 docker pull ubuntu ubuntu for arm64 接下来启动容器，保留标准输入、伪terminal、后台运行，除了转发一个用于ssh(remote port=22)的端口，随手转发几个端口以备后用，同时挂载一个本地目录用于同步：\n1 2 3 4 5 6 7 docker run -i -t -d \\ -p 2200:22 -p 8000:8000 -p 8001:8001 -p 8002:8002 \\ --name ubuntu_docker \\ -v /Users/kambri/Documents/ubuntu_docker:/home/dev \\ --privileged=true \\ b59d21599a2b \\ /bin/bash 接下来需要在容器的terminal继续操作，懒得复现了这里就简单罗列一下：\n更新apt和安装OpenSSH服务\n修改root账户密码，创建用户\n创建/var/run/sshd目录，开启OpenSSH服务，并运行普通用户和root用户登陆\n然后就可以像远程连接WSL一样，编写/Users/\u0026lt;username\u0026gt;/.ssh/config配置来远程连接Docker容器了。但是这里遇到一个问题，每次重启容器后不能自动帮我启动OpenSSH服务，需要进入容器terminal手动启动一下（应该可以通过更新容器启动命令解决）。同时，也可以使用Docker volume来优化，实现数据持久化存储（虽然我觉得还是目录同步比较方便）。总之，为了以后更方便地复用和构建容器，我选择写Dockerfile来准备一个能够开箱即用的基础容器。\nDockerfile 我平时Docker用得少，因此选择的方案和写的Dockerfile或许会比较toy。大致思路是从ubuntu继承，然后设置时区、更换apt源、更新apt并安装一些必要的基础软件、允许用户密码登陆、创建用户并修改密码、启动OpenSSH服务：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 FROM ubuntu ENV TZ=Asia/Shanghai RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone RUN sed -i \u0026#39;s@http://ports.ubuntu.com@http://mirrors.ustc.edu.cn@g\u0026#39; /etc/apt/sources.list.d/ubuntu.sources RUN sed -i \u0026#39;s@//.*archive.ubuntu.com@//mirrors.ustc.edu.cn@g\u0026#39; /etc/apt/sources.list.d/ubuntu.sources RUN apt clean \u0026amp;\u0026amp; apt update \u0026amp;\u0026amp; apt install -y openssh-server sudo vim git curl python3 python3-pip htop pciutils wget RUN mkdir /var/run/sshd RUN echo \u0026#39;root:\u0026lt;password\u0026gt;\u0026#39; | chpasswd RUN useradd -m -s /bin/bash kambri \u0026amp;\u0026amp; echo \u0026#39;kambri:\u0026lt;password\u0026gt;\u0026#39; | chpasswd \u0026amp;\u0026amp; adduser kambri sudo RUN sed -i \u0026#39;s/#PasswordAuthentication yes/PasswordAuthentication yes/\u0026#39; /etc/ssh/sshd_config \\ \u0026amp;\u0026amp; sed -i \u0026#39;s/PermitRootLogin prohibit-password/PermitRootLogin yes/\u0026#39; /etc/ssh/sshd_config RUN service ssh start CMD [\u0026#34;/usr/sbin/sshd\u0026#34;, \u0026#34;-D\u0026#34;] 这里踩了两个坑需要记录一下：\nUbuntu for ARM架构的官方源区别于传统的x86架构镜像，为http://ports.ubuntu.com/ubuntu-ports/，因此需要修改一下换源指令（保险起见，把两架构的镜像地址都换一下也没什么损失）\n需要加RUN service ssh start，否则docker run时不会自动打开OpenSSH服务\n之后照常docker run就可以正常使用了，如果之前连接过本地的127.0.0.1:\u0026lt;hostport\u0026gt;的话会在/Users/\u0026lt;username\u0026gt;/.ssh/known_hosts中记录远程主机密钥，因此当远程主机（Docker容器）变化时需要使用ssh-keygen -R \u0026quot;[127.0.0.1]:\u0026lt;hostport\u0026gt;\u0026quot;清理一下“缓存”。\n远程主机密钥变化警告 1 docker run -itd -p 2200:22 --name ubuntu-dev -v /Users/kambri/Documents/ubuntu_docker:/home/dev \u0026lt;IMAGE ID\u0026gt; MacOS Subsystem for Linux, 启动! VMware Method TODO: (maybe) coming soon \u0026hellip;\n","date":"2025-06-12T17:05:00+08:00","image":"https://kaigezheng.github.io/p/ops3/img/cover_hu_e2d36c53b74b8ad0.jpg","permalink":"https://kaigezheng.github.io/p/ops3/","title":"对MacOS的初步探索：MacOS \"Subsystem\" for Linux"},{"content":"Self-Attention SDPA $$Attention(Q, K, V)=softmax(\\frac{QK^{T}}{\\sqrt[]{d_{k}} })V$$根据计算公式可以得到Attention的计算流程，\n首先计算Attention Score：将$q$和$k^T$批量矩阵乘（BMM, Batch Matrix Multiplication）并除以Scaled因子，如果是Masked Self-Attention则需要通过掩码对mask为0的位置替换为-inf(exp(-inf)=0)\n对Attention Score在行维度上softmax后与$v$批量矩阵乘，得到Attention的输出\n1 2 3 4 5 6 7 8 def scaled_dot_product_attention(query, key, value, mask=None): dim_k = key.size(-1) attn_scores = torch.bmm(query, key.transpose(1, 2)) / np.sqrt(dim_k) if mask is not None: attn_scores.masked_fill_(mask == 0, float(\u0026#39;-inf\u0026#39;)) attn_weights = F.softmax(attn_scores, dim=-1) attn_outputs = torch.bmm(attn_weights, value) return attn_outputs 同时，PyTorch也提供了一个Efficient的SDPA算子，在矩阵规模较大时有一定加速效果：\n1 2 3 4 5 6 7 8 def scaled_dot_product_attention(query, key, value, mask=None): attn_output = F.scaled_dot_product_attention( query, key, value, attn_mask=mask, dropout_p=0.0, is_causal=False ) return attn_output SDPA是一个高度优化的算子，通过Python接口封装底层C++/CUDA实现，下面是Python的接口调用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # Efficient implementation equivalent to the following: def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None, enable_gqa=False) -\u0026gt; torch.Tensor: L, S = query.size(-2), key.size(-2) scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device) if is_causal: assert attn_mask is None temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0) attn_bias.masked_fill_(temp_mask.logical_not(), float(\u0026#34;-inf\u0026#34;)) attn_bias.to(query.dtype) if attn_mask is not None: if attn_mask.dtype == torch.bool: attn_bias.masked_fill_(attn_mask.logical_not(), float(\u0026#34;-inf\u0026#34;)) else: attn_bias = attn_mask + attn_bias if enable_gqa: key = key.repeat_interleave(query.size(-3)//key.size(-3), -3) value = value.repeat_interleave(query.size(-3)//value.size(-3), -3) attn_weight = query @ key.transpose(-2, -1) * scale_factor attn_weight += attn_bias attn_weight = torch.softmax(attn_weight, dim=-1) attn_weight = torch.dropout(attn_weight, dropout_p, train=True) return attn_weight @ value MHA Self-Attention 首先需要通过AttentionHead类实现一个单头注意力机制（SHA）作为MHA的组件，每个SHA会将embed_dim维度的信息映射到head_dim维度上：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class AttentionHead(nn.Module): def __init__(self, embed_dim, head_dim): super().__init__() # Learnable Parameters self.Wq = nn.Linear(embed_dim, head_dim) self.Wk = nn.Linear(embed_dim, head_dim) self.Wv = nn.Linear(embed_dim, head_dim) def forward(self, query_input, key_value_input): # Project Q q = self.Wq(query_input) # Project K k = self.Wk(key_value_input) # Project V v = self.Wv(key_value_input) attn_outputs = scaled_dot_product_attention(q, k, v) return attn_outputs 注意在Encoder Layer中，Self-Attention的q、k、v的输入都是同样的hidden states；但是在Decoder Layer中，q的输入是上一层hidden states，但是k、v的输入是来自最后一层Encoder Layer的hidden states，因此Attention Head如此设计。\n接下来是MHA的实现，注意变量经过MHA维度是不会发生变化的（embed_dim -\u0026gt; embed_dim）：\n$$MHA(Q, K, V) = concat(head_{1}, ..., head_{h})W^{O} \\newline (where\\ head_{i} = Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i}))$$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class MultiHeadAttention(nn.Module): def __init__(self, config): super().__init__() embed_dim = config.hidden_size # 768 num_heads = config.num_attention_heads # 12 head_dim = embed_dim // num_heads # 64 self.heads = nn.ModuleList([ AttentionHead(embed_dim, head_dim) for _ in range(num_heads) ]) # 768 -\u0026gt; 768 self.output_layer = nn.Linear(embed_dim, embed_dim) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, query_input, key_value_input, mask=None): x = torch.cat([head(query_input, key_value_input, mask) for head in self.heads], dim=-1) x = self.output_layer(x) x = self.dropout(x) return x Transformer Encoder Layer Transformer Encoder Layer Feed Forward Network Transformer架构的FFN使用GeLU（Gaussian Error Linear Unit）激活函数，可以看作是ReLU的平滑版本，具体公式如下：\n$$GeLU(x) = x \\cdot \\Phi(x) = x \\cdot \\frac{1}{2} (1 + \\text{erf}(\\frac{x}{\\sqrt{2}}))$$其中，\n$\\Phi(x)$是标准高斯分布的累计分布函数（CDF）\n$erf(x)$是误差函数，定义为$\\text{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_0^x e^{-t^2}$\nActivation Function Comparison 接下来实现FFN，具体为两层MLP和一次GeLU激活：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class FeedForward(nn.Module): def __init__(self, config): super().__init__() # (intermediate) self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size) # (output) self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size) self.gelu = nn.GELU() self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, hidden_states): hidden_states = self.fc1(hidden_states) hidden_states = self.gelu(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.fc2(hidden_states) return hidden_states Add \u0026amp; Layer Norm 这里直接将Add \u0026amp; Layer Norm写到最后的TransformerEncoderLayer中，一般取layer_norm_eps=1e-12，这里给出Post-LN（Transformer原文）的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Post-LN class TransformerEncoderLayer(nn.Module): def __init__(self, config): super().__init__() self.attention = MultiHeadAttention(config) self.ffn = FeedForward(config) self.layernorm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.layernorm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) def forward(self, hidden_states): attn_output = self.attention(hidden_states, hidden_states) hidden_states = self.layernorm1(hidden_states + attn_output) ffn_output = self.ffn(hidden_states) hidden_states = self.layernorm2(hidden_states + ffn_output) return hidden_states Comparison between Post-LN and Pre-LN Transformer Decoder Layer Transformer Decoder Layer 有了前面Encoder Layer的实现，Decoder Layer也就能够跟着架构图水到渠成了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class TransformerDecoderLayer(nn.Module): def __init__(self, config): super().__init__() self.self_attn = MultiHeadAttention(config) self.cross_attn = MultiHeadAttention(config) self.ffn = FeedForward(config) self.layernorm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.layernorm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.layernorm3 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) def forward(self, hidden_states, encoder_outputs, self_attn_mask=None, cross_attn_mask=None): self_attn_output = self.self_attn(hidden_states, hidden_states, self_attn_mask) hidden_states = self.layernorm1(hidden_states + self_attn_output) cross_attn_output = self.cross_attn(hidden_states, encoder_outputs, cross_attn_mask) hidden_states = self.layernorm2(hidden_states + cross_attn_output) ffn_output = self.ffn(hidden_states) hidden_states = self.layernorm3(hidden_states + ffn_output) return hidden_states Source Code 完整测试Demo如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 import torch import torch.nn as nn import torch.nn.functional as F import numpy as np def scaled_dot_product_attention(query, key, value, mask=None): dim_k = key.size(-1) attn_scores = torch.bmm(query, key.transpose(1, 2)) / np.sqrt(dim_k) if mask is not None: attn_scores.masked_fill_(mask == 0, float(\u0026#39;-inf\u0026#39;)) attn_weights = F.softmax(attn_scores, dim=-1) attn_outputs = torch.bmm(attn_weights, value) return attn_outputs class AttentionHead(nn.Module): def __init__(self, embed_dim, head_dim): super().__init__() # Learnable Parameters self.Wq = nn.Linear(embed_dim, head_dim) self.Wk = nn.Linear(embed_dim, head_dim) self.Wv = nn.Linear(embed_dim, head_dim) def forward(self, query_input, key_value_input, mask=None): # Project Q q = self.Wq(query_input) # Project K k = self.Wk(key_value_input) # Project V v = self.Wv(key_value_input) attn_outputs = scaled_dot_product_attention(q, k, v, mask) return attn_outputs class MultiHeadAttention(nn.Module): def __init__(self, config): super().__init__() embed_dim = config.hidden_size # 768 num_heads = config.num_attention_heads # 12 head_dim = embed_dim // num_heads # 64 self.heads = nn.ModuleList([ AttentionHead(embed_dim, head_dim) for _ in range(num_heads) ]) # 768 -\u0026gt; 768 self.output_layer = nn.Linear(embed_dim, embed_dim) self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, query_input, key_value_input, mask=None): x = torch.cat([head(query_input, key_value_input, mask) for head in self.heads], dim=-1) x = self.output_layer(x) x = self.dropout(x) return x class FeedForward(nn.Module): def __init__(self, config): super().__init__() # (intermediate) self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size) # (output) self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size) self.gelu = nn.GELU() self.dropout = nn.Dropout(config.hidden_dropout_prob) def forward(self, hidden_states): hidden_states = self.fc1(hidden_states) hidden_states = self.gelu(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.fc2(hidden_states) return hidden_states class TransformerEncoderLayer(nn.Module): def __init__(self, config): super().__init__() self.attention = MultiHeadAttention(config) self.ffn = FeedForward(config) self.layernorm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.layernorm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) def forward(self, hidden_states): attn_output = self.attention(hidden_states, hidden_states) hidden_states = self.layernorm1(hidden_states + attn_output) ffn_output = self.ffn(hidden_states) hidden_states = self.layernorm2(hidden_states + ffn_output) return hidden_states class TransformerDecoderLayer(nn.Module): def __init__(self, config): super().__init__() self.self_attn = MultiHeadAttention(config) self.cross_attn = MultiHeadAttention(config) self.ffn = FeedForward(config) self.layernorm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.layernorm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) self.layernorm3 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps) def forward(self, hidden_states, encoder_outputs, self_attn_mask=None, cross_attn_mask=None): self_attn_output = self.self_attn(hidden_states, hidden_states, self_attn_mask) hidden_states = self.layernorm1(hidden_states + self_attn_output) cross_attn_output = self.cross_attn(hidden_states, encoder_outputs, cross_attn_mask) hidden_states = self.layernorm2(hidden_states + cross_attn_output) ffn_output = self.ffn(hidden_states) hidden_states = self.layernorm3(hidden_states + ffn_output) return hidden_states class DummyConfig: hidden_size = 768 num_attention_heads = 12 intermediate_size = 3072 hidden_dropout_prob = 0.1 layer_norm_eps = 1e-12 if __name__ == \u0026#39;__main__\u0026#39;: config = DummyConfig() batch_size = 2 seq_len = 10 hidden_size = config.hidden_size # 输入张量 dummy_input = torch.randn(batch_size, seq_len, hidden_size) # 测试 Encoder Layer encoder_layer = TransformerEncoderLayer(config) encoder_output = encoder_layer(dummy_input) print(\u0026#34;Encoder Output Shape:\u0026#34;, encoder_output.shape) # 测试 Decoder Layer decoder_layer = TransformerDecoderLayer(config) decoder_input = torch.randn(batch_size, seq_len, hidden_size) decoder_output = decoder_layer(decoder_input, encoder_output) print(\u0026#34;Decoder Output Shape:\u0026#34;, decoder_output.shape) Outputs:\n1 2 Encoder Output Shape: torch.Size([2, 10, 768]) Decoder Output Shape: torch.Size([2, 10, 768]) Reference On Layer Normalization in the Transformer Architecture\nAttention Is All You Need\nDocument: torch.nn.functional.scaled_dot_product_attention\nbilibili-五道口纳什-BERT、T5、GPT合集\n","date":"2025-06-03T22:05:15+08:00","image":"https://kaigezheng.github.io/p/llm2/img/cover_hu_8c81a53c7401f325.png","permalink":"https://kaigezheng.github.io/p/llm2/","title":"手搓Transformer：深入架构细节"},{"content":"BERT Architecture Total Summary BERT的模型架构完全基于Transformer架构的编码器（Encoder）堆叠（原文使用12层或24层Transformer Layer），每个Encoder包括多头自注意力机制（MHA，Multi-Head Self-Attention，支持双向上下文理解）、前馈神经网络（FFN，Feed-Forward Network，对注意力输出进行非线性变换），参差连接和层归一化（Add \u0026amp; Norm，提升训练稳定性）。\nTransformer Model Architecture A Simple Demo 这里以一个Huggingface发布的用于英文句子情感二分类的蒸馏BERTDistilBERT (distilbert-base-uncased-finetuned-sst-2-english) 66M为例，使用transformers库实现加载并推理。\n1 huggingface-cli download distilbert/distilbert-base-uncased-finetuned-sst-2-english --local-dir distilbert-base-uncased-finetuned-sst-2-english 下载模型后，即可通过AutoTokenizer和AutoModelForSequenceClassification导入模型。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import torch.nn.functional as F from transformers import AutoTokenizer, AutoModelForSequenceClassification # 加载tokenizer和model model_name = \u0026#39;/path/to/bert-model\u0026#39; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name) # sentence -\u0026gt; tokens test_sentences = [\u0026#39;today is not that bad\u0026#39;, \u0026#39;today is so bad\u0026#39;] batch_input = tokenizer(test_sentences, padding=True, truncation=True, return_tensors=\u0026#39;pt\u0026#39;) # inference with torch.no_grad(): outputs = model(**batch_input) # 解包tokens -\u0026gt; inference result print(\u0026#34;outputs\u0026#34;, outputs) scores = F.softmax(outputs.logits, dim=1) # 对logits(预测分数)对每一行（类别）进行softmax print(\u0026#34;scores\u0026#34;, scores) labels = torch.argmax(scores, dim=1) # 沿类别维度获取最大值索引 labels = [model.config.id2label[id.item()] for id in labels] # 0-\u0026gt;LABEL_0, 1-\u0026gt;LABEL_1 print(\u0026#34;labels\u0026#34;, labels) 即可得到以下推理结果：\n1 2 3 4 5 outputs SequenceClassifierOutput(loss=None, logits=tensor([[-3.4620, 3.6118], [ 4.7508, -3.7899]]), hidden_states=None, attentions=None) scores tensor([[8.4632e-04, 9.9915e-01], [9.9980e-01, 1.9531e-04]]) labels [\u0026#39;POSITIVE\u0026#39;, \u0026#39;NEGATIVE\u0026#39;] 关于with torch.no_grad()和param.requires_grad=False的区别：\nwith torch.no_grad()适用于eval阶段，定义了一个上下文管理器，隐式不进行梯度更新，不会改变requires_grad\nparam.requires_grad=False显式地frozen掉一些layer的梯度更新\n接下来需要对一些细节进行补充。\nmodel.config model.config用于存储模型架构和训练配置。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 DistilBertConfig { \u0026#34;activation\u0026#34;: \u0026#34;gelu\u0026#34;, \u0026#34;architectures\u0026#34;: [ \u0026#34;DistilBertForSequenceClassification\u0026#34; ], \u0026#34;attention_dropout\u0026#34;: 0.1, \u0026#34;dim\u0026#34;: 768, \u0026#34;dropout\u0026#34;: 0.1, \u0026#34;finetuning_task\u0026#34;: \u0026#34;sst-2\u0026#34;, \u0026#34;hidden_dim\u0026#34;: 3072, \u0026#34;id2label\u0026#34;: { \u0026#34;0\u0026#34;: \u0026#34;NEGATIVE\u0026#34;, \u0026#34;1\u0026#34;: \u0026#34;POSITIVE\u0026#34; }, \u0026#34;initializer_range\u0026#34;: 0.02, \u0026#34;label2id\u0026#34;: { \u0026#34;NEGATIVE\u0026#34;: 0, \u0026#34;POSITIVE\u0026#34;: 1 }, \u0026#34;max_position_embeddings\u0026#34;: 512, \u0026#34;model_type\u0026#34;: \u0026#34;distilbert\u0026#34;, \u0026#34;n_heads\u0026#34;: 12, \u0026#34;n_layers\u0026#34;: 6, \u0026#34;output_past\u0026#34;: true, \u0026#34;pad_token_id\u0026#34;: 0, \u0026#34;qa_dropout\u0026#34;: 0.1, \u0026#34;seq_classif_dropout\u0026#34;: 0.2, \u0026#34;sinusoidal_pos_embds\u0026#34;: false, \u0026#34;tie_weights_\u0026#34;: true, \u0026#34;torch_dtype\u0026#34;: \u0026#34;float32\u0026#34;, \u0026#34;transformers_version\u0026#34;: \u0026#34;4.52.3\u0026#34;, \u0026#34;vocab_size\u0026#34;: 30522 } Rules on Tokenizer 调用tokenizer即调用tokenizer.__call__或tokenizer.encoder（不完全等价，encoder默认不返回attention_mask），将返回含有input_ids和attention_mask的输入字典（inputs_ids和attention_mask长度一致）。（具体返回什么视模型具体需要而定）\ntokenizer.encoder的调用分为两步，先分词，再编码，所以等价于先调用tokenizer.tokenize再调用tokenizer.convert_tokens_to_ids。\n在句子对编码时，使用tokenizer.encode_plus，在返回字典中除了inputs_ids和attention_mask，还会返回token_type_ids$\\in {0, 1}$用于标记第一句话和第二句话。在tokens中，会用[SEQ]分割。\n1 2 3 4 5 6 7 8 9 10 # 编码 print(tokenizer(test_sentences[0])) # tokenizer.encode = tokenizer.tokenize + tokenizer.convert_tokens_to_ids print(tokenizer.encode(test_sentences[0],)) ## tokenize print(tokenizer.tokenize(test_sentences[0])) ## convert_tokens_to_ids print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_sentences[0]))) # 解码 print(tokenizer.decode([101, 2651, 2003, 2025, 2008, 2919, 102])) 可以得到以下结果：\n1 2 3 4 5 6 7 8 9 # 编码 {\u0026#39;input_ids\u0026#39;: [101, 2651, 2003, 2025, 2008, 2919, 102], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1, 1]} [101, 2651, 2003, 2025, 2008, 2919, 102] ## tokenize [\u0026#39;today\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;not\u0026#39;, \u0026#39;that\u0026#39;, \u0026#39;bad\u0026#39;] ## convert_tokens_to_ids [2651, 2003, 2025, 2008, 2919] # 解码 [CLS] today is not that bad [SEP] tokenizer是根据tokenizer.vocab为依据进行编码的，以下是特殊token表（可以通过tokenizer.special_tokens_map或直接tokenizer查看），tokenizer会尽量避免将词分为[UNK]（存在5828个##开头的后缀子词）。\n1 2 3 4 5 {\u0026#39;unk_token\u0026#39;: \u0026#39;[UNK]\u0026#39;, # 100 \u0026#39;sep_token\u0026#39;: \u0026#39;[SEP]\u0026#39;, # 102 \u0026#39;pad_token\u0026#39;: \u0026#39;[PAD]\u0026#39;, # 0 \u0026#39;cls_token\u0026#39;: \u0026#39;[CLS]\u0026#39;, # 101 \u0026#39;mask_token\u0026#39;: \u0026#39;[MASK]\u0026#39; # 103} Parameter 这里以Google发布的google-bert/bert-base-uncased（12层BertLayer）为例。\n通过model可以看到BERT的架构如下：\nEmbedding由word embeddings、position embeddings和token type embedding三部分组成\nEncoder由12层BertLayer组成，每层BertLayer都由一次Self Attention和一次FFN组成\nPooler全连接层\nOutput(optional)作为下游任务的输出层\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 BertModel( (embeddings): BertEmbeddings( (word_embeddings): Embedding(30522, 768, padding_idx=0) (position_embeddings): Embedding(512, 768) (token_type_embeddings): Embedding(2, 768) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) (encoder): BertEncoder( (layer): ModuleList( (0-11): 12 x BertLayer( (attention): BertAttention( (self): BertSdpaSelfAttention( (query): Linear(in_features=768, out_features=768, bias=True) (key): Linear(in_features=768, out_features=768, bias=True) (value): Linear(in_features=768, out_features=768, bias=True) (dropout): Dropout(p=0.1, inplace=False) ) (output): BertSelfOutput( (dense): Linear(in_features=768, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) (intermediate): BertIntermediate( (dense): Linear(in_features=768, out_features=3072, bias=True) (intermediate_act_fn): GELUActivation() ) (output): BertOutput( (dense): Linear(in_features=3072, out_features=768, bias=True) (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (dropout): Dropout(p=0.1, inplace=False) ) ) ) ) (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) ) 可以用以下代码计算每一部分的参数量：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 total_params = 0 total_learnable_params = 0 total_embedding_params = 0 total_encoder_params = 0 total_pooler_params = 0 for name, param in model.named_parameters(): # print(name, \u0026#39;-\u0026gt;\u0026#39;, param.shape, \u0026#39;-\u0026gt;\u0026#39;, param.numel()) # 加上`if param.requires_grad:`可以计算可学习参数量 if \u0026#39;embedding\u0026#39; in name: total_embedding_params += param.numel() if \u0026#39;encoder\u0026#39; in name: total_encoder_params += param.numel() if \u0026#39;pooler\u0026#39; in name: total_pooler_params += param.numel() if param.requires_grad: total_learnable_params += param.numel() total_params += param.numel() params = [ (\u0026#34;total_embedding_params\u0026#34;, total_embedding_params), (\u0026#34;total_encoder_params\u0026#34;, total_encoder_params), (\u0026#34;total_pooler_params\u0026#34;, total_pooler_params) ] for name, param in params: percentage = (param / total_params) * 100 print(f\u0026#34;{name}: {percentage:.2f} %, {param}\u0026#34;) 得到输出结果：\n1 2 3 total_embedding_params: 21.77 %, 23837184 total_encoder_params: 77.69 %, 85054464 total_pooler_params: 0.54 %, 590592 Output 在outputs = model(**input)后调用type(outputs)可以发现Bert的输出类型是transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions。参考Huggingface-BERT文档，默认情况下长度为2（last_hidden_state和pooler_output）。当定义模型时指定output_hidden_states=True时，还会返回hidden_state，其他参数类似。\noutput[0] (last_hidden_state), shape = (batch_size, seq_len, hidden_size)\noutput[1] (pooler_output), shape = (batch_size, hidden_size)\n最终隐藏状态（classification token, [CLS]的输出）\noutput[2] (hidden_states), tuple, embedding layer和每个layer的输出（1+12）, shape = 13 * (batch_size, seq_len, hidden_size) 如model.embeddings(input['input_ids'], input['token_type_ids']) == outputs[2][0]表示Embedding层的输出。\nEmbedding 上文提到，BERT的Embedding层由word embeddings、position embeddings和token type embedding三部分组成，以下代码实现了简单的Embedding层：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from transformers import BertTokenizer, BertModel tokenizer = BertTokenizer.from_pretrained(\u0026#34;bert-base-uncased\u0026#34;) model = BertModel.from_pretrained(\u0026#34;bert-base-uncased\u0026#34;) input = tokenizer(sentence, return_tensors=\u0026#39;pt\u0026#39;) # {input_ids, token_type_ids, attention_mask} input_ids = input[\u0026#39;input_ids\u0026#39;] # shape = (batch_size, token_len) token_type_ids = input[\u0026#39;token_type_ids\u0026#39;] # shape = (batch_size, token_len) pos_ids = torch.arange(input_ids.shape[1]) # shape = (token_len) # 1. Word Embedding word_embed = model.embeddings.word_embeddings(input_ids) # shape = (batch_size, token_len, embedding_size=768) # 2. Token Type Embedding tok_embed = model.embeddings.token_type_embeddings(token_type_ids) # shape = (batch_size, token_len, embedding_size=768) # 3. Position Embedding pos_embed = model.embeddings.position_embeddings(pos_ids) # shape = (token_len, embedding_size=768) # **Input Embedding** input_embed = word_embed + tok_embed + pos_embed.unsqueeze(0) # 也可以不unsqueeze, 会broadcast的 # 后处理 embed = model.embeddings.LayerNorm(input_embed) embed = model.embeddings.dropout(embed) Self-Attention $$Attention(Q, K, V)=softmax(\\frac{QK^{T}}{\\sqrt[]{d_{k}} })V$$接下来是从Embedding层输出到Multi-Head Self-Attention (MHA)的代码实现（first head）：\n1 2 3 4 5 6 7 8 9 10 11 att_head_size = int(model.config.hidden_size / model.config.num_attention_heads) # 768 / 12 = 64 emb_output = model.embeddings(input[\u0026#39;input_ids\u0026#39;], input[\u0026#39;token_type_ids\u0026#39;]) # shape = (batch_size, seq_len, embedding_dim) # emb_output[0].shape = (seq_len, embedding_dim) ## Why Transpose? 因为PyTorch中的Linear里就是x@A^T（左乘转置） Q_first_head_first_layer = emb_output[0] @ model.encoder.layer[0].attention.self.query.weight.T[:, :att_head_size] + model.encoder.layer[0].attention.self.query.bias[:att_head_size] K_first_head_first_layer = emb_output[0] @ model.encoder.layer[0].attention.self.key.weight.T[:, :att_head_size] + model.encoder.layer[0].attention.self.key.bias[:att_head_size] # (seq_len, att_head_size) @ (seq_len, att_head_size).T -\u0026gt; (seq_len, seq_len) attn_scores = torch.nn.Softmax(dim=-1)(Q_first_head_first_layer @ K_first_head_first_layer.T) / math.sqrt(att_head_size) V_first_head_first_layer = emb_output[0] @ model.encoder.layer[0].attention.self.value.weight.T[:, :att_head_size] + model.encoder.layer[0].attention.self.value.bias[:att_head_size] attn_emb = attn_scores @ V_first_head_first_layer # shape = (seq_len, att_head_size) 接下来是关于MHA的公式推导，定义$E$为Embedding层的输出$q$、$k$、$v$分别为同一token对应的query、key、value，$W_q$、$W_k$、$W_v$分别为同一token的权重，$Q$、$K$、$V$分别为整个序列的query、key、value，$W_Q$、$W_K$、$W_V$分别为整个序列的权重。这里省略bias项。\n先从某一token出发，$T$表示序列长度，$d_e$表示Embedding层维度，$d_q$、$d_k$、$d_v$分别表示q、k、v的维度，$E \\in \\mathbb{R}^{T \\times d_e}$，那么：\n$$E \\cdot W_q = q \\in \\mathbb{R}^{T \\times d_q}$$$$E \\cdot W_k = k \\in \\mathbb{R}^{T \\times d_k}$$$$E \\cdot W_v = v \\in \\mathbb{R}^{T \\times d_v}$$其中$d_q == d_k$，因为后续需要计算$q \\cdot k^{T}$，$d_v$则没有要求：\n$$Attention\\ Score = Softmax(\\frac{q \\cdot k^{T}}{\\sqrt{d_k}}) \\in \\mathbb{R}^{T \\times T}$$$$Attention\\ Output = Softmax(\\frac{q \\cdot k^{T}}{\\sqrt{d_k}}) \\cdot v \\in \\mathbb{R}^{T \\times d_v}$$接下来定义Attention头数为$n$，将单头的情况拓展到多头：\n\\[ \\left[ \\begin{array}{c|c|c|c} E\\cdot W_{q_1} \u0026 E\\cdot W_{q_2} \u0026 ... \u0026 E\\cdot W_{q_n} \\\\ \\end{array} \\right]=E\\cdot \\left[ \\begin{array}{c|c|c|c} W_{q_1} \u0026 W_{q_2} \u0026 ... \u0026 W_{q_n} \\\\ \\end{array} \\right]=E\\cdot W_Q=Q \\in \\mathbb{R}^{T \\times n\\cdot d_q} \\]\\[ \\left[ \\begin{array}{c|c|c|c} E\\cdot W_{k_1} \u0026 E\\cdot W_{k_2} \u0026 ... \u0026 E\\cdot W_{k_n} \\\\ \\end{array} \\right]=E\\cdot \\left[ \\begin{array}{c|c|c|c} W_{k_1} \u0026 W_{k_2} \u0026 ... \u0026 W_{k_n} \\\\ \\end{array} \\right]=E\\cdot W_K=K \\in \\mathbb{R}^{T \\times n\\cdot d_k} \\]\\[ \\left[ \\begin{array}{c|c|c|c} E\\cdot W_{v_1} \u0026 E\\cdot W_{v_2} \u0026 ... \u0026 E\\cdot W_{v_n} \\\\ \\end{array} \\right]=E\\cdot \\left[ \\begin{array}{c|c|c|c} W_{v_1} \u0026 W_{v_2} \u0026 ... \u0026 W_{v_n} \\\\ \\end{array} \\right]=E\\cdot W_V=V \\in \\mathbb{R}^{T \\times n\\cdot d_v} \\]那么就可以得到完整的MHA了：\n$$Attention(Q, K, V)=softmax(\\frac{QK^{T}}{\\sqrt[]{d_{k}} })V \\in \\mathbb{R}^{T \\times n \\cdot d_v}$$Add \u0026amp; Norm Encoder of Transformer 在Encoder中共有两次参差连接和层归一化（Add \u0026amp; Norm），第一次发生在MHA中：\n1 2 3 4 layer = model.encoder.layer[0] # First Layer embeddings = model(**input)[2][0] # Embeddings Layer Output mha_output = layer.attention.self(embeddings) attn_output = layer.attention.output(mha_output[0], embeddings) 第二次发生在MLP中：\n1 2 mlp1 = layer.intermediate(attn_output) # shape = (batch_size, seq_len, 4x768) mlp2 = layer.output.output(mlp1, attn_output) # 这个结果和output[2][1]是相同的（layer 1的输出结果） Add \u0026amp; Norm的实现很简单，如下：\n1 2 3 4 5 def forward(self, hidden_states, input_tensor): hidden_states = self.dense(hidden_states) hidden_states = self.dropout(hidden_states) hidden_states = self.LayerNorm(hidden_states + input_tensor) return hidden_states Pooler 对于output = model(**input)，一般有两个keys，即last_hidden_state(shape=(batch_size, seq_len, emb_dim))和pooler_output(shape=(batch_size, emb_dim))。\n其中pooler_output少了seq_len的维度，观察BERT源码可以发现，pooler_output是BERT encoder只对第一个token（也就是[CLS]）进行了一次全连接和激活的输出。\n1 2 3 4 5 6 7 def forward(self, hidden_states): # We \u0026#34;pool\u0026#34; the model by simply taking the hidden state corresponding # to the first token. first_token_tensor = hidden_states[:, 0] # 第一个元素的hidden_states pooled_output = self.dense(first_token_tensor) pooled_output = self.activation(pooled_output) return pooled_output 也可以翻译为以下代码：\n1 2 3 first_sentence = output[\u0026#39;last_hidden_state\u0026#39;][0] pool_output = bert.pooler.dense(first_sentence[0, :]) pool_output = bert.pooler.activation(pool_output) 下面的图可以清晰地诠释这一过程：\nChris McCormick的好图 这一Pooler Layer可以视为BERT的一个默认head，作为最后BERT的输出。在不同的任务下，一般保留同样的Embedding Layer和中间Layer，只替换最后这一部分。\nMasked Language Model 掩码语言模型（MLM, Masked Language Model）是BERT的一种自监督学习任务，模型的目标是预测输入文本中被随机覆盖（masked）的token（完形填空）。当我们使用BertForMaskedLM加载模型后，观察配置（主要关注最后一层）。\nCLS Layer BERT（base）的最后一层是上一节提到的简单的Pooler Player：\n1 2 3 4 (pooler): BertPooler( (dense): Linear(in_features=768, out_features=768, bias=True) (activation): Tanh() ) 而BERT（MLM）的最后一层是一个略微复杂一些的CLS Layer，由transform（全连接+激活+层归一化）和decoder（全连接）两个运算构成：\n1 2 3 4 5 6 7 8 9 10 (cls): BertOnlyMLMHead( (predictions): BertLMPredictionHead( (transform): BertPredictionHeadTransform( (dense): Linear(in_features=768, out_features=768, bias=True) (transform_act_fn): GELUActivation() (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) ) (decoder): Linear(in_features=768, out_features=30522, bias=True) ) ) 经过self.transform运算后，仍然保持shape=(batch_size, seq_len, emb_dim=768)，作用仅仅是做一次同样维度的全连接激活；self.decoder也只是一个简单的全连接，将768维映射到vocab_size=30522维（多分类任务）。\nMasking 既然是监督学习，就需要制作Label了。以下代码可以对经过tokenizer的文本进行随机mask并加入label标记原始文本：\n1 2 3 4 5 6 7 8 9 inputs = tokenizer(text, return_tensors=\u0026#39;pt\u0026#39;) inputs[\u0026#39;labels\u0026#39;] = inputs[\u0026#39;input_ids\u0026#39;].detach().clone() # 生成掩码矩阵（序列） mask_arr = (torch.rand(inputs[\u0026#39;input_ids\u0026#39;].shape) \u0026lt; 0.15) * (inputs[\u0026#39;input_ids\u0026#39;] != 101) * (inputs[\u0026#39;input_ids\u0026#39;] != 102) # 筛选掩码列表 selection = torch.flatten(mask_arr[0].nonzero()).tolist() # 随机mask inputs[\u0026#39;input_ids\u0026#39;][0, selection] = tokenizer.vocab[\u0026#39;[MASK]\u0026#39;] # or 103 Computing Process 与Base模型的Pooler Layer将最后一层的第一个token的隐藏状态作为输入不同，MLM将最后一层的所有隐藏状态作为输出，即mlm_output = mlm.cls(outputs['hidden_states'][-1])。实际上就是这么个流程：\n1 2 3 4 5 mlm.eval() last_hidden_state = outputs[\u0026#39;hidden_states\u0026#39;][-1] # (batch_size, seq_len, emb_dim) with torch.no_grad(): transformed = mlm.cls.predictions.transform(last_hidden_state) # (batch_size, seq_len, emb_dim) still logits = mlm.cls.predictions.decoder(transformed) # (batch_size, seq_len, vocab_size) Loss \u0026amp; Translate mlm(**inputs)的返回类型是transformers.modeling_outputs.MaskedLMOutput，即odict_keys(['loss', 'logits', 'hidden_states'])。\noutput.loss是一个tensor标量，使用CrossEntropy，实现如下：\n1 2 ce = nn.CrossEntropyLoss() outputs.loss = ce(logits[0], inputs[\u0026#39;labels\u0026#39;][0].view(-1)) 而翻译也很简单，使用torch.argmax(logits[0], dim=1)找到最大概率分数的索引即可：\n1 \u0026#39; \u0026#39;.join(tokenizer.convert_ids_to_tokens(torch.argmax(logits[0], dim=1))) Fine-Tuning Task (Text Classification) 这里参考fine tune transformers 文本分类/情感分析教程，实现一个基于BERT的情感分析的全流程全参微调任务。情感分析是文本/序列分类任务的一种，实质上就是对文本/序列进行多分类的自监督学习。\nData Data Load - emotions 这里选择使用emotions数据集，常用于文本情感分类，识别句子或段落中表达的情感类别，包括6类标签（sadness, joy, love, anger, fear, surprise）。\n1 2 from datasets import load_dataset emotions = load_dataset(\u0026#34;emotions\u0026#34;) 通过打印emotions可以看到emotions数据集的组成，共有两万条数据，$Size_{Train} : Size_{Vali} : Size_{Test} = 8 : 1 : 1$，每个数据有text和label两个features（dict of dict）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 DatasetDict({ train: Dataset({ features: [\u0026#39;text\u0026#39;, \u0026#39;label\u0026#39;], num_rows: 16000 }) validation: Dataset({ features: [\u0026#39;text\u0026#39;, \u0026#39;label\u0026#39;], num_rows: 2000 }) test: Dataset({ features: [\u0026#39;text\u0026#39;, \u0026#39;label\u0026#39;], num_rows: 2000 }) }) labels = emotions['train'].features['label'].names可以查看各个标签。\nData Visualization Analysis 简单可视化分析一下数据集，主要任务有：\n将dataset转化为dataframe，方便后续操作\n分析文本长度和标签频率\n1 2 3 4 # Task 1: dataset -\u0026gt; dataframe emotions_df = pd.DataFrame.from_dict(emotions[\u0026#39;train\u0026#39;]) # 取出训练集 emotions_df[\u0026#39;label_name\u0026#39;] = emotions_df[\u0026#39;label\u0026#39;].apply(lambda x: labels[x]) # 加入标签名列 emotions_df[\u0026#39;words per tweet\u0026#39;] = emotions_df[\u0026#39;text\u0026#39;].str.split().apply(len) # 统计words数 接下来可以简单分析：\n1 2 3 4 5 6 7 # 统计标签数 emotions_df.label.value_counts() emotions_df.label_name.value_counts() # 查看最长/短文本 emotions_df[\u0026#39;words per tweet\u0026#39;].max() emotions_df[\u0026#39;words per tweet\u0026#39;].idxmax() emotions_df.iloc[...][\u0026#39;text\u0026#39;] 简单的可视化：\n1 2 3 4 5 6 7 8 9 10 11 # Labels\u0026#39; Freq plt.figure(figsize=(4, 3)) emotions_df[\u0026#39;label_name\u0026#39;].value_counts(ascending=True).plot.barh() plt.title(\u0026#39;freq of labels\u0026#39;) # Words / Tweet plt.figure(figsize=(4, 3)) emotions_df[\u0026#39;words per tweet\u0026#39;] = emotions_df[\u0026#39;text\u0026#39;].str.split().apply(len) # 简单统计 emotions_df.boxplot(\u0026#39;words per tweet\u0026#39;, by=\u0026#39;label_name\u0026#39;, showfliers=True, grid=False, color=\u0026#39;black\u0026#39;) plt.suptitle(\u0026#39;\u0026#39;) plt.xlabel(\u0026#39;\u0026#39;) 标签频率 文本长度 Text2Tokens 为了后续模型的训练，需要将数据集转换为模型接受的输入类型。对于model，需要关注BERT/DistillBERT使用subword tokenizer；对于tokenizer，需要关注tokenizer.vocab_size、model_max_length和model_input_name几个参数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from transformers import AutoTokenizer model_ckpt = \u0026#34;/path/to/bert-distill\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_ckpt) print(tokenizer.vocab_size, tokenizer.model_max_length, tokenizer.model_input_names) # 30522 512 [\u0026#39;input_ids\u0026#39;, \u0026#39;attention_mask\u0026#39;] # 输出一个batch的text， 输出一个batch的tokens def batch_tokenize(batch): return tokenizer(batch[\u0026#39;text\u0026#39;], padding=True, truncation=True) emotions_encoded = emotions.map(batch_tokenize, batched=True, batch_size=None) # type(emotions_encoded[\u0026#39;train\u0026#39;][\u0026#39;input_ids\u0026#39;][0]) == list # list to tensor emotions_encoded.set_format(\u0026#39;torch\u0026#39;, columns=[\u0026#39;input_ids\u0026#39;, \u0026#39;attention_mask\u0026#39;, \u0026#39;label\u0026#39;]) Model Fine-Tuning Load Model DistillBERT-base-uncased是huggingface提供的一个轻量级BERT模型，由知识蒸馏（Knowledge Distillation）技术训练，在保持高性能的同时大幅减少了模型参数，使推理速度更快、计算资源需求更低。\n1 2 3 from transformers import AutoModel model_ckpt = \u0026#39;/home/HPC_ASC/bert-distill\u0026#39; model = AutoModel.from_pretrained(model_ckpt) 通过model可以发现，相较于BERT baseline，这个蒸馏后的模型在Embedding Layer减少了token_type_embedding（只有word embedding和position embedding），将原本12层Transformer Layer改为6层。\n1 2 3 4 5 6 def get_params(model): model_parameters = filter(lambda p: p.requires_grad, model.parameters()) params = sum([np.prod(p.size()) for p in model_parameters]) return params get_params(model) # return np.int64(66362880) 相较于bert-base-uncased少了约40%参数 接下来加载模型，可以通过nvidia-smi或nvtop（如果正确安装和配置了的话）可以看到加载到显存中的模型占用约546MiB。\n1 2 3 4 from transformers import AutoModelForSequenceClassification # 和AutoModel不同，后者没有分类头 -\u0026gt; 下游任务 modle_ckpt = \u0026#39;/home/HPC_ASC/distilbert-base-uncased\u0026#39; device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_classes, ignore_mismatched_sizes=True).to(device) Transformers Trainer 我们需要导入并定义huggingface提供的训练API来完成高效的模型训练。在正式定义Trainer前，还需要一个辅助函数来确定Trainer的参数指标：\n1 2 3 4 5 6 7 8 9 10 def compute_classification_metrics(pred): # pred: PredictionOutput, from trainer.predict(dataset) # true label labels = pred.label_ids # pred preds = pred.predictions.argmax(-1) f1 = f1_score(labels, preds, average=\u0026#34;weighted\u0026#34;) acc = accuracy_score(labels, preds) precision = precision_score(labels, preds, average=\u0026#34;macro\u0026#34;) return {\u0026#34;accuracy\u0026#34;: acc, \u0026#34;f1\u0026#34;: f1, \u0026#34;precision\u0026#34;: precision} 接下来正式定义Trainer（在这一步可能会提示你需要安装transformers[torch]或accelerator \u0026gt;= 0.26?，务必不要直接pip install transformers[torch]，这会导致卸载我本地已安装的torch (v2.7.0)并安装torch (v2.6.0)，从而让torchvision等包的版本不匹配，进而引发更多错误），并开始模型训练：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # https://huggingface.co/docs/transformers/main_classes/trainer from transformers import TrainingArguments, Trainer batch_size = 64 logging_steps = len(emotions_encoded[\u0026#39;train\u0026#39;]) // batch_size # 160,000 // batch_size model_name = f\u0026#39;{model_ckpt}_emotion_ft_0531\u0026#39; training_args = TrainingArguments(output_dir=model_name, num_train_epochs=4, learning_rate=2e-5, weight_decay=0.01, # 默认使用AdamW的优化算法 per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, eval_strategy=\u0026#34;epoch\u0026#34;, disable_tqdm=False, logging_steps=logging_steps, # write push_to_hub=True, log_level=\u0026#34;error\u0026#34;) trainer = Trainer(model=model, tokenizer=tokenizer, train_dataset=emotions_encoded[\u0026#39;train\u0026#39;], eval_dataset=emotions_encoded[\u0026#39;validation\u0026#39;], args=training_args, compute_metrics=compute_classification_metrics) trainer.train() nvtop: 硬件环境是一机八卡Tesla P100 16G Model Training 训练好的模型权重会存放在当前目录的model_name（bert-distill_emotion_ft_0531）下。\nInference 1 2 3 4 5 6 7 8 9 10 11 12 13 14 preds_output = trainer.predict(emotions_encoded[\u0026#39;test\u0026#39;]) y_preds = np.argmax(preds_output.predictions, axis=-1) y_true = emotions_encoded[\u0026#39;validation\u0026#39;][\u0026#39;label\u0026#39;] # for classification def plot_confusion_matrix(y_preds, y_true, labels): cm = confusion_matrix(y_true, y_preds, normalize=\u0026#34;true\u0026#34;) fig, ax = plt.subplots(figsize=(4, 4)) disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels) disp.plot(cmap=\u0026#34;Blues\u0026#34;, values_format=\u0026#34;.2f\u0026#34;, ax=ax, colorbar=False) plt.title(\u0026#34;Normalized confusion matrix\u0026#34;) plot_confusion_matrix(y_preds, y_true, labels) Confusion Matrix 可以写一个辅助函数，将测试集的loss和预测结果映射到测试集中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from torch.nn.functional import cross_entropy def forward_pass_with_label(batch): # Place all input tensors on the same device as the model inputs = {k:v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names} with torch.no_grad(): output = model(**inputs) pred_label = torch.argmax(output.logits, axis=-1) loss = cross_entropy(output.logits, batch[\u0026#39;label\u0026#39;].to(device), reduction=\u0026#39;none\u0026#39;) # Place outputs on CPU for compatibility with other dataset columns return {\u0026#34;loss\u0026#34;: loss.cpu().numpy(), \u0026#34;predicted_label\u0026#34;: pred_label.cpu().numpy()} emotions_encoded[\u0026#39;validation\u0026#39;] = emotions_encoded[\u0026#39;validation\u0026#39;].map( forward_pass_with_label, batched=True, batch_size=16 ) Push into Huggingface 1 2 3 4 5 6 # 在Jupyter Jotebook中登录huggingface # 需要在huggingface注册一个writable token from huggingface_hub import notebook_login notebook_login() trainer.push_to_hub(commit_message=\u0026#34;Training completed!\u0026#34;) Huggingface Login Huggingface Repo Page 当需要在其他地方调用这个模型时，可以通过transformers包的pipeline轻松实现：\n1 2 3 4 5 6 7 from transformers import pipeline model_id = \u0026#34;KambriKG/bert-distill_emotion_ft_0531\u0026#34; classifier = pipeline(\u0026#34;text-classification\u0026#34;, model=model_id) custom_tweet = \u0026#34;I saw a movie today and it was really good\u0026#34; preds = classifier(custom_tweet, return_all_scores=True) Reference BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nHuggingface-BERT文档\nChris McCormick\u0026rsquo;s Blog\n动手学深度学习(PyTorch)\nbilibili-五道口纳什-BERT、T5、GPT合集\n学习BERT时在五道口纳什的频道收益良多，《动手写BERT》系列教程非常适合对LLM有一定认识但缺乏实践经验的入门者学习参考。\n","date":"2025-06-02T22:55:00+08:00","image":"https://kaigezheng.github.io/p/llm1/img/cover_hu_e21dc3c225e6d47b.png","permalink":"https://kaigezheng.github.io/p/llm1/","title":"从Transformer开始探索BERT"},{"content":"这段时间和机房打交道比较多，负责对接了来三江源数据分析中心安装新采购机器的工程师和学院的运维老师，所以来总结一下这段时间搭建高性能集群的经验。\n硬件 组装 新采购的整机服务器，硬盘、内存条、网卡（Ethernet）、RAID卡都是在的，务必保证启动时，主电源和备用电源都是亮灯状态。机器配置如下，分别是两台Intel双路处理器和两台AMD双路处理器的机器，RDMA网卡是分开采购的Mellanox MT28908（ConnectX-6）。从高性能计算中心借用了8张NVIDIA A800 80GB PCIE，预期拓扑是四机八卡。\nServer CPU Memory Hard Disk H3C UniServer R4900 G6 Intel(R) Xeon(R) GOLD 6542Y(250w)×2 32G DDR5 5600×8 960G SSD×2 + 4TB SATA×3 + RAID PM8204-2G H3C UniServer R4950 G6 AMD EPYC 9654 96-Core Processor(360w)×2 32G DDR5 4800×8 960G SSD×2 + 4TB SATA×3 + RAID PM8204-2G 制作启动盘 需要准备可引导ISO、烧录软件(Rufus或UltraISO)和一个U盘(最好是USB3.0)。这里选择服务器端Ubuntu Server 22.04.5 LTS(上一个Ubuntu Server长期支持版，支持到2027年4月)，在Ubuntu官网获取可引导ISO文件。\n接下来用烧录软件写入硬盘镜像，UltraISO注意选择写入方式为USB-HDD+，Rufus注意选择目标系统类型为BIOS或UEFI，文件系统和其他设置根据需求配置。格式化U盘后即可写入系统镜像（约2-3分钟）。\n软件 网络配置 一般在系统安装时先不配置，安装完成后通过ip link show查看网口名。通过vim /etc/netplan/50-cloud-init.yaml，配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # ... network: ethernets: ens16f0: addresses: - \u0026lt;A.B.C.D\u0026gt;/24 nameservers: addresses: [] search: [] routes: - to: default via: \u0026lt;A.B.C.D\u0026gt; version: 2 设置root密码 1 2 su root passwd root 禁用Linux kernel自动更新 如果不禁用的话,每次apt update后都会提示你是否需要重启服务并更新。修改/etc/apt/apt.conf.d/下的10periodic和20auto-upgrades内的参数为0即可。\nsudo vim /etc/apt/apt.conf.d/10periodic\n更新为:\n1 2 3 APT::Periodic::Update-Package-Lists \u0026#34;0\u0026#34;; APT::Periodic::Download-Upgradeable-Packages \u0026#34;0\u0026#34;; APT::Periodic::AutocleanInterval \u0026#34;0\u0026#34;; sudo vim /etc/apt/apt.conf.d/20auto-upgrades\n更新为:\n1 2 APT::Periodic::Update-Package-Lists \u0026#34;0\u0026#34;; APT::Periodic::Unattended-Upgrade \u0026#34;0\u0026#34;; 反向代理 由于计算节点不联网，因此把本地作为跳板机进行反向代理。\n修改本地Users/username/.ssh/config:\n1 2 3 4 5 Host \u0026lt;hostname\u0026gt; HostName \u0026lt;IP\u0026gt; Port \u0026lt;port\u0026gt; User \u0026lt;username\u0026gt; RemoteForward \u0026lt;Port1\u0026gt; 127.0.0.1:\u0026lt;Port2\u0026gt; 补充一点最近刚学到的，在.ssh/config中通过配置ProxyJump来实现无缝跳板连接的方法。需要注意的是，如果需要免密的话，需要在destination主机的~/.ssh/authorized_keys加入本地公钥。\n1 2 3 4 5 6 7 8 9 10 11 12 Host \u0026lt;hostname\u0026gt; HostName \u0026lt;IP\u0026gt; Port \u0026lt;port\u0026gt; User \u0026lt;username\u0026gt; RemoteForward \u0026lt;Port1\u0026gt; 127.0.0.1:\u0026lt;Port2\u0026gt; Host \u0026lt;destination\u0026gt; HostName \u0026lt;IP\u0026gt; Port \u0026lt;port\u0026gt; User \u0026lt;username\u0026gt; ProxyJump \u0026lt;hostname\u0026gt; RemoteForward \u0026lt;Port1\u0026gt; 127.0.0.1:\u0026lt;Port2\u0026gt; 值得被做的准备 1 2 3 4 5 6 7 8 9 10 11 # 禁用Linux自动休眠 echo \u0026#34;\\$nrconf{kernelhints} = 0;\u0026#34; \u0026gt;\u0026gt; /etc/needrestart/needrestart.conf echo \u0026#34;\\$nrconf{restart} = \u0026#39;l\u0026#39;;\u0026#34; \u0026gt;\u0026gt; /etc/needrestart/needrestart.conf systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target # 更新apt和apt-get apt update apt upgrade apt-get update apt-get upgrade # 安装一些必要组件 apt install git wget vim curl htop net-tools pciutils build-essential 分盘 如果不识别硬盘，可能在系统安装阶段就报错block probing did not discover any disks，检查硬盘、RIAD卡是否亮灯。\n1 2 3 4 5 6 7 8 9 10 11 12 13 lsblk # 确认新硬盘设备名称(如/dev/sdX) sudo pvcreate /dev/sdb /dev/sdc /dev/sdd # 创建物理卷PV sudo vgcreate vg_home /dev/sdb /dev/sdc /dev/sdd # 创建卷组VG vgdisplay # 检查卷组信息 sudo lvcreate -l 100%FREE -n lv_home vg_home # 创建一个逻辑卷，占用卷组的全部空间 sudo mkfs.ext4 /dev/vg_home/lv_home # 格式化逻辑卷为ext4文件系统 sudo mount /dev/vg_home/lv_home /home # 挂载逻辑卷到/home # 自动挂载 sudo blkid /dev/vg_home/lv_home # 获取逻辑卷的UUID sudo vim /etc/fstab \u0026gt;\u0026gt;\u0026gt; UUID=\u0026lt;UUID\u0026gt; /home ext4 defaults 0 2 sudo mount -a df -a # 验证 NFS共享文件系统 1 sudo apt install nfs-kernel-server nfs-common rdma-core # 所有节点都执行 NFS服务器节点 1 2 3 4 5 6 7 8 9 sudo mkdir -p /home sudo chmod 777 /home sudo vim /etc/exports \u0026gt;\u0026gt;\u0026gt; /home *(rw,sync,no_root_squash) sudo vim /etc/nfs.conf # 启用RMDA \u0026gt;\u0026gt;\u0026gt; [nfsd] \u0026gt;\u0026gt;\u0026gt; rdma=y sudo systemctl restart nfs-kernel-server sudo systemctl enable nfs-kernel-server 客户端节点 1 2 3 4 sudo mount -o rdma,vers=4.2 \u0026lt;server_ip\u0026gt;:/home /home df -h | grep /home # 检查挂载是否成功 sudo vim /etc/fstab # 设置开机自动挂载 \u0026gt;\u0026gt;\u0026gt; \u0026lt;server_ip\u0026gt;:/home /home nfs4 rdma,vers=4.2 0 0 验证RMDA传输:\n1 2 mount | grep /home cat /proc/fs/nfsfs/servers # 看transport列是否为rdma InfiniBand Driver 驱动下载：NVIDIA InfiniBand Software | NVIDIA | NVIDIA Developer\nMLNX_OFED：Linux InfiniBand Drivers\n对于旧版本的IB，请务必查看Release Note确认是否支持。\n查看IB设备：\n1 lspci | grep -i mell 打开IB的opensm服务：\n1 2 3 4 5 6 # 如果没有安装 sudo apt update \u0026amp;\u0026amp; sudo apt upgrad -y sudo apt install opensm infiniband-diags ibutils perftest -y sudo systemctl start opensm systemctl status opensm sudo systemctl enable opensm # 开机自启 验证能否正常识别IB设备：\n1 2 ibv_devinfo ibstat 测试服务器和客户端的ib带宽：\n1 2 3 ibv_devices # 查询设备名称, 如mlx5_0 ib_read_bw -a -d \u0026lt;device_name\u0026gt; --report_gbits # 服务器 ib_read_bw -a -F \u0026lt;ip_addr\u0026gt; -d \u0026lt;device_name\u0026gt; --report_gbits # 客户端 (-a测试所有消息大小, -F强制使用服务端连接(需要服务端先启动), --report_gbits以Gbps显示带宽结果) 设置IB的MTU（最大传输单元）：\n1 2 ifconfig | grep ib # 查询 ifconfig ib0 mtu 65520 # 保证两台机器的IB接口配置相同的MTU 简单测试：ibping和ibping \u0026lt;ip_addr\u0026gt;即可\nCUDA 禁用/卸载Nouveau驱动(非必要) 1 sudo vim /etc/modprobe.d/blacklist.conf 在最后两行加入：\n1 2 blacklist nouveau options nouveau modeset=0 重建initramfs并重启服务器使其生效：\n1 2 sudo update-initramfs -u sudo reboot 通过lsmod | grep nouveau验证，若没有输出则禁用成功。\n安装驱动 查看GPU型号：lspci | grep -i nvidia\n驱动下载：NVIDIA Driver\nA800/V100 for CUDA12.6： Data Center Driver for Linux x64 560.35.03 | Linux 64-bit | NVIDIA\n卸载驱动 sudo /usr/bin/nvidia-uninstall\n安装CUDA CUDA：CUDA Toolkit 12.6 Update 3 Downloads | NVIDIA Developer\nCUDA Toolkit：CUDA Toolkit Archive | NVIDIA Developer\nNVCC需要完整的cuda toolkit，只需要在官网找到对应cuda版本的下载版本（建议通过runfile(local)方式），按照提供的命令wget和sudo run即可：\n1 2 3 # 以CUDA Toolkit 12.2为例 wget https://developer.download.nvidia.com/compute/cuda/12.9.1/local_installers/cuda_12.9.1_575.57.08_linux.run sudo sh cuda_12.9.1_575.57.08_linux.run 不需要重新安装驱动，安装完成后写入环境变量：\n1 2 3 echo \u0026#39;export PATH=/usr/local/cuda-12.2/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc 如果望默认用/usr/local/cuda路径访问，也可以创建软链接:\n1 2 3 4 sudo ln -s /usr/local/cuda-12.2 /usr/local/cuda echo \u0026#39;export PATH=/usr/local/cuda/bin:$PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc ","date":"2025-04-23T20:23:31+08:00","image":"https://kaigezheng.github.io/p/ops1/img/cover_hu_287e6a5b8f629599.png","permalink":"https://kaigezheng.github.io/p/ops1/","title":"高性能集群运维——装机"},{"content":"前言 有一个多月没写博客了，在网上冲浪时偶然看到关于无锁队列的blog，突然想到了在ASC25初赛优化hisat-3n-table时面对互斥锁超级加倍的SafeQueue时的头疼，或许能够使用互斥锁来优化，突然就来了兴致学习，因此这篇blog就诞生了。\nPreliminaries CAS (Compare-And-Swap) CAS(Compare-And-Swap, 比较并交换)是并发编程中常用的一种原子操作，广泛用于实现无锁（lock-free）算法，核心思想是：只有当变量的值是预期值时，才将其更新为新值；否则不做任何操作。\nCAS操作涉及三个操作数：\n内存地址V 需要被更新的变量\n旧值A (expected) 当前线程认为变量的值\n新值B (new_val) 希望写入的新值\n计算逻辑如下：\n1 2 3 4 5 6 7 8 9 template \u0026lt;typename T\u0026gt; bool CAS(T* addr, T\u0026amp; expected, const T\u0026amp; new_val) { if(*addr == expected) { *addr = new_val; return true; } new = *addr; return false; } 在GCC中，通过内建函数__sync_bool_compare_and_swap(\u0026amp;addr, expected, new_val)来实现；在C++11后，通过原子操作函数atomic_compare_exchange_strong(\u0026amp;addr, \u0026amp;expected, new_val)（引用expected，CAS失败时会被更新为当前内存值）来实现。\nweak允许伪失败（值匹配时更新失败），性能更高，常用于循环结构；strong不允许失败，性能一般，但安全可靠。\nABA Problem CAS会导致ABA问题，即一个变量的值从A变成B，然后又变回A，而CAS检查时只看到了“值还是A”，误以为没有变化，导致错误的原子更新。\n在引用计数、资源管理（如从栈pop一个节点，然后被别的线程push回去，用旧指针处理时可能会重复删除或释放后访问）等场景，ABA问题会导致资源错误地释放或复用。一种比较简单易懂的解决方法是从“值比较”变成”值+标记比较”，可以通过DWCAS(Double-Width CAS)实现，在64-bit环境下，用双倍大小的指针，在原指针后附加计数器。另一种解决方法是提前分配内存的环形缓冲。\nAtomic Operator 原子操作（Atomic Operator）是指在多线程环境中执行的不可分割的操作，执行过程中不会被中断。这样可以避免竞态条件，实现线程安全。\nTest-And-Set, TAS 常见的原子加锁原语，如果为true则返回true，如果为false则设置为true并返回false，由__sync_lock_test_and_set支持：\n1 2 3 4 5 bool TAS(bool *flag) { bool ret = *flag; *flag = true; return ret; } Compared-And-Swap, CAS\nAtomic Exchange\n用一个新值替换旧值，返回旧值，由__atomic_exchange_n支持：\n1 2 3 4 5 6 template \u0026lt;typename T\u0026gt; T Exchange(T* addr, const T\u0026amp; new_val) { T old_val = *addr; *addr = new_val; return old_val; } Atomic Load/Store 从共享变量中安全读取或写入数据，由__atomic_load_n或atomic_store_n支持：\nAtomic Clear 常与TAS配合使用，实现释放/复位（就是设置为false），由__sync_lock_release支持。\nAtomic Fetch Add/Sub 对指定位置内存的值通过传参进行加减，分别由__sync_fetch_and_add和__sync_fetch_and_sub支持，也可以用__atomic_fetch_add和__atomic_fetch_sub等价。乘和除似乎没有内建函数的支持。\nVolatile Keyword 在C++中，volatile是一个类型修饰符，用于高速编译器该变量的值可能在程序政策控制流程之外被改变（如多线程、硬件中断、特殊寄存器等），因此编译器不应对其进行某些优化，必须每次都从内存中读取值，而不是使用寄存器中的缓存值（禁用常量优化）。\n如在以下的基于CAS的无锁队列出队操作中，需要将_head与head声明为volatile，否则可能会被编译器优化掉_head与head的比较：\n1 2 3 4 5 do{ res = head; newHead = res-\u0026gt;next; } while(!CAS(_head, head, newHead)); 在循环中，尝试获取头指针并后移（弹出当前头指针），CAS(_head, head, newHead)尝试用原子操作将_head从head更新为newHead，这个操作只有在_head == head时才会成功；如果CAS失败（说明_head被其他线程改动过），旧充新读取_head并再次尝试，直到成功\nLock-Free Queue 接下来参考@Clawko和GPT-4o的思路，实现一个基于Double-Width CAS的无锁队列，但不局限于原作者基于Windows/x86-64指令的实现，主要基于Linux/x86-64环境。实现这个无锁队列的过程中也从前人的经验中学习到了很多。完整的实现和带锁队列并发测试代码在GitHub仓库中，建议跳转阅读，欢迎参考与优化。\n核心思路如下：\n用__int128打包队列元素指针（低64位）与计数器（高64位），在入队/出队操作中不会显示地判断计数器情况，而是通过CAS内建函数来隐式地判断\n入队/出队时，当tail被其他线程更新的同时，会进行帮助tail推进的“Lazy Optimization”\n入队/出队的主要实现是循环结构的CAS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 #ifndef LOCKFREEQUEUE_H #define LOCKFREEQUEUE_H #include \u0026lt;atomic\u0026gt; #include \u0026lt;cstdint\u0026gt; #include \u0026lt;cassert\u0026gt; #include \u0026lt;iostream\u0026gt; /* Double-Width CAS Pointer */ struct Pointer { void* ptr; uint64_t count; }; template\u0026lt;typename T\u0026gt; struct Node { T data; std::atomic\u0026lt;Node*\u0026gt; next; Node(T val) : data(val), next(nullptr) {} Node() : next(nullptr) {} // dummy node }; template\u0026lt;typename T\u0026gt; class LockFreeQueue { private: // 使用__int128作为双宽CAS数据载体 typedef __int128 AtomicPointerType; // 头尾指针封装 struct PtrCount { Node\u0026lt;T\u0026gt;* ptr; uint64_t count; }; std::atomic\u0026lt;AtomicPointerType\u0026gt; head; std::atomic\u0026lt;AtomicPointerType\u0026gt; tail; // 将PtrCount打包成__int128 static AtomicPointerType pack(PtrCount pc) { AtomicPointerType val = 0; // 低64位放指针， 高64位放计数器 uint64_t ptrVal = reinterpret_cast\u0026lt;uint64_t\u0026gt;(pc.ptr); val = ptrVal; val |= (AtomicPointerType)pc.count \u0026lt;\u0026lt; 64; return val; } // 将__int128拆成PtrCount static PtrCount unpack(AtomicPointerType val) { PtrCount pc; pc.ptr = reinterpret_cast\u0026lt;Node\u0026lt;T\u0026gt;*\u0026gt;(static_cast\u0026lt;uint64_t\u0026gt;(val \u0026amp; 0xFFFFFFFFFFFFFFFF)); pc.count = static_cast\u0026lt;uint64_t\u0026gt;(val \u0026gt;\u0026gt; 64); return pc; } public: LockFreeQueue() { Node\u0026lt;T\u0026gt;* dummy = new Node\u0026lt;T\u0026gt;(); PtrCount pc{dummy, 0}; head.store(pack(pc)); tail.store(pack(pc)); } ~LockFreeQueue() { PtrCount h = unpack(head.load()); while(h.ptr != nullptr) { Node\u0026lt;T\u0026gt;* next = h.ptr-\u0026gt;next.load(); delete h.ptr; h.ptr = next; } } void enqueue(const T\u0026amp; value) { Node\u0026lt;T\u0026gt;* newNode = new Node\u0026lt;T\u0026gt;(value); PtrCount tailOld; while (true) { // 加载当前的tail并解包为tailOld AtomicPointerType tailVal = tail.load(std::memory_order_acquire); tailOld = unpack(tailVal); // 查看当前尾节点的下一个节点 Node\u0026lt;T\u0026gt;* tailPtr = tailOld.ptr; Node\u0026lt;T\u0026gt;* nextPtr = tailPtr-\u0026gt;next.load(std::memory_order_acquire); // 检查tail是否在这段时间被其他线程更改 if (tailVal == tail.load(std::memory_order_acquire)) { // tail是尾节点 if (nextPtr == nullptr) { // 尾节点next为空，尝试插入 if (tailPtr-\u0026gt;next.compare_exchange_weak(nextPtr, newNode, std::memory_order_release, std::memory_order_relaxed)) { PtrCount newTail{newNode, tailOld.count + 1}; tail.compare_exchange_strong(tailVal, pack(newTail), std::memory_order_release, std::memory_order_relaxed); return; } } else { // tail不是尾节点，落后了 -\u0026gt; 帮助推进tail PtrCount newTail{nextPtr, tailOld.count + 1}; tail.compare_exchange_strong(tailVal, pack(newTail), std::memory_order_release, std::memory_order_relaxed); } } } } bool dequeue(T\u0026amp; result) { PtrCount headOld; while(true) { AtomicPointerType headVal = head.load(std::memory_order_acquire); AtomicPointerType tailVal = tail.load(std::memory_order_acquire); headOld = unpack(headVal); PtrCount tailOld = unpack(tailVal); Node\u0026lt;T\u0026gt;* headPtr = headOld.ptr; Node\u0026lt;T\u0026gt;* tailPtr = tailOld.ptr; Node\u0026lt;T\u0026gt;* nextPtr = headPtr-\u0026gt;next.load(std::memory_order_acquire); if (headVal == head.load(std::memory_order_acquire)) { if (headPtr == tailPtr) { if(nextPtr == nullptr) { // head与tail指向同一节点，且dummy的next为空 -\u0026gt; 队列空 return false; } // 尾指针落后了，帮助推进tail PtrCount newTail{nextPtr, tailOld.count + 1}; tail.compare_exchange_strong(tailVal, pack(newTail), std::memory_order_release, std::memory_order_relaxed); } else { // 读取数据准备出队 result = nextPtr-\u0026gt;data; PtrCount newHead{nextPtr, headOld.count + 1}; if (head.compare_exchange_strong(headVal, pack(newHead), std::memory_order_release, std::memory_order_relaxed)) { delete headPtr; // 释放旧dummy节点 return true; } } } } } }; #endif 简单写了一个并发环境，能够比通过std::lock_guard实现的LockedQueue快一些，具体性能提升效果取决于并发压力：\n1 2 [LockedQueue] Total consumed: 400000, time: 0.0363734 seconds [LockFreeQueue] Total consumed: 400000, time: 0.121427 seconds Reference 知乎：迈向多线程——解析无锁队列的原理与实现 - Clawko\n","date":"2025-07-30T00:23:00+08:00","image":"https://kaigezheng.github.io/p/hpc1/img/cover_hu_ef6cd1d77ae8815d.png","permalink":"https://kaigezheng.github.io/p/hpc1/","title":"并发环境下的队列优化——无锁队列"},{"content":"本篇博文介绍集群常用版本管理软件MODULES，以及MPI的两种实现（UCX+OpenMPI/MPICH）、Intel oneAPI的安装配置。MODULES（最新版的发行版发布自24年11月）的软件依赖繁琐且modulefile需要用TCL写（可以用生成式AI解决），未来有机会学习一下更易用的spack。\nMODULES (v5.4.0) 参考 TCL官网\nMODULES安装文档\nGCC官方镜像站\nGCC构建指南\n[ Module ] 环境变量管理工具 Modules 安装和使用 - YEUNGCHIE\nmodule使用 - 北京大学高性能计算校级公共平台用户文档\n安装依赖 TCL (\u0026gt;=v8.5) 1 2 3 4 5 6 7 8 9 sudo wget http://prdownloads.sourceforge.net/tcl/tcl8.6.14-src.tar.gz sudo tar -zxvf tcl8.6.14-src.tar.gz cd tcl8.6.14/unix sudo ./configure --prefix=/usr/local sudo make sudo make install sudo whereis tcl sudo ln /usr/local/bin/tclsh8.6 /usr/bin/tclsh GMP 1 2 3 4 5 6 7 8 sudo wget ftp://ftp.gnu.org/gnu/gmp/gmp-5.0.1.tar.bz2 sudo tar -vxf gmp-5.0.1.tar.bz2 cd gmp-5.0.1/ sudo ./configure --prefix=/usr/local/gmp-5.0.1 sudo make sudo make install sudo make check \u0026gt;\u0026gt;\u0026gt; All 30 tests passed... MPFR (buggy but acceptable) 1 2 3 4 5 6 sudo wget https://ftp.gnu.org/gnu/mpfr/mpfr-3.1.5.tar.xz sudo tar -vxf mpfr-3.1.5.tar.gz cd mpfr-3.1.5/ sudo ./configure --prefix=/usr/local/mpfr-3.1.5 --with-gmp=/usr/local/gmp-5.0.1 sudo make sudo make install MPC 1 2 3 4 5 6 sudo wget http://www.multiprecision.org/downloads/mpc-0.9.tar.gz sudo tar -vxf mpc-0.9.tar.gz cd mpc-0.9/ sudo ./configure --prefix=/usr/local/mpc-0.9 --with-gmp=/usr/local/gmp-5.0.1/ --with-mpfr=/usr/local/mpfr-3.1.5/ sudo make sudo make install 安装MODULES 1 2 3 4 5 6 7 8 9 sudo curl -LJO https://github.com/cea-hpc/modules/releases/download/v5.4.0/modules-5.4.0.tar.gz sudo tar xfz modules-5.4.0.tar.gz sudo ./configure --with-tcl=/usr/local/lib --prefix=/home/Modules --modulefilesdir=/home/modulefiles sudo make sudo make install sudo ln -s /home/Modules/init/profile.sh /etc/profile.d/module.sh sudo ln -s /home/Modules/init/profile.csh /etc/profile.d/module.csh source /home/Modules/init/profile.sh # 建议写入/etc/profile，否则每次进入shell需要手动初始化(`source /home/Modules/init/profile.sh`) Note因为某些特殊的原因，我们不得不将MODULES和其他软件安装在/home目录下 /home/moduledownload/暂时存放TCL（8.6）和MODULE（5.4.0）的安装包等 /home/Module/存放Module的实际文件，内含初始化文件（已做软链接） /home/modulefiles/存放各个软件的版本文件（modulefile），第二级文件为软件名，第三级文件为版本号文本 /home/apps/存放实际软件\nMPI 参考 MPICH官方镜像站\nOpenMPI v5.0.0\nUCX仓库\n编译安装 UCX 1.15.0 与 OpenMPI 5.0.0：详尽指南\n安装UCX (optional) Unified Communication X 1 2 3 4 5 6 7 wget https://github.com/openucx/ucx/releases/download/v1.15.0/ucx-1.15.0.tar.gz tar -xvzf ucx-1.15.0.tar.gz cd ucx-1.15.0 mkdir build \u0026amp; cd build ../configure --prefix=/home/zhengkaige/ucx make -j N make install 安装MPICH (v4.2.2) 1 2 3 4 5 tar -xvzf mpich-4.2.2.tar.gz cd mpich-4.2.2 ./configure --prefix=/home/apps/MPICH/4.2.2 make make install 可能遇到报错：configure: error: UCX installation does not meet minimum version requirement (v1.9.0). Please upgrade your installation, or use --with-ucx=embedded.\n安装OpenMPI (v5.0.0) 1 2 3 4 wget https://download.open-mpi.org/release/open-mpi/v5.0/openmpi-5.0.0.tar.gz tar -xzvf openmpi-5.0.0.tar.gz cd openmpi-5.0.0 mkdir build \u0026amp;\u0026amp; cd build vim ~/.bashrc\n1 2 export PATH=/home/kambri/software/openmpi/5.0.0-ucx-1.15.0/bin:$PATH export LD_LIBRARY_PATH=/home/kambri/software/openmpi/5.0.0-ucx-1.15.0/lib:$LD_LIBRARY_PATH 编写Modulefile 1 2 3 4 5 6 #%Module set version 4.2.2 set MPI_HOME /home/apps/MPICH/4.2.2 prepend-path PATH \u0026#34;${MPI_HOME}/bin\u0026#34; prepend-path LD_LIBRARY_PATH \u0026#34;${MPI_HOME}/lib\u0026#34; prepend-path MANPATH \u0026#34;${MPI_HOME}/share/man\u0026#34; Intel oneAPI 参考 Developer Toolkits\nGet the Intel® oneAPI Base Toolkit\nGet Intel® oneAPI HPC Toolkit\nIntel oneAPI镜像站\n安装Intel oneAPI(v2025.0 including Base Toolkit and HPC Toolkit) 按照官方的offline installation方式下载安装即可。需要注意的是Intel更新oneAPI时会移除老版本界面，因此安装老版本时需要靠镜像站等途径。但新版本又不好用，如2025.0的mpiicc仍然使用icc作为compiler，但是2025.0（包括2023后期版本和2024.x）的套件里都已不包含icc了。icc已在2023下半年发布的oneAPI中被移除。\n1 2 3 4 5 6 # install base toolkit wget https://registrationcenter-download.intel.com/akdlm/IRC_NAS/dfc4a434-838c-4450-a6fe-2fa903b75aa7/intel-oneapi-base-toolkit-2025.0.1.46_offline.sh sudo sh ./intel-oneapi-base-toolkit-2025.0.1.46_offline.sh -a --silent --cli --eula accept # install HPC toolkit wget https://registrationcenter-download.intel.com/akdlm/IRC_NAS/b7f71cf2-8157-4393-abae-8cea815509f7/intel-oneapi-hpc-toolkit-2025.0.1.47_offline.sh sudo sh ./intel-oneapi-hpc-toolkit-2025.0.1.47_offline.sh -a --silent --cli --eula accept ","date":"2025-05-11T19:47:14+08:00","image":"https://kaigezheng.github.io/p/ops2/img/cover2_hu_e475b465e2a6cd26.png","permalink":"https://kaigezheng.github.io/p/ops2/","title":"高性能集群运维——软件环境（Modules、MPI、oneAPI）"},{"content":"Reference 教程 | Triton 中文站\nTutorials — Triton documentation\n并行编程语言（Triton \u0026amp; 九齿） - 2024 冬季大模型与人工智能系统训练营\nVector Addition Compute Kernel 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import torch import triton import triton.language as tl DEVICE = torch.device(\u0026#34;cuda:0\u0026#34;) @triton.jit def add_kernel(x_ptr, # *Pointer* to first input vector. y_ptr, # *Pointer* to second input vector. output_ptr, # *Pointer* to output vector. n_elements, # Size of the vector. BLOCK_SIZE: tl.constexpr, # Number of elements each program should process. # NOTE: `constexpr` so it can be used as a shape value. ): pid = tl.program_id(axis=0) # We use a 1D launch grid so axis is 0. block_start = pid * BLOCK_SIZE # Offsets is a list of pointers offsets = block_start + tl.arange(0, BLOCK_SIZE) mask = offsets \u0026lt; n_elements # Load x and y from DRAM x = tl.load(x_ptr + offsets, mask=mask) y = tl.load(y_ptr + offsets, mask=mask) output = x + y # Write x + y back to DRAM. tl.store(output_ptr + offsets, output, mask=mask) 编译遇到的第一个报错是AttributeError: 'CudaDriver' object has no attribute 'get_active_torch_device'，在issue#5388下找到了原因和解决方案，DEVICE = triton.runtime.driver.active.get_active_torch_device()似乎对应了旧本版本，用DEVICE = torch.device(\u0026quot;cuda:0\u0026quot;)可以暂缓燃眉之急。\nkernel的实现思路是简单的：\n对于输入(input_x, input_y, output, n, BLOCK_SIZE)，获取programd_id，通过pid * BLOCK_SIZE计算分块索引，通过block_start + tl.arange(0, BLOCK_SIZE)确定每个元素的索引\n确定mask，防止边界溢出\n加载输入元素，实现向量加法，最后写回DRAM\n接下来需要一个合适的辅助函数，负责生成output张量和创建合适的layout\n1 2 3 4 5 6 7 8 9 def add(x: torch.Tensor, y: torch.Tensor): output = torch.empty_like(x) # preallocate the output assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE n_elements = output.numel() grid = lambda meta: (triton.cdiv(n_elements, meta[\u0026#39;BLOCK_SIZE\u0026#39;]), ) add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024) # We return a handle to z but, since `torch.cuda.synchronize()` hasn\u0026#39;t been called, the kernel is still # running asynchronously at this point. return output 核心只有两行代码，\ngrid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )定义一个grid匿名函数，用triton.cdiv计算$\\lceil\\frac{n}{BLOCKSIZE}\\rceil$\nadd_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)调用grid(meta)来确定网格大小并启动GPU线程块，并将(x, y, output, n_elements, BLOCK_SIZE=1024)传递给kernel\n1 2 3 4 5 6 7 8 9 10 torch.manual_seed(0) size = 984320000 x = torch.rand(size, device=DEVICE) y = torch.rand(size, device=DEVICE) output_torch = x + y output_triton = add(x, y) print(output_torch) print(output_triton) print(f\u0026#39;The maximum difference between torch and triton is \u0026#39; f\u0026#39;{torch.max(torch.abs(output_torch - output_triton))}\u0026#39;) Benchmark 这个设计真的非常喜欢，不用费事去写一些benchmark测试代码了！\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @triton.testing.perf_report( triton.testing.Benchmark( x_names=[\u0026#39;size\u0026#39;], # Argument names to use as an x-axis for the plot. 用作绘图 x 轴的参数名称。 x_vals=[2**i for i in range(12, 28, 1)], # Different possible values for `x_name`. `x_name` 的不同可能值。 x_log=True, # x axis is logarithmic. x 轴为对数。 line_arg=\u0026#39;provider\u0026#39;, # Argument name whose value corresponds to a different line in the plot. 参数名称，其值对应于绘图中的不同线条。 line_vals=[\u0026#39;triton\u0026#39;, \u0026#39;torch\u0026#39;], # Possible values for `line_arg`. `line_arg` 的可能值。 line_names=[\u0026#39;Triton\u0026#39;, \u0026#39;Torch\u0026#39;], # Label name for the lines. 线条的标签名称。 styles=[(\u0026#39;blue\u0026#39;, \u0026#39;-\u0026#39;), (\u0026#39;green\u0026#39;, \u0026#39;-\u0026#39;)], # Line styles. 线条样式。 ylabel=\u0026#39;GB/s\u0026#39;, # Label name for the y-axis. y 轴标签名称。 plot_name=\u0026#39;vector-add-performance\u0026#39;, # Name for the plot. Used also as a file name for saving the plot. 绘图名称。也用作保存绘图的文件名。 args={}, # Values for function arguments not in `x_names` and `y_name`. 不在 `x_names` 和 `y_name` 中的函数参数值。 )) def benchmark(size, provider): x = torch.rand(size, device=\u0026#39;cuda\u0026#39;, dtype=torch.float32) y = torch.rand(size, device=\u0026#39;cuda\u0026#39;, dtype=torch.float32) quantiles = [0.5, 0.2, 0.8] if provider == \u0026#39;torch\u0026#39;: ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles) if provider == \u0026#39;triton\u0026#39;: ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles) gbps = lambda ms: 3 * x.numel() * x.element_size() / ms * 1e-6 return gbps(ms), gbps(max_ms), gbps(min_ms) benchmark.run(print_data=True, show_plots=True, save_path=\u0026#39;./\u0026#39;) 以下是单卡Tesla V100-PCIE-32GB的benchmark结果，可以看出和Torch还是有一些差距的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 vector-add-performance: size Triton Torch 0 4096.0 9.600000 9.600000 1 8192.0 19.200000 19.200000 2 16384.0 38.400001 38.400001 3 32768.0 76.800002 76.800002 4 65536.0 127.999995 127.999995 5 131072.0 219.428568 219.428568 6 262144.0 341.333321 341.333321 7 524288.0 472.615390 474.898540 8 1048576.0 614.400016 614.400016 9 2097152.0 702.171410 702.171410 10 4194304.0 756.184613 756.184613 11 8388608.0 792.974002 793.173993 12 16777216.0 812.429770 815.800825 13 33554432.0 822.627612 824.352211 14 67108864.0 827.823144 828.695462 15 134217728.0 830.445624 831.344043 Fused Softmax Naive Softmax $$softmax(z_{i})=\\frac{e^{z_{i}}}{\\Sigma_{j=1}^{n}e^{z_{j}}}=\\frac{exp(z_{i})}{\\Sigma_{j=1}^{n}exp(z_{j})}$$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 def naive_softmax(x): \u0026#34;\u0026#34;\u0026#34;Compute row-wise softmax of X using native pytorch \u0026#34;\u0026#34;\u0026#34; # read MN elements ; write M elements x_max = x.max(dim=1)[0] # read MN + M elements ; write MN elements z = x - x_max[:, None] # read MN elements ; write MN elements numerator = torch.exp(z) # read MN elements ; write M elements denominator = numerator.sum(dim=1) # read MN + M elements ; write MN elements ret = numerator / denominator[:, None] # in total: read 5MN + 2M elements ; wrote 3MN + 2M elements return ret 由于softmax具有平移不变性，而exp(x)容易上溢，因此需要减去最大元素： $$softmax(x)=softmax(x+c)$$ 对于naive的softmax实现，需要从DRAM中读取$5MN+2M$个元素，并写回$3MN+2M$个元素，IO共需要$8MN+4M$个单位，显然是不理想的。理想情况下，只读写一次并完成所有计算，因此可以得到理想加速约为4倍（$\\frac{8MN+4M}{2MN}$）。\nCompute Kernel softmax kernel需要补充的内容都写在注释里了：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 @triton.jit def softmax_kernel(output_ptr, input_ptr, input_row_stride, output_row_stride, n_rows, n_cols, BLOCK_SIZE: tl.constexpr, num_stages: tl.constexpr): # kernel思路 : 让多个program并行处理不同的行 row_start = tl.program_id(0) # program ID row_step = tl.num_programs(0) # program 数量 # 遍历每一行, tl.range(..)行索引list ## 线程0处理:[0, row_step, 2 * row_step, ...] ## 线程1处理:[1, 1 + row_step, 1 + 2 * row_step, ...] ## 线程2处理:[2, 2 + row_step, 2 + 2 * row_step, ...] ## ... **实现了不同program处理不同row** for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages): # row_start_ptr指向当前行的首个元素 row_start_ptr = input_ptr + row_idx * input_row_stride # 计算列索引 col_offsets = tl.arange(0, BLOCK_SIZE) # 列偏移, 生成[0, BLOCK_SIZE - 1] input_ptrs = row_start_ptr + col_offsets # 当前行首 + 列偏移 # 计算mask mask = col_offsets \u0026lt; n_cols # 注意是用列偏移和列数生成的mask row = tl.load(input_ptrs, mask=mask, other=-float(\u0026#39;inf\u0026#39;)) # 根据mask读取内存, 超出n_cols的部分填充为-∞ row_minus_max = row - tl.max(row, axis=0) # softmax numerator = tl.exp(row_minus_max) denominator = tl.sum(numerator, axis=0) softmax_output = numerator / denominator # Write back output to DRAM output_row_start_ptr = output_ptr + row_idx * output_row_stride # 计算输出索引 output_ptrs = output_row_start_ptr + col_offsets # 计算输出内存地址 tl.store(output_ptrs, softmax_output, mask=mask) 认真看完源码后，会发现softmax kernel的实现思路并不难，每个program处理输入矩阵的一组行（按program数量跨步处理），执行完softmax操作后写回DRAM。\nNote: Triton的一个重要限制是BLOCK_SIZE必须是$2^n$的元素\n同样，我们需要一个辅助函数。在此之前，triton.runtime.driver.active.utils.get_device_properties(torch.cuda.current_device())非常有用，能够帮助我们收集必要的计算卡硬件信息。\n1 2 3 4 5 6 7 8 device = torch.cuda.current_device() properties = driver.active.utils.get_device_properties(device) NUM_SM = properties[\u0026#34;multiprocessor_count\u0026#34;] NUM_REGS = properties[\u0026#34;max_num_regs\u0026#34;] SIZE_SMEM = properties[\u0026#34;max_shared_mem\u0026#34;] WARP_SIZE = properties[\u0026#34;warpSize\u0026#34;] target = triton.runtime.driver.active.get_current_target() kernels = {} Tesla V100-PCIE-32GB的输出如下：\n1 2 3 4 5 6 7 8 9 { \u0026#39;max_shared_mem\u0026#39;: 98304, # 共享内存 \u0026#39;max_num_regs\u0026#39;: 65536, # 最大寄存器数 \u0026#39;multiprocessor_count\u0026#39;: 80, # SM 数量 \u0026#39;warpSize\u0026#39;: 32, # WARP 大小 \u0026#39;sm_clock_rate\u0026#39;: 1380000, # SM 频率 (Hz) \u0026#39;mem_clock_rate\u0026#39;: 877000, # 内存频率 (Hz) \u0026#39;mem_bus_width\u0026#39;: 4096 # 内存总线宽度 (bit) } 辅助函数如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def softmax(x): n_rows, n_cols = x.shape # BLOCK_SIZE是大于矩阵列数的最小二次幂 BLOCK_SIZE = triton.next_power_of_2(n_cols) num_warps = 8 # 如果 GPU 共享内存（SMEM）足够大（\u0026gt;200KB），就用 4 个流水线阶段，否则用 2 个 num_stages = 4 if SIZE_SMEM \u0026gt; 200000 else 2 # Allocate output y = torch.empty_like(x) # pre-compile kernel to get register usage and compute thread occupancy. # 预编译内核以获取寄存器使用情况并计算线程占用情况。 kernel, num_programs = kernels.get(BLOCK_SIZE, (None, 0)) if kernel is None: kernel = softmax_kernel.warmup(y, x, x.stride(0), y.stride(0), n_rows, n_cols, BLOCK_SIZE=BLOCK_SIZE, num_stages=num_stages, num_warps=num_warps, grid=(1, )) kernel._init_handles() n_regs = kernel.n_regs size_smem = kernel.metadata.shared occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps) occupancy = min(occupancy, SIZE_SMEM // size_smem) num_programs = NUM_SM * occupancy kernels[BLOCK_SIZE] = (kernel, num_programs) num_programs = min(num_programs, n_rows) # Create a number of persistent programs. # 创建一些持久化程序。 kernel[(num_programs, 1, 1)]( y, x, x.stride(0), y.stride(0), n_rows, n_cols, ) return y 在预编译kernel前需要确定BLOCK_SIZE大小、warp数量、流水线阶段并分配输出tensor空间。更多的流水线阶段可以提高并行效率，但会消耗更多的shared memory。\n在预编译阶段，softmax_kernel.warmup(...)预编译softmax kernel，用于获取寄存器使用情况、shared memory使用量、线程占用率，并通过kernel._init_handles()初始化kernel的CUDA句柄。最终计算num_programs（SM数量×线程占用率）并保证安全运行kernel。\nWhy Fused Softmax? 读完Fused Softmax Tutorial的代码后，我不太理解为什么这个kernel被成为融合Softmax（Fused Softmax），为什么将原本$\\Theta(8MN+4M)$的IO开销缩减到了一次读写，为什么能够比PyTorch的实现更快（明明softmax的步骤是一样的）。\n除了用到了pipelining优化了计算流程，主要是在PyTorch传统softmax实现中，涉及多个kernel启动并在全局现存（global memory）之间频繁读写数据，导致了额外的内存访问开销和kernel启动开销（计算最大值，计算指数函数，行归一化，输出）。而用Triton写的Fused Softmax kernel编写了一个自定义CUDA kernel，在单个kernel内部完成了所有计算的步骤，也可以更好地利用shared memory和registers。\nMatrix Multiplication Blocked Matrix Multiplication 对于一个以$(BLOCKSIZEM, BLOCKSIZEK, BLOCKSIZEN)$为分块大小的，$(M, K)$和$(K, N)$相乘的分块矩阵乘法如下：\n1 2 3 4 5 6 7 8 9 10 # Do in parallel for m in range(0, M, BLOCK_SIZE_M): # Do in parallel for n in range(0, N, BLOCK_SIZE_N): acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32) for k in range(0, K, BLOCK_SIZE_K): a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K] b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N] acc += dot(a, b) C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc 这里$C[\u0026hellip;] = acc$是正确的，因为已经遍历完k维度了\nCompute Kernel 多维指针 对于行主序的二维张量$X$，$X[i, j]$的内存位置由\u0026amp;X[i, j] = X + i * stride_xi + j * stride_xj给出，因此$A[m : m + BLOCKSIZEM, k : k + BLOCKSIZEK]$和$B[k : k + BLOCKSIZEK, n : n + BLOCKSIZEN]$的指针块可以用伪代码定义为：\n1 2 \u0026amp;A[m : m+BLOCK_SIZE_M, k:k+BLOCK_SIZE_K] = a_ptr + (m : m+BLOCK_SIZE_M)[:, None]*A.stride(0) + (k : k+BLOCK_SIZE_K)[None, :]*A.stride(1); \u0026amp;B[k : k+BLOCK_SIZE_K, n:n+BLOCK_SIZE_N] = b_ptr + (k : k+BLOCK_SIZE_K)[:, None]*B.stride(0) + (n : n+BLOCK_SIZE_N)[None, :]*B.stride(1); 其中，[:, None]和[None, :]是用于扩展数组维度的操作，通常用于广播。（注意，stride(x)的单位是字节）\n[:, None]表示在数组的第二个维度（列维度）上增加一个大小为1的维度，将形状为(n,)的数组变为(n, 1)\n[None, :]表示在数组的第一个维度（行维度）上增加一个大小为1的维度，将形状为(n,)的数组变为(1, n)\n接下来计算offset：\n1 2 3 4 5 offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N offs_k = tl.arange(0, BLOCK_SIZE_K) a_ptrs = a_ptr + (offs_am[:, None]*stide_am + offs_k[None, :]*stride_ak) b_ptrs = b_ptr + (offs_k[:, None]*stride_bk + offs_bn[None, :]*stride_bn) 在内循环中更新：\n1 2 a_ptrs += BLOCK_SIZE_K * stride_ak b_ptrs += BLOCK_SIZE_K * stride_bk L2 Cache Optimization 对于$N \\times N$的矩阵（以$BLOCKSIZEN$为分块大小），可以通过以下代码将一维的program_id线程块索引映射到二维的块索引，从而确定当前program负责计算结果矩阵$C$的哪一块。\n1 2 3 4 pid = tl.program_id(axis=0) grid_n = tl.cdiv(N, BLOCK_SIZE_N) pid_m = pid // grid_n pid_n = pid % grid_n 每个程序实例计算$C$的一个$[BLOCKSIZEM, BLOCKSIZEN]$块，简单的行主序排序是行不通的。\n可以实现以下的分组（Grouping）优化，目的是为了优化内存访问模式：\n1 2 3 4 5 6 7 8 9 10 pid = tl.program_id(axis=0) num_pid_m = tl.cdiv(M, BLOCK_SIZE_M) # M轴program数量 num_pid_n = tl.cdiv(N, BLOCK_SIZE_N) # N轴program数量 num_pid_in_group = GROUP_SIZE_M * num_pid_n # 在每个GROUP_M中的program数量 group_id = pid // num_pid_in_group # group id first_pid_m = group_id * GROUP_SIZE_M # 组内第一个program的行id group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M) # 最后一组偏小 # 在group内, program按**列主序**排序 pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m) # 启动网格中的行id pid_n = (pid % num_pid_in_group) // group_size_m # 启动网格中的列id （Triton Matmul的结果还有点问题，日后再更新）\n","date":"2025-03-06T20:22:10+08:00","image":"https://kaigezheng.github.io/p/triton1/img/cover_hu_debe95e87945669a.jpg","permalink":"https://kaigezheng.github.io/p/triton1/","title":"Triton学习——Vector Addition, Fused Softmax, Matrix Multiplication"},{"content":"本文写于2024-2-23至2024-3-4。\n参考资料 我学习MPI时主要参考MPI Tutorial，部分参考《An Introduction to Parallel Programming》。MPI Tutorial的内容由浅入深，配套了简单但实用的程序案例，支持中文，非常适合入门；《An Introduction to Parallel Programming》只适合补充，中文的翻译挺一般的。\nMPI Tutorial\n《An Introduction to Parallel Programming》\nMPI基础 编译 mpicc -o output input：使用mpich自带的脚本进行编译和链接，mpicc、mpicxx、mpifort分别对应C、C++、Fortran\n运行 在本地机器上运行number个进程的程序：mpiexec -n \u0026lt;number\u0026gt; ./output 在多个节点上运行number个进程的程序：mpiexec -f machinefile -n \u0026lt;number\u0026gt; ./output mpirun是MPI的实现用来启动任务的一个程序，进程会在host文件里指定的所有机器上面生成，MPI程序就会在所有进程上面运行。-n参数告诉MPI程序要运行\u0026lt;number\u0026gt;个进程。\n编程 引入头文件和初始化 MPI环境必须以MPI_Init(int* argc, char*** argv)来初始化。 在MPI_Init的过程中，所有MPI的全局变量或者内部变量都会被创建。举例：一个communicator会根据所有可用的进程被创建出来（进程是通过mpi运行时的参数指定的），每个进程会被分配独一无二的rank。\n函数调用 1 2 3 MPI_Comm_size( MPI_Comm communicator, int* size) MPI_Comm_size会返回communicator的可用进程数量，MPI_COMM_WORLD（这个communicator是MPI帮忙生成的）这个变量包含了当前MPI任务中所有的进程，因此这个调用会返回所有的可用进程数目。 MPI_COMM_WORLD是预定义的、所有进程的默认通信器，当MPI程序启动时，每个进程都会加入这个通信器，可通过MPI_Comm_rank得到每个进程的唯一标识符。 1 2 3 MPI_Comm_rank( MPI_Comm name, int* name_length) MPI_Comm_rank会返回communicator中当前进程的rank，communicator中每个进程会以此得到一个从0开始递增的数字作为rank值（主要用来指定发送或接受信息时对应的进程）。 1 2 3 MPI_Get_processor_name( char* name, int* name_Length) MPI_Get_processor_name会得到当前进程实际跑的时候所在的处理器名。 MPI_Get_Processor_name(processor_name, \u0026amp;name_len)将处理器名存储在processor_name中，并更新name_len（存储处理器名称的实际长度）。 1 MPI_Finalize(void) MPI_Finalize是用来清理MPI环境的，被调用后就没有MPI函数可以被调用了。 Hello World 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #include \u0026lt;mpi.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; int main(int argc, char** argv){ MPI_Init(NULL, NULL); int world_size; MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;world_size); int world_rank; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;world_rank); char processor_name[MPI_MAX_PROCESSOR_NAME]; int name_len; MPI_Get_processor_name(processor_name, \u0026amp;name_len); printf(\u0026#34;Hello world from processor %s, rank %d out of %d processors\\n\u0026#34;, processor_name, world_rank, world_size); MPI_Finalize(); } MPI的发送和接收 MPI Send and Receive A进程决定发送一些消息给B进程，将需要发送的数据打包放入缓存，根据特定的rank确定发送的进程 B需要确认接收A的数据，A会接收到数据传递成功的信息 有时A需要传递很多不同消息，为了让B更方便地区别不同消息，MPI运行发送者和接受者额外地指定一些信息ID（标签，tags），当B只要求接收某种特定标签地信息时，其他非该标签地信息会先被缓存直到B需要 MPI_Send和MPI_Recv方法定义 1 2 3 4 5 6 7 MPI_Send( void* data, //数据缓存 int count, //数据数量（发送） MPI_Datatype datatype, //数据类型 int destination, //发送方进程rank int tag, //信息标签 MPI_Comm communicator) 1 2 3 4 5 6 7 8 MPI_Recv( void* data, int count, //数据数量（**最多**接收） MPI_Datatype datatype, int source, //接收方进程rank int tag, MPI_Comm communicator, MPI_Status* status) //可以是MPI_STATUS_IGNORE 代码 简单的P2P通信 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #include \u0026lt;mpi.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int world_rank; MPI_Comm_rank(MPI_COMM_WORLD, \u0026amp;world_rank); int world_size; MPI_Comm_size(MPI_COMM_WORLD, \u0026amp;world_size); int number; if (world_rank == 0) { number = -1; MPI_Send(\u0026amp;number, 1, MPI_INT, 1, 0, MPI_COMM_WORLD); } else if (world_rank == 1) { MPI_Recv(\u0026amp;number, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); //tag=MPI_ANY_TAG printf(\u0026#34;Process 1 received number %d from process 0\\n\u0026#34;, number); } 若当前进程是0进程，那么初始化一个数字-1通过MPI_Send以MPI_INT数据类型发送给1进程 在else if中，进程1会调用MPI_Recv接收这个数字并打印 每个进程使用了0作为消息标签来指定消息（由于这里只有一种类型地消息被传递，因此进程也可以使用预先定义好的常量MPI_ANY_TAG来作为tag） 乒乓程序-循环P2P通信 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int ping_pong_count = 0; int partner_rank = (world_rank + 1) % 2; while (ping_pong_count \u0026lt; PING_PONG_LIMIT) { if (world_rank == ping_pong_count % 2) { // Increment the ping pong count before you send it ping_pong_count++; MPI_Send(\u0026amp;ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD); printf(\u0026#34;%d sent and incremented ping_pong_count %d to %d\\n\u0026#34;, world_rank, ping_pong_count, partner_rank); } else { MPI_Recv(\u0026amp;ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); printf(\u0026#34;%d received ping_pong_count %d from %d\\n\u0026#34;, world_rank, ping_pong_count, partner_rank); } } 在两个进程中，ping_pong_count在每次发送消息后递增，随着ping_pong_count的递增，两个进程会轮流成为发送者和接收者直到limit被触发 环通信（重要） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 int token; if (world_rank != 0) { MPI_Recv(\u0026amp;token, 1, MPI_INT, world_rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); printf(\u0026#34;Process %d received token %d from process %d\\n\u0026#34;, world_rank, token, world_rank - 1); } else { // Set the token\u0026#39;s value if you are process 0 token = -1; } MPI_Send(\u0026amp;token, 1, MPI_INT, (world_rank + 1) % world_size, 0, MPI_COMM_WORLD); // Now process 0 can receive from the last process. if (world_rank == 0) { MPI_Recv(\u0026amp;token, 1, MPI_INT, world_size - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); printf(\u0026#34;Process %d received token %d from process %d\\n\u0026#34;, world_rank, token, world_size - 1); } 在进程0上初始化了token = -1，然后这个值会一次传递给每个进程，程序会在最后一个进程接收到值后结束 对于进程0：保证了在想要接收数据之前发送了token 其他进程：仅仅调用MPI_Recv并调用MPI_Send MPI_Send和MPI_Recv会阻塞直到数据传递完成，避免了死锁 动态接收消息 MPI_Status结构体 MPI_Recv将MPI_Status结构体地地址作为参数（可以使用MPI_STATUS_IGNORE忽略）。如果将MPI_Status结构体传递给MPI_Recv函数，则操作完成后将在该结构体中填充有关接收操作地其他信息，包括：\n发送端rank：存储在结构体的MPI_SOURCE元素中，如声明一个MPI_Status stat变量，则可以通过stat.MPI_SOURCE访问rank 消息的tag：通过MPI_TAG元素访问 消息的长度：没有预定义的元素，必须使用MPI_Get_count找出消息的长度 1 2 3 4 MPI_Get_count( MPI_Sratus* status, MPI_Datatype datatype, int* count) WHY？MPI_Recv可以将MPI_ANY_SOURCE用作发送端的rank，将MPI_ANY_TAG用作消息的tag。此时，MPI_Status就是找出消息的实际发送端和tag的唯一方法。此外，并不能保证MPI_Recv能够接收函数调用参数的全部元素；相反，它只接收已发送给它的元素数量（如发送的元素多于所需的接收数量则返回错误），而MPI_Get_count函数用于确定实际的接收量。\nMPI_Status结构体查询的示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 const int MAX_NUMBERS = 100; int numbers[MAX_NUMBERS]; int number_amount; if (world_rank == 0) { // Pick a random amount of integers to send to process one srand(time(NULL)); number_amount = (rand() / (float)RAND_MAX) * MAX_NUMBERS; // Send the amount of integers to process one MPI_Send(numbers, number_amount, MPI_INT, 1, 0, MPI_COMM_WORLD); printf(\u0026#34;0 sent %d numbers to 1\\n\u0026#34;, number_amount); } else if (world_rank == 1) { MPI_Status status; // Receive at most MAX_NUMBERS from process zero MPI_Recv(numbers, MAX_NUMBERS, MPI_INT, 0, 0, MPI_COMM_WORLD,\u0026amp;status);//！ // After receiving the message, check the status to determine // how many numbers were actually received MPI_Get_count(\u0026amp;status, MPI_INT, \u0026amp;number_amount);//！ // Print off the amount of numbers, and also print additional // information in the status object printf(\u0026#34;1 received %d numbers from 0. Message source = %d, \u0026#34; \u0026#34;tag = %d\\n\u0026#34;, number_amount, status.MPI_SOURCE, status.MPI_TAG); } 注：srand(time(NULL))用于生成随机数种子，(rand()/(float)RAND_MAX)用于随机生成0~1的数（需包含\u0026lt;time.h\u0026gt;头文件\n使用MPI_Probe找出消息大小 1 2 3 4 5 MPI_Probe( int source, int tag, MPI_Comm comm, MPI_Status* status) 可以将MPI_Probe视为MPI_Recv（除了不接收消息外执行相同的功能）。与MPI_Recv类似，MPI_Probe将阻塞具有匹配标签和发送端的消息，当消息可用时将填充status结构体，然后用户可以使用MPI_Recv接收实际的消息。 在上面的示例中调用MPI_Probe：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 else if (world_rank == 1) { MPI_Status status; // Probe for an incoming message from process zero MPI_Probe(0, 0, MPI_COMM_WORLD, \u0026amp;status); // When probe returns, the status object has the size and other // attributes of the incoming message. Get the message size MPI_Get_count(\u0026amp;status, MPI_INT, \u0026amp;number_amount); // Allocate a buffer to hold the incoming numbers int* number_buf = (int*)malloc(sizeof(int) * number_amount); // Now receive the message with the allocated buffer MPI_Recv(number_buf, number_amount, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); printf(\u0026#34;1 dynamically received %d numbers from 0.\\n\u0026#34;, number_amount); free(number_buf); } 通过MPI_Probe阻塞消息并填充status，再调用MPI_Get_count得到消息个数，然后进程1分配适当大小的缓冲区并接收数字 P2P通信应用——随机游走 随机游走 给定Min，Max和随机游走器W，让游走器W向右以任意长度的S随机移动。如果该过程越过边界，它就会绕回。W一次只能左右移动一个单位。\n随机游走问题的并行化 在许多并行程序的应用中，首要任务是在各个进程之间划分域。随机游走问题的一维域大小为$Max-Min+1$（因为游走器包含Max和Min）。假设游走器只能采取整数大小的步长，我们可以轻松地将域在每个进程中划分为大小近乎相等的块。例如，如果Min为0，Max为20，并且我们有四个进程，则将像这样拆分域。\n前三个进程拥有域的五个单元，而最后一个进程则拥有最后五个单元并且再加上一个剩余的单元。一旦队域进行了分区，应用程序将初始化游走器，游走器将以步长S进行总步数随机的游走。 例如，如果游走器在进程0上进行了移动总数为6的游走，执行如下：\n游走器的步行长度开始增加。但是值达到4时，已达到进程0的边界，因此进程0必须域进程1交流游走器信息。 进程1接收游走器并继续移动直到达到移动总数6，然后进行新的随机移动 使用MPI_Send和MPI_Recv组织代码 初步特征和功能：\n明确每个进程在域中的部分 每个进程初始化N个walker，所有这些walker都从其局部域的第一个值开始 每个walker都有两个相关的整数值：当前位置和剩余步数 walkers开始遍历该域，并传递到其他进程直到完成所有移动 当所有walker完成时，该进程终止 MPI_Abort(MPI_COMM_WORLD, 错误代码)可以终止指定通讯器中的所有进程并退出MPI环境，将错误代码返回给操作系统\n分解域 该函数将考虑域的总大小，并为MPI进程找到合适的子域并将域的其余部分交给最终的进程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 void decompose_domain(int domain_size, int world_rank, int world_size, int* subdomain_start, int* subdomain_size) { if (world_size \u0026gt; domain_size) { // 通常总进程数小于域的规模 MPI_Abort(MPI_COMM_WORLD, 1); } *subdomain_start = domain_size / world_size * world_rank; *subdomain_size = domain_size / world_size; if (world_rank == world_size - 1) { // 最后一个进程特殊处理domain_size *subdomain_size += domain_size % world_size; } } 该函数将域分成偶数个块，并考虑了存在余数的情况。该函数返回一个子域开始和一个子域大小。\n定义并初始化walkers 1 2 3 4 typedef struct { int location, int num_steps_left_in_walker; } Walker; 初始化函数如下：（用于填充传入的walker列表）\n1 2 3 4 5 6 7 8 9 10 11 12 void initialize_walkers(int num_walkers_per_proc, int max_walk_size, int subdomain_start, int subdomain_size, vector\u0026lt;Walker\u0026gt;* incoming_walkers) { Walker walker; for (int i = 0; i \u0026lt; num_walkers_per_proc; i++) { // Initialize walkers in the middle of the subdomain walker.location = subdomain_start; walker.num_steps_left_in_walk = (rand() / (float)RAND_MAX) * max_walk_size; incoming_walkers-\u0026gt;push_back(walker); } } walker移动功能 此功能负责使walkers前进，直到完成移动为止；如果超出局部域范围，则将其添加到outgoing_wallers(vector)中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 void walk(Walker* walker, int subdomain_start, int subdomain_size, int domain_size, vector\u0026lt;Walker\u0026gt;* outgoing_walkers) { while (walker-\u0026gt;num_steps_left_in_walk \u0026gt; 0) { if (walker-\u0026gt;location == subdomain_start + subdomain_size) { // 抵达边界 if (walker-\u0026gt;location == domain_size) { walker-\u0026gt;location = 0; } outgoing_walkers-\u0026gt;push_back(*walker); break; } else { walker-\u0026gt;num_steps_left_in_walk--; walker-\u0026gt;location++; // 向前移动一步（剩余步数--；当前位置++）直到走完 } } } 发送函数 1 2 3 4 5 6 7 8 9 10 void send_outgoing_walkers(vector\u0026lt;Walker\u0026gt;* outgoing_walkers, int world_rank, int world_size) { // 向下一个进程发送消息（如果是最后一个进程则向0进程发送） MPI_Send((void*)outgoing_walkers-\u0026gt;data(), outgoing_walkers-\u0026gt;size() * sizeof(Walker), MPI_BYTE, (world_rank + 1) % world_size, 0, MPI_COMM_WORLD); // 清除待传出walker列表 outgoing_walkers-\u0026gt;clear(); } 接收函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 void receive_incoming_walkers(vector\u0026lt;Walker\u0026gt;* incoming_walkers,int world_rank, int world_size) { MPI_Status status; // Receive from the process before you. If you are process zero, // receive from the last process int incoming_rank = (world_rank == 0) ? world_size - 1 : world_rank - 1; MPI_Probe(incoming_rank, 0, MPI_COMM_WORLD, \u0026amp;status); // Resize your incoming walker buffer based on how much data is // being received int incoming_walkers_size; MPI_Get_count(\u0026amp;status, MPI_BYTE, \u0026amp;incoming_walkers_size); incoming_walkers-\u0026gt;resize( incoming_walkers_size / sizeof(Walker)); MPI_Recv((void*)incoming_walkers-\u0026gt;data(), incoming_walkers_size, MPI_BYTE, incoming_rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); } 由于事先不知道将接收多少walkers，因此需要调用MPI_Probe。\nmain函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // Find your part of the domain decompose_domain(domain_size, world_rank, world_size, \u0026amp;subdomain_start, \u0026amp;subdomain_size); // Initialize walkers in your subdomain initialize_walkers(num_walkers_per_proc, max_walk_size, subdomain_start, subdomain_size, \u0026amp;incoming_walkers); while (!all_walkers_finished) { // Determine walker completion later // Process all incoming walkers for (int i = 0; i \u0026lt; incoming_walkers.size(); i++) { walk(\u0026amp;incoming_walkers[i], subdomain_start, subdomain_size, domain_size, \u0026amp;outgoing_walkers); } // Send all outgoing walkers to the next process. send_outgoing_walkers(\u0026amp;outgoing_walkers, world_rank, world_size); // Receive all the new incoming walkers receive_incoming_walkers(\u0026amp;incoming_walkers, world_rank, world_size); } 死锁及预防 死锁是指两个或多个进程各自在等待另一个进程释放资源，或者两个或多个进程在循环链中等待资源的特定条件。 MPI规范表面MPI_Send会一直阻塞，直到可以回收发送缓冲区为止。者意味着当网络可以缓冲消息时，MPI_Send将返回。如果发送最终无法被网络缓冲，它们将一直阻塞直到发布匹配的接收。 避免可能发生的发送和接收死锁的最佳方法是对消息进行排序，以使发送将具有匹配的接收。一种简单的方法是更改循环，使偶数编号的进程在接收walkers之前发送传出的walkers，而奇数编号的进程相反。\n确认完成 最后一步——确定每个walker何时结束。由于walkers可以随机行走，因此它们可以在任何一个进程中结束。因此，如果没有某种额外的通信，所有进程都很难知道walkers何时全部结束。一种可能的解决方案是让进程-跟踪所有已完成的walker，然后告诉其他所有进程何时终止。但是每个进程都必须向进程0报告所有瓦纳城的walker，然后还要处理不同类型的传入信息。 由于我们直到任意一个walker可以行进的最大距离和每对发送和接收对它可以行进的最小总大小（子域大小），因此可以计算出终止之前每个进程应该执行的发送和接收量。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // Find your part of the domain decompose_domain(domain_size, world_rank, world_size, \u0026amp;subdomain_start, \u0026amp;subdomain_size); // Initialize walkers in your subdomain initialize_walkers(num_walkers_per_proc, max_walk_size, subdomain_start, subdomain_size, \u0026amp;incoming_walkers); // Determine the maximum amount of sends and receives needed to // complete all walkers int maximum_sends_recvs = max_walk_size / (domain_size / world_size) + 1; for (int m = 0; m \u0026lt; maximum_sends_recvs; m++) { // Process all incoming walkers for (int i = 0; i \u0026lt; incoming_walkers.size(); i++) { walk(\u0026amp;incoming_walkers[i], subdomain_start, subdomain_size, domain_size, \u0026amp;outgoing_walkers); } // Send and receive if you are even and vice versa for odd if (world_rank % 2 == 0) { send_outgoing_walkers(\u0026amp;outgoing_walkers, world_rank, world_size); receive_incoming_walkers(\u0026amp;incoming_walkers, world_rank, world_size); } else { receive_incoming_walkers(\u0026amp;incoming_walkers, world_rank, world_size); send_outgoing_walkers(\u0026amp;outgoing_walkers, world_rank, world_size); } } ","date":"2024-11-27T13:22:00+08:00","image":"https://kaigezheng.github.io/p/mpi1/img/cover_hu_294e50cefb838f.png","permalink":"https://kaigezheng.github.io/p/mpi1/","title":"MPI学习笔记——消息传递模型和P2P通信"},{"content":"本文写于2024-3-19至2024-3-24。\nMPI广播及collective communication 集合通信及其同步点 MPI_Barrier(MPI_Comm communicator)用于同步进程，所有进程在执行代码时必须首先都到达一个同步点才能继续执行后面的代码。\n使用MPI_Bcast进行广播 一个广播发生时，一个进程会把同样一份数据传递给一个communicator里的所有其他进程。广播的主要用途之一是把用户输入传递给一个分布式程序，或把一些配置参数传递给所有进程。\n1 2 3 4 5 6 MPI_Bcast( void* data, int count, MPI_Datatype datatype, int root, MPI_Comm communicator) 当根进程调用MPI_Bcast函数时，data变量会被发送到其他进程，其他进程调用MPI_Bcast时，data变量会被复制成从根进程接受到的数据。\n使用MPI_Send和MPI_Recv广播 类似的这层封装：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 void bcast(void* data, int count, MPI_Datatype datatype, int root, MPI_Comm communicator){ int world_rank; MPI_Comm_rank(communicator, \u0026amp;world_rank); int world_size; MPI_Comm_size(communicator, \u0026amp;world_size); if(world_rank == root) { int i; for(i = 0; i \u0026lt; world_size; i++){ if(i != world_rank){ MPI_Send(data, count, datatype, i, 0, communicator); } }else{ MPI_Recv(data, count, datatype, root, 0, communicator, MPI_STATUS_IGNORE); } } } 但是效率特别低！因为每个进程都只有一个I/O网络连接，只使用进程0的一个输出连接来传递数据-\u0026gt;优化为树的通信算法：\n而MPI_Bcast的实现使用了一个类似的树形广播算法来获得比较好的网络利用率。\n时间比较 MPI_Wtime不接收参数，仅仅返回以浮点数形式展示的从1970-01-01到现在为止进过的秒数，与C的time函数类似，使用结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 for(int i = 0; i \u0026lt; num_trails; i++){ MPI_Barrier(MPI_COMM_WORLD);//在开始之前同步 total_trial_A_time -= MPI_Wtime(); /* trial_A */ MPI_Barrier(MPI_COMM_WORLD);//在获得最终时间前再次同步 total_trial_A_time += MPI_Wtime(); MPI_Barrier(MPI_COMM_WORLD); total_trial_B_time -= MPI_Wtime(); /* trial_B */ MPI_Barrier(MPI_COMM_WORLD); total_trial_B_time += MPI_Wtime(); } 当然需要接收实验次数，并在最终时间上除以这个数，同时做多个进程的比较（Processors=2时二者相同）。\nMPI Scatter,Gather,and Allgather MPI_Scatter MPI_Bcast给每个进程发送的是同样的数据，然而MPI_Scatter给每个进程发送的是一个数组的一部分数据。\nMPI_Scatter接收一个数组，并把元素按进程的rank分发。\n1 2 3 4 5 6 7 8 9 MPI_Scatter{ void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_count, int recv_count, MPI_Datatype recv_datatype, int root,//规定了根进程 MPI_Comm communicator} send_data实在跟进程上的一个数据数组，send_count和send_datatype描述了发送给每个进程的数据数量和数据类型 recv_data参数是一个缓存，里面存了recv_count个recv_datatype数据类型的元素 root和communicator指定开始分发数组的跟进程以及对应的communicator MPI_Gather MPI_Gather与MPI_Scatter相反，从多进程里面收集数据到一个进程，这个机制对很多平行算法很有用，如并行的排序和搜索。\n元素根据接收到的进程的rank排序。\n1 2 3 4 5 6 7 8 9 MPI_Gather{ void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, int root, MPI_Comm communicator} 在MPI_Gather中，只有跟进程需要一个有效的接收缓存，其他所有的调用进程可以传递NULL给recv_data，需要注意recv_count参数是从每个进程接收到的数据量。\n使用MPI_Scatter和MPI_Gather来计算平均数 在根进程上生成一个充满随机数字的数组 把所有数字用MPI_Scatter分发同样多给每个进程 每个进程计算它们各自的搭配的数字的平均数 根进程收集所有平均数并计算平均数 生成随机浮点数 1 2 3 4 5 6 7 8 9 float *create_rand_nums(int num_elements) { float *rand_nums = (float *)malloc(sizeof(float) * num_elements); assert(rand_nums != NULL); int i; for(i = 0; i \u0026lt; num_elements; ++i) { rand_nums[i] = (rand() / (float)RAND_MAX); } return rand_nums; } 完整代码 MPI_Allgather 到目前为止都是操作多对一或一对多通信模式的MPI方法，即要么多个进程向一个进程发送数据，要么从一个进程接收数据。很多时候发送多个元素到多个进程也很有用（多对多通信）-\u0026gt;MPI_Allgather 对于分发在所有进程上的一组数据来说，MPI_Allgather会收集所有数据到所有进程上。从最基础的角度来看，MPI_Allgather相当于一个MPI_Gather操作之后跟着一个MPI_Bcast操作。\n与MPI_Gather类似，每个进程上的元素是根据rank顺序被收集的。MPI_Allgather的方法定义跟MPI_Gather几乎一样，不过不需要root参数来指定根进程。\n1 2 3 4 5 6 7 8 9 MPI_Allgather( void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, MPI_Comm communicator ) MPI_Reduce and MPI_Allreduce 归约是函数式编程中的经典概念。数据归约包括通过函数将一组数字归约为较小的一组数字。MPI_Reduce将处理在并行程序中需要执行的几乎所有常见的归约操作。\nMPI_Reduce 与MPI_Gather类似，MPI_Reduce在每个进程上获取一个输入元素数组，并将输出元素数组返回给根进程。\n1 2 3 4 5 6 7 8 MPI_Reduce( void* send_data, void* recv_data, int count, MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm communicator) send_data参数是每个进程都希望归约的datatype类型元素的数组，recv_data仅与具有rootrank的进程有关。recv_data数组包含归约的结果，大小为sizeof(datatype) * count。op参数是希望应用于数据的操作。\nMPI_MAX返回最大元素 MPI_MIN返回最小元素 MPI_SUM元素求和 MPI_PROD元素相乘 MPI_LAND与 MPI_LOR或 MPI_BAND按位与 MPI_BOR按位或 MPI_MAXLOC返回最大值和所在进程的rank MPI_MINLOC返回最小值和所在进程的rank 下面是MPI_Reduce通信模式的说明：\nMPI_Reduce计算均值 1 2 3 4 5 6 7 8 float local_sum = 0; int i; for(i = 0; i \u0026lt; num_elements_per_proc; ++i) { local_sum += rand_nums[i]; } float global_sum; MPI_Reduce(\u0026amp;local_sum, \u0026amp;global_sum, 1, MPI_FLOAT, MPI_SUM, 0, MPI_COMM_WORLD); MPI_Allreduce 在许多并行程序中，需要在所有进程而不是仅仅在根进程中访问归约的结果。与MPI_Gather相似的补充方式，MPI_Allreduce将归约值并将结果分配给所有进程。\n1 2 3 4 5 6 7 MPI_Allreduce( void* send_data, void* recv_data, int count, MPI_Datatype datatype, MPI_Op op, MPI_Comm communicator) MPI_Allreduce等效于执行MPI_Reduce+MPI_Bcast。\nMPI_Allreduce计算标准差 标准差：数字与均值之间的离散程度的度量。 要计算标准差，必须先计算所有数字的平均值。总和均值的平方根是最终结果。 思路：将整体功能分为“计算avg”和“计算$(x_i-avg)^2$的sum\u0026quot;，其中第二个功能需要第一个功能的值。\n通信子和MPI组 通讯子 概述 对于简单的应用程序，使用MPI_COMM_WORLD进行所有操作并不罕见，但是对于更复杂的用例，拥有更多的通讯器可能会有所帮助。如，想对网格中进程的子集执行计算。\n1 2 3 4 5 MPI_Comm_split( MPI_Comm comm, int color, int key, MPI_Comm* newcomm) MPI_Comm_split通过输入参数color和key将通讯器拆分为一组子通讯器来创建新的通讯器。原始的通讯器并没有消失，但是在每个进程中都会创建一个新的通讯器。color确定每个进程将属于哪个新的通讯器，为color传递相同值的所有进程都分配给同一通讯器。如果color为MPI_UNDEFINED，泽该进程将不包含在任何新的通讯器中。key确定每个新通讯器中的顺序（rank）。key最小值的进程为0，下一个为1，以此类推。如果存在相等，则在原始通讯器中rank较低的进程是第一位。newcomm是MPI如何将新的通讯器返回给用户。\n使用多个通信子的示例 color = world_rank / 4将通讯器矩阵分为了四层 我们使用原始rank作为拆分操作的key，新通讯器中的所有进程与原始通讯器中的所有进程处于相同的顺序，保证了正确的排序。 最后通过MPI_Comm_free释放通讯器。MPI一次可以创建的对象数量有限，如果MPI用完了可分配对象，则不释放对象可能会导致运行时错误。\n其他通信子创建函数 MPI_Comm_dup是最基本的通讯器创建函数，创建一个通讯器的副本。对于使用库执行特殊函数的应用（例如数学库）非常有用。在这类应用中，重要的是用户代码和库代码不要互相干扰。为了避免这种情况，每个应用程序应该做的第一件事是创建MPI_COMM_WORLD副本，浙江避免其他使用MPI_COMM_WORLD的库的问题。 另一个功能是MPI_Comm_create。\n1 2 3 4 MPI_Comm_create( MPI_Comm comm, MPI_Group group, MPI_Comm* newcom) 与MPI_Comm_create_group的区别：缺少tag参数；MPI_Comm_create_group仅是group中包含的一组进程的集合，而MPI_Comm_create是comm中每个进程的集合。如果尝试在运行很多很多个进程时创建MPI_COMM_WORLD的子集，则重要的是使用尽可能少的进程来执行此操作，因为大型集的开销会变得非常昂贵。\n组 概述 创建通讯器有更灵活的方法，使用一种新的MPI对象MPI_Group。\n通讯器的实际含义 在内部，MPI必须保持通讯器的两个主要部分，即区分一个通讯器与另一个通讯器的上下文以及该通讯器包含的一组进程。 The context is what prevents an operation on one communicator from matching with a similar operation on another communicator.\n上下文阻止了与一个通讯器上的操作匹配的另一通讯器上的类似操作。MPI在内部为每个通讯器保留一个ID以防混淆。\n使用MPI组 在MPI中，很容易通过API调用MPI_Comm_group来获取通讯器中的进程组。\n1 2 3 MPI_Comm_group( MPI_Comm comm, MPI_Group* group) 通讯器包含一个上下文或ID，以及一个组。调用MPI_Comm_group会得到对该组对象的引用。组对象的工作方式与通讯器对象相同，不同之处在于您不能使用它与其他rank进行通信（因为它没有附加上下文）。但仍然可以获取组的rank和size（MPI_Group_rank和MPI_Group_size）。但是，组特有的功能而通讯器无法完成的工作是可以使用组在本地构建新的组。 在此记住本地操作和远程操作之间的区别很重要。 远程操作涉及与其他秩的通信，而本地操作则没有。 创建新的通讯器是一项远程操作，因为所有进程都需要决定相同的上下文和组，而在本地创建组是因为它不用于通信，因此每个进程不需要具有相同的上下文。 您可以随意操作一个组，而无需执行任何通信。\n并\n1 2 3 4 MPI_Group_union( MPI_Group group1, MPI_Group group2, MPI_Group* newgroup) 交\n1 2 3 4 MPI_Group_intersection( MPI_Group group1, MPI_Group group2, MPI_Group* newgroup) ","date":"2024-11-27T17:34:00+08:00","image":"https://kaigezheng.github.io/p/mpi2/img/cover_hu_294e50cefb838f.png","permalink":"https://kaigezheng.github.io/p/mpi2/","title":"MPI学习笔记——集合通信"},{"content":"旅行回忆录——寄语 这一段我写了很多次，总是没码几个字就删了，总感觉写不好。旅行回忆录是我在搭建博客时最想写的部分，但是真正下笔的时候却总是恍惚了起来。或许太久没写过文章了对自己的文字不太满意，也或许是不知怎样把本就迷离的回忆编织成清晰的文字。\n孩童时我是不喜欢旅游的，只想在家打游戏，依稀只记得随家人去过云南和上海（世博会），也仅此泡影般的印象了。在高中毕业前，我也确实很少旅游，游戏和学业填充着我的童年和青春。\n本科入学以后，差不多也是疫情结束的时候，或许是远赴离家两千公里外的地方读书让我觉得我不应该被束缚在某一个地方，该去看看外面的世界了，不知怎的我就开始爱上了旅行。\n旅行本身带给我的除了新鲜感，还有只能在某一人生阶段的某时某刻带来的独一无二的感想，可惜的是此前少有记录。我人生的苦旅还长，今后也会有更多精彩的旅行，如果只留下一条朋友圈就太可惜了。某些弥足珍贵的感想，某些改变我人生的场景，我还是想在这里记录下来，分享给我希望分享的人。好在这个站点及时建立，我还来得及回忆以前的经历并记录下来。于是，我的旅行回忆录诞生了。\n这篇文章回忆的是我人生的第一站，上海，一次足以（也确实）改变了我人生轨迹的旅行。\n寄语写于2024年11月29日19：44，计算机学院304办公室。\n梦开始的地方——上海 前言 五年前是我初中毕业的时候，那年中考正常发挥，后来被在三校统招中摇到了温二外，这在那时算是个不错的结果。初升高的暑假是快乐的，没有繁忙的学业和看不到头的作业，除了上初升高暑期衔接班就是打游戏和看动画。在某次衔接班放学回家的公交车上，偶然在b站看见梶浦由记演唱会的售票信息（FictionJunction vol.15，一个小插曲是在我于闲鱼收了一张票后的当晚就开了追加席的售票），那时对梶浦由记的认知仅限于为fate、刀剑、魔圆这些当时的霸权番配过乐，仅此而已，所以其实没有特别感兴趣。但是时间刚好和我非常想去的ChinaJoy能搭上，于是就下定决心去一次上海吧！\n可惜当时还没有成年不能单独住hotel，于是拉上了同上衔接班的小学同学，和他的两位朋友，一行四人（虽然除了住宿外基本是我单独旅行）包了一间青年旅舍四人间，踏上了这次上海之旅。\n牛肉面 2019年8月1日，这是我人生第二次来到上海。\n由于是第一次自己规划旅行，我做了（自认为）充足的准备，虽然现在再次看当时的计划目的景点，大多有些坑外地人，不过，that\u0026rsquo;s fine。我在兰图绘（一个能够在地图上标记的网页，我至今也经常使用，也确实没找到其他的替代品）大致标记了我可能会去的地方，所幸这张很有纪念意义的地图还没被删掉。\n时间久远加上照片丢失，很多细节我已无力追溯，我只能靠当时在社交媒体的推文和一些残存的记忆来尽可能地还原当时的经历和所思所想。既然流水账行不通的话，就先从我最先想到的地方开始讲吧，大可随便一些。\n当时在一番考虑之下，我们选择了上海一间青年旅舍（静安店寺地铁站店）。价格在旺季算是挺合适了，一个床位96元每天。去地铁站8号线中兴路站需要步行大约一公里，不算太远，但在上海的夏天已经足够折磨人了。房间的硬件设施比较简单，四张宿舍般的上下铺床位，一张桌子，一个晾衣杆和四个衣架，此外值得一提的是，四人间有独立卫浴。青旅正对高架桥，但是隔音还不错。只是以上这些，这间青旅不值得我惦记。青旅出门右手一两百米有一家铺子不大的牛肉面店，我在上海的每天早饭都是在那里解决的。四天三晚的行程（实际上只有三次早餐）中，我在那家小店里分别吃了牛肉面、牛杂面和牛杂拌面，价格大概是二十元左右，牛肉或是牛杂给的很多，对于上海的物价而言是非常实惠了。我不擅长评价食物，但是牛肉和牛杂味道很棒，肉汤鲜味十足，我能确信这是我这辈子吃过最好的牛肉面。这两年我去上海的次数不少，在某一次行程中我尝试回到那间青旅，目的是再尝一尝记忆中最完美的牛肉面。可惜或许是因为导航错了地方，我没找到那间青旅，也自然找不到那家牛肉面店。或许那碗牛肉面在我的回忆中被美化到了不应达到的高度，不过我也希望能在此生再吃上一碗。\n外滩 来上海不能不知道外滩。在抵达上海的第一天晚上，我们选择去乘一次外滩的游船，我们提前订好了船票（当时不知道有轮渡，虽然我现在也没坐过轮渡）。船票108元一张，从十六铺码头出发，在外滩和陆家嘴之间环游一圈回到码头，大约有一个小时。那时我还不太清楚外滩是怎样个范围，也不知道最受欢迎的观景位是南京东路的尽头，只知道似乎发生过外滩灯光秀的踩踏事故。\n坐上游船开始观景，这是繁丽魔幻的现代都市对二线城市小镇做题家灵魂冲击的开始。我无法想象晚上的河岸能够如此闪耀，（陆家嘴的）电视塔和各式高楼的灯光能够如此霓虹变幻。尽管时间久远，但我能够确信那时我肯定也陷入了一些恍惚，是一种超越我迄今为止对这个小小世界的认知的恍惚。或许是散光，亦或许是一些触动心神的震撼带来的泪水，尽管远处（外滩一侧）的建筑和灯光我看不太清楚，但是这也够了。这些模糊感足够让我感到外滩的神秘，和一种发自内心的游离感。\n由于是抵达上海的第一天，还没感受到上海轨道交通系统的强大，一群坐惯了15分钟一趟公交车的学生决定打的回青旅。滴滴打车排队100多人属实吓到了我们。最后我们选择在路边叫车，四个人分摊一百元车费倒是还可以接受。后来在跟别的出租车司机聊天中得知，那种颜色（具体忘了）的出租车经常宰客，确实，7公里一百块钱确实有些狠。\nChinaJoy和Yuki Kajiura LIVE 8月2日应该是我这趟行程最期待的一天。早上会去上海新国际博览中心参观期待已久的ChinaJoy Day2，晚上则是去国家会展中心的虹馆EH听梶浦的演唱会。\n从7号线花木路站下车步行至龙阳路，这段路程会很熟悉，因为第二天临时起意想去的海贼王巡展上海站也就在那附近（喜马拉雅艺术中心）。地铁上能看到很多coser印象比较深的有蝶祈（那年罪恶王冠还很火）、fgo里的小达芬奇、还有不少bilibili的2233娘。\nChinaJoy的场馆真的很大，从早八点半左右开始排队，进场就走了将近半个钟头。一天下来在场馆绕了三圈左右，走了得有二十多公里。ChinaJoy有很多游戏硬件厂商和汽车厂商参展，可惜我对这些不是很感兴趣，其次就是游戏展和动漫展了。动漫展只占十个展馆中的一个，数量上少但也足矣，有木棉花等一些厂商，周边价格也不便宜，只简单地买了一些。游戏展有不少知名厂商如bilibili、腾讯、任天堂、索尼、育碧等，也算是大饱眼福了。当时听说索尼为了死亡搁浅的中国首发放映会准备了价值两亿的大屏幕，我也算是去凑了个热闹（虽然我到现在都没有玩）。以及由于当时风口浪尖的原神登录PS4而在索尼摊位前怒砸PS4的事件也发生在现场（虽然我并不在，是事后在贴吧看到的）。\n很多人逛CJ可以带一麻袋礼物出来，不过我比较社恐，没参与任何活动。记得买了张地平线零之曙光的光盘（后来因为晕3d也没玩儿下去），路人女主里英梨梨的色纸和刀剑的挂轴。就这么到处随便看看，逛了二十多公里。\n中午在CJ现场啃了麦当劳，下午逛累了就从龙阳路转2号线去虹馆EH了。4点就到虹馆了（迷宫一般的虹馆，当时还有进博会，当同时有多个活动在会展中心举办的时候就非常难走），绕了好久的路，在上楼的路上收到了无料（同人卡套和小扇子），在场贩买了个80块钱的束口袋（后来用得破破烂烂的了），然后就去那家五六点就下班的KFC里坐着等了。那时不知道有梶浦由记音乐共享会（群聊），后来在23年11月去FJ18 Shanghai时跟群友闲聊才知道当时也有一些歌迷活动，不过我在KFC根本不知道。初三毕业就有幸听到一场Yuki桑的LIVE，我兴许也是那时最年轻的听众；Yuki的歌迷大多也是已经工作了的社畜。\n这是一次难忘的LIVE，也是我人生的第一场LIVE。虹馆的音响向来不好，但颤动心弦的震响足以在这一魔幻的旅途给我带来冲击灵魂的震撼。起先只听过light sword、满天的路人，即将成为Yuki桑的忠实歌迷。\n一些更细节的回忆，我在参加FJ vol.18 Shanghai前在知乎留下了这篇回答，感兴趣的话可以去看看。今后我也可能会写一篇LIVE回忆录，专门来记录LIVE的体验，这里就不多写了。\n演出结束已经是十点左右，很幸运地赶上了地铁末班车。相比于温州八点多公交就停运地差不多了，在十点钟能在半小时左右跨越二十多公里回府，这是我不敢想的。\n隔日，偶然在b站看见海贼王巡展上海站正好在举办，就打算去看看。里面有一个光影演出非常精彩，路飞在一面墙不断躲避障碍，穿梭到另一面墙，可惜这部分不允许拍摄。里面有许多复制原画，可惜当时只注意拍照留档（最后也没留住），没有慢慢欣赏。参展结束后体验了一下VR，那是我第一次看VR视频。临走前买了份明信片和海报，那个时候路飞的悬赏刚到15亿贝利。\n走马灯 到此为止，本篇回忆最精彩的部分就结束了。接下来就是杂七杂八的随想。除了外滩、展馆和LIVE，也去了不少地方。海洋水族馆的内容一般，门票却要144元，毫无性价比，至少我是不会再去了；田子坊确实是个坑外地人的人造景点，没有什么意思；城隍庙的建筑是我喜欢的，那天的晴天和雨天切换地很快，从外滩走到城隍庙的这段路也是我非常喜欢的，后来我也走过不少次；每次路过豫园都没有开门，我也不大清楚这里的参观时间是怎样的，总之也没进去过；陆家嘴的中央绿地后来也没少去过，在那里仰望几座大楼，绝对可以感受到自我的渺小；最后一天来到静安大悦城想坐个摩天轮来着，可惜来的有些太早了，遗憾，该坐动车回温州了……\n这一次上海之旅，对我而言不仅仅是第一次人生的旅行，它还意味着很多。上海地铁在炎热的夏天提供了清凉的环境和舒适的交通体验，上海的交通系统也是这次旅行给我带来最大感触的要素之一。对于在温州只能乘坐公交或是出租车地我，第一次如此自由、高效、经济地在一座城市里穿梭，这得以让每一个突然的想法充实着我快节奏的旅行之中。我也开始理解为什么交通与城市发展为何息息相关了，最近几年第一次在温州乘坐轻轨时，我也有了当初乘坐上海地铁时的恍惚感了。我能确信有这么一个瞬间，在隧道中疾驰的列车中听着空气间的呼啸，我应该也应当感慨于上海轨道交通的便利，一些小镇（在上海面前没有几座城市能自称都市）人的向往和自卑，一些认知被打破的茫然。\n外滩是我每次来上海不会错过的地方，这点执念有些近似于朝圣了，因为这里对我而言不仅仅是外滩。也带着些南京东路的嘈杂与聒噪，外滩的人永远都不少，尤其是旺季，这几年更甚。或许某种程度上也能印证这一“朝圣之地”的想法。在外滩看的是陆家嘴，有哪些大楼自不必多说，霓虹变幻的灯光不免让人感到恍惚（这个词在这篇文章似乎提到很多次了）。恍惚于什么？我每次来看外滩陆家嘴，或者说每次处于不同的人生阶段、带着不同的心情来看的时候，都能给我不同的感受。这里魔幻般的繁华带给我最大的感受是——我向往这里，但我不属于这里。当然随着人生阅历的丰富，近几年我的想法也有所改变，对这里的思考不再是理想乡，多了些理性，削减了几分向往。我感受到了个体的、我的渺小，感受到了上海、世界的遥远。这里的生活（或许）很美好，这里的发展（确实）很宏伟，但这与我无关，它们不属于我，应该说是我不属于这里。踏上离开的地铁，你终于得在这恍惚间反应过来，你的生活还在继续，与这里无关。类似的想法也会产生在陆家嘴散步时，所以我也喜欢在陆家嘴散步，这里的参差感能让我反思人生，重新审视自己。\n这次旅行让我彻头彻尾地着迷于上海这一现代都市，我也开始向往大学能考到上海。虽然事与愿违，但我在高中一改以往玩物丧志的思想，开始认真规划以后的人生道路。不论是在精神上，在思想上，这次旅行一定程度上也改变了我的人生。也算是借这次回忆，再一次审视了自己和未来吧。应当还有很多能写的，但就到此为止吧。化用《东五往事》的话，窗外天色如墨，在这安静的氛围里，我写下最后一字，宣告追忆的结束并继续迈向未来。\n全文完。\n五年半后的后记 2025年1月15日，大三寒假回家，因为中转又一次造访上海。这两年来我也来过很多次了，按频率算的话，平均每两三个月都会因为或大或小的原因来上海（多数是因为中转或是看live）。因为来得多了，少了初识的新鲜感，也随着人生阅历的丰富，对现代化城市表面繁华的思考也趋于理性了，或者说是祛魅了。不论如何，总归还是来了。\n由于每次待的时间都不长，因此从南京东路（当然会看一看百联和SE cafe）走到外滩，在外滩休息一会已经是我“重温”上海的习惯流程了，这次也不例外。每回在外滩站立，我纵向重新审视一下自己、酝酿些能体感人生的感悟，似乎有些刻意为之了；每回在外滩观景，我总想把眼前的一切都刻印到脑海，无奈充斥回忆的外滩比平整的一张照片立体太多。Fine，人生的好风景远不止外滩，何必囿于此地，继续行走便是。\n不一样的是，正巧这次住的hotel和五年前的青年旅舍在同一个地铁站附近（8号线的中兴路），甚至只隔了两条街道！那就没有不去看看的理由了。\n从地铁站4口出来，沿着导航的路线，周围已经有了很大的变化，不愧是静安区的城建速度，好在我熟知的那一块变化无几。往南走100多米就能找到那家牛肉面店，店面装潢显然翻新过，以至于我最开始怀疑还是不是那家店。（后来问过老板娘，老板娘说她在这里已经开了六七年了）\n点了份之前常吃的牛杂拌面，虽然不及记忆中那么好吃，不过也是差强人意的。后来看大众点评在发现这家店只有3.5分，如果不是这家店在我的记忆中独树一帜，我了能永远也不会来这家店。总之，这家店的牛肉面是，也依旧是很好吃的。\n全文完，again。\n","date":"2024-11-30T17:18:00+08:00","image":"https://kaigezheng.github.io/p/travel1/img/cover_hu_57e7179a46468075.jpg","permalink":"https://kaigezheng.github.io/p/travel1/","title":"人生苦旅的起点，也是梦开始的地方——上海"},{"content":"前言 15号突然得知18号要去武汉办事，顺便还能去见见世面，加上不用自己出钱，未尝不是件美差。本以为2024年的年末不会有什么出行计划了，前不久很突然的北京之行和这回也蛮突然的武汉之行也算是在我意料之外了。这次也是我开始搭建博客后的第一次出行，也就借这个机会记录一下吧！在武汉的停留时间很短，只有19号下午和晚上可以稍微在武汉溜达溜达。\n武汉(2024/12/18-2024/12/20) DAY1 出发的机票定在下午，不用早起赶路是件好事。吃完早饭在食堂门口偶遇可爱小猫，它看上去很冷的样子。上午就做了些杂事，玩了会ComfyUI，然后就该出门了！\n本以为年末的淡季应该没多少人出行，没想到还是舱满出发的。落地已经是晚上六点半了，加上滑行，实际出机场已经是七点多了。好在这次行程不长，只带了个双肩包，不用等托运行李。\n天河机场很大，似乎在中转服务上做的不错，看见了有免费的中转荆楚小食和餐券提供，也有免费住宿可以申请，缺点是离市区有点远，而且T2航站楼没有地铁。要去的酒店在花山，有五十多公里远，天河机场-市中心-花山差不多是练成一条直线的情况。后来问司机才知道地铁在T3，得坐摆渡车过去，这点还是不太方便；加上比较赶时间，所以只能打车了（这一程花了175元，心碎）。签到入住，组委会提供的酒店还不错，大且整洁。\n大概八点多入住，时间不早也不晚，加上还没吃晚饭，纠结一番还是打算出来溜达溜达。花山也算是大郊区了，附近没有什么选项，就只能去稍微远一点的光谷步行街了。\n光谷步行街的商圈范围很大，应该是由好几个商场和步行街区组成，在华科南侧（羡慕华科的学生门口有这么多吃的）。步行街里也有一层谷店，这似乎是这两年各种商圈的趋势，更何况这边基本都是年轻人出没。最后坐下吃了点，看了眼光谷广场（没有想象中的灯光），去HUST门口瞄了眼就打道回府了。\n回去后就开始看SDAA C文档，看到两点多才睡。怕第二天起不来床，预定了份热干面外卖来喊我，顺便拜托了前台明早打电话来喊我。\nDAY2 第二天上午也是顺利完成了任务，在现场也学习到了不少，这里就一笔带过了。第二天转移阵地，来到武大附近住，因为花山离机场、市中心都实在是太远了。住处门口就有茶颜悦色，还没喝过，尝尝味，感觉没什么特别的，不过做的速度特别快。下午两点来武大门口拍了个照，可惜工作日不允许社会人士进入。\n我计划逆时针溜达一圈，所以下一站该是黄鹤楼了。\n黄鹤楼公园的游览体验是极好的，也是我整趟旅途体验最好的一部分。半价学生票是35元/张，这也是我第一次遇到（真正）检查我学生身份的景区。公园本身像座山，进门就得爬一段楼梯还挺独特的。\n黄鹤楼本身也很amazing，这样中式木制建筑风格造型的塔我还是非常喜欢的。内部有一些瓷砖壁画，显然是最近几年或十几年重新装饰的。黄鹤楼一共有5楼，不过最上层的人太多了，有些拥挤，所以在4楼看看城市风貌足矣。能够在同时看到鹦鹉洲长江大桥和长江大桥，论造型我还是更喜欢鹦鹉洲长江大桥这样有悬索的吊桥。这里值得驻足一会，有点想念之前在兰州的黄河塔上眺望城市。\n华昙林和户部巷不远（1.5公里左右），就顺便过去看看，但是我觉得没什么意思，前者是商业化严重的文艺小镇，后者是遍布网红小吃摊的小街。不同地方流行的小吃确实不太一样，这里的三鲜豆皮、冰粉、藕粉格外多。买了份三鲜豆皮尝尝，感觉像是千层饼夹糯米饭？如果饿的时候尝我应该会更喜欢。\n然后就是期待已久的横渡长江了！长江大桥下的江滩看着不错，不知道为什么我对跨江大桥和江滩尤其喜欢，可能是重庆的千厮门大桥和江滩公园给我的印象太深刻了。穿行大桥需要大约25分钟，遗憾的是天气雾蒙蒙的，如果有落日黄昏就更好了。一路上没遇到多少行人，大多是自行车或电动车，我也不建议闲着没事来走，一来江边的风很冷，二来过江后是在山上，得先坐一站公交才能到地铁站，打车也极其不方便。\n不过我还蛮喜欢这一段散步的，可以远眺鹦鹉洲长江大桥，没什么人，我可以高歌我最喜欢的暁の車（随处发癫）。\n之后我还依次逛了黎黄陂路、江汉路、汉口江滩，感觉一般般：黎黄陂路仿佛是上海的武康路（虽然没去过），巴公邸和武康大楼太像了；江汉路就是没啥特点的步行街；汉口江滩其实主要还是附近居民的绿地公园吧，简单的散散步不错，如果是抱着看桥或是下江滩吹吹江风的想法是会失望的。\n最后去住处附近的武商梦时代（据说是世界最大的商城）觅食，吃了海鲜烩饭。一个下午加一个晚上总共走了33000步，也算是在这么短的时间了看过武汉比较受欢迎的地方了。\n至此，我在武汉的旅途就结束了。\nDAY3 七点起床，啃了份皮蛋瘦肉粥和油条麻糍，就该回去赶飞机了。不得不吐槽，武汉的地铁分布还是有些松散，我在武汉大部分的通行方式是打的或是公交，通过地铁去机场也不太方便。\n时间虽短，也算是在武汉大致游玩了一圈，下次有机会的话，想去东湖那边转转。\n完。\n","date":"2024-12-17T20:02:25+08:00","image":"https://kaigezheng.github.io/p/travel3/img/cover_hu_2c231a105a6b07b4.png","permalink":"https://kaigezheng.github.io/p/travel3/","title":"突然的武汉之旅"},{"content":"前言 此篇博客系补档，写于2023/11/04至2023/11/14，最后更新于2023/11/14 16:23，曾发布于知乎。\n朴素字符串匹配 朴素字符串匹配算法(Naive-String-Matcher)是通过一个循环找到所有有效偏移，该循环对n-m+1个可能的s值进行检测，看是否满足条件$P[1..m]=T[s+1..s+m]$。\n1 2 3 4 5 6 //伪代码NAIVE-STRING-MATCHER(T,P) n = T.length m = P.length for s = 0 to n-m if P[1..n] == T[s+1..s+m] print \u0026#34;Pattern occurs with shift\u0026#34;s 由于不需要预处理，在最坏情况下的时间复杂度为O((n-m+1)m)（对偏移s的n-m+1个可能值比较，或者说对前n-m+1的每个字符做一次长度为m的匹配）。 KMP算法在最坏情况下比朴素算法好得多，这种匹配算法效率不高，因为当其他无效的s值存在时，它也只关心一个有效的s值，而完全忽略了检测无效s值时获得的文本信息（这种信息可能非常有用）。\n利用有限自动机进行字符串匹配 很多字符串匹配算法都要建立一个有限自动机，它时一个处理信息的简单机器，通过对文本字符串T进行扫描，找出模式P的所有出现位置。\n有限自动机 一个有限自动机$M$是一个五元组( $Q$ , $q$0 , $A$ , $\\sum$ , $\\delta$ )，其中：\n$Q$是状态的有限集合 $q$0$\\in$Q是初始状态 $A$$\\subseteq$Q是一个特殊的接受状态集合 $\\sum$是有限输入字母表 $\\delta$是一个从Q$\\times$$\\sum$到Q的函数，称为$M$的转移函数 有限自动机开始于状态$q$0，每次读入输入字符串的一个字符。如果有限自动机在状态q时读入了字符a，则它从状态q变为状态$\\delta( q , a )$（进行了一次转移）。每当其当前状态$q$属于$A$时，就表明自动机M接受了迄今为止所读入的字符串，没有被接受的输入称为被拒绝的输入。 用一个简单的两状态自动机说明 该自动机拥有状态集$Q=${0,1}，开始状态$q$0=0，输入字母表$\\sum$={a,b}。 (a)用表格表示状态函数$\\delta$，(b)是一个等价的状态转换图。 状态1是唯一的被接受状态，有向边代表着转换（例如从状态1到状态0的b边表示$\\delta(1,b)=0$。显然，这个自动机接受以奇数个a结尾的字符串。或者说，一个 字符串$x$被接受当且仅当$x=yz$，其中$y=\\varepsilon$或$y$以b结尾，并且$z=a$k（k为奇数）。 例如，对于输入abaaa，包括初始状态，这个自动机输入状态序列为$\u0026lt;0,1,0,1,0,1\u0026gt;$，因而它接受这个输入；对于输入abbaa，自动机输入状态序列为$\u0026lt;0,1,0,0,1,0\u0026gt;$，因而它拒绝这个输入。\n有限自动机$M$引入一个函数$\\phi$，称为终态函数，它是从$\\sum*$到$Q$的函数，满足$\\phi(\\omega)$是$M$在扫描字符串$\\omega$后终止时的状态。因此，当且仅当$\\phi(\\omega)\\in A$时，$M$接受字符串$\\omega$。我们可以用转移函数递归定义$\\phi$： $\\phi(\\varepsilon)=q$0 $\\phi(\\omega a)=\\delta(\\phi(\\omega),a), \\omega\\in\\sum*,a\\in\\sum$\n字符串匹配自动机 对于一个给定的模式P，我们可以在预处理阶段构造出一个字符串匹配自动机，根据模式构造出相应的自动机后，再利用它来搜寻文本字符串。\n定义一个辅助函数$\\sigma$，称为对应模式串的后缀函数。 满足$\\sigma(x)$是$x$的后缀与对应模式串P的前缀的最大公共长度： $\\sigma(x)=$max{k:Pk$\\sqsupset$ $x$} 如对于模式串P=ab，有$\\sigma$($\\varepsilon$)=0，$\\sigma$(ccaca)=1，$\\sigma$(ccab)=2\n给定模式$P[1..m]$，其相应的字符串匹配自动机定义如下：\n状态集合$Q$为{0,1,\u0026hellip;,m}。开始状态$q$0是0状态，并且只有状态m是唯一被接受的状态 对任意的状态$q$和字符$a$，转移函数$\\delta$定义为$\\delta(q,a)$=$\\sigma$$($Pq$a$$)$ 我们定义$\\delta(q,a)$=$\\sigma$$($Pq$a$$)$，目的是记录已得到的与模式P匹配的文本呢字符串T的最长前缀。考虑最近一次扫描T的字符，为了使T的一个子串能够和P的某些前缀Pj匹配，前缀Pj必须是Ti的一个后缀。如果自动机处于状态$q$并且读入下一个字符$T[i+1]=a$，那么我们希望这个转换能够指向Tia的后缀状态，它对应着P的最长前缀，即$\\sigma$(Tia)。由于Pq是P的最长前缀，也就是Ti的一个后缀，那么P的最长前缀也就是Tia的一个后缀，即$\\sigma$$($Tia$)$=$\\sigma$$($Pqa$)$（由后缀函数递归引理证明，此处省略）。因此，当自动机处在状态$q$时，我们希望这个在字符a上的转移函数能使自动机转移到状态$\\sigma$$($Pqa$)$。 考虑两种情况：\n$a=P[q+1]$，使得字符a继续匹配模式，那么状态前进，即$\\delta(q,a)=q+1$ $a\\ne P[q+1]$，使得字符a不能继续匹配模式，那么回溯到一个更小的字串，它是**P的前缀同时也是Ti的后缀。 在这个例子中，状态0是初始状态，状态7是仅有的接受状态。 图中的自动机有$\\delta(5,c)=6$，说明其是第一种情况，匹配继续进行。 但是，如果自动机在$q=5$状态时读到b，那么Pqb=ababab，并且P的最长前缀也是ababab的后缀P4=abab，因此转移到状态$q=5$。\n1 2 3 4 5 6 7 //伪代码有限自动机(T,δ,m) n = T.length q = 0 for i = 1 to n q = δ(q,T[i]) if q == m print \u0026#34;Pattern occurs with shift\u0026#34;i-m 从有限自动机的简单循环结构可以看出，对于一个长度为n的文本字符串，它的匹配时间为$O(n)$。但是，这一匹配时间没有包括计算转移函数$\\delta$所需要的预处理时间。\n定理：如果$\\phi$是字符串匹配自动机关于给定模式P的终态函数，T[1..n]是自动机的输入文本，则对i=0,1,\u0026hellip;,n，有$\\phi($Ti$)$=$\\sigma($Ti$)$\n1 2 3 4 5 6 7 8 9 10 //伪代码计算转移函数 m = P.length for q = 0 to m for each charater a ∈ Σ k = min(m+1, q+2) repeat k = k - 1 until Pk是Pqa的后缀 δ(q,a) = k return δ 这个过程根据定理直接计算$\\delta(q,a)$，在嵌套循环中考察所有的状态$q$和字符a。第4~8行把$\\delta(q,a)$置为满足Pk$\\sqsupset$Pqa的最大的k。代码从k的最大可能值$min(m, q+1)$开始，随着过程的执行，k递减至Pk$\\sqsupset$Pqa，这种情况必然会发生，因为P0=$\\varepsilon$是每个字符串的一个后缀。\nKMP算法（Knuth-Morris-Pratt A.） 这个算法无需计算转移函数$\\delta$，匹配时间为$O(n)$，只用到辅助函数$\\pi$，它在O(m)时间内根据模式预先计算出来，并且存储在数组$\\pi$$[1..m]$中。数组$\\pi$使得我们可以按需要有效地计算转移函数$\\delta$。粗略地说，对任意状态q=0,1,\u0026hellip;,m和任意字符$a\\in\\sum$，$\\pi[q]$的值包含了与a无关但在计算$\\delta(q,a)$时需要的信息。由于数组$\\pi$只有m个元素，而$\\delta$有$O(m\\sum)$个值，所以通过预先计算$\\pi$而不是$\\delta$，可以使计算时间减少一个$\\sum$因子。\n关于模式的前缀函数 模式的前缀函数$\\pi$包含模式与其自身的偏移进行匹配的信息，这些信息可用在朴素字符串匹配算法中避免对无用偏移的检测，也可以避免在字符串匹配自动机中对整个转移函数$\\delta$的预先计算。\n在这个用模式串P=ababaca匹配文本字符串T的例子中，q=5个字符已经匹配成功，但模式的第6个字符不能匹配。已知的这q个文本字符使我们能够立即确定某些偏移是无效的，如s+1必然是无效的，因为文本字符已知不能与模式的第一个字符a匹配，但能与模式的第二个字符b匹配。因此，如下图所示的$s\u0026rsquo;=s+2$的偏移显然比s+1的偏移更有效率。 在这里，我们发现P3是能构成P5真后缀的P的最长前缀（如下图所示），这些信息被预先计算出来，并用数组$\\pi$表示，即$\\pi[5]=3$。我们可以归纳得到，在偏移s有q个字符成功匹配时，则下一个可能有效的偏移为$s\u0026rsquo;=s+(q-\\pi[q])$。 接下来我们来讨论$\\pi[q]$从何而来。假设模式字符$P[1..q]$与文本字符$T[s+1..s+q]$匹配，$s\u0026rsquo;$是最小的偏移量，$s\u0026rsquo;\u0026gt;s$，那么对某些k\u0026lt;q，满足$P[q..k]=T[s\u0026rsquo;+1..s\u0026rsquo;+k]$的最小偏移$s\u0026rsquo;$是多少（其中$s\u0026rsquo;+k=s+q$）？ 换句话说，已知Pq$\\sqsupset$Ts+q，我们希望Pq的最长真前缀Pk也是Ts+q的后缀。由于$s\u0026rsquo;+k=s+q$，那么找到的最小偏移$s\u0026rsquo;$等价于找到最长前缀的长度k。我们把P前缀长度范围内的差值$q-k$加入到偏移s中，用于找到新的偏移$s\u0026rsquo;$，使得$s\u0026rsquo;=s+(q-k)$。在最好情况下$k=0$，因此$s\u0026rsquo;=s+q$。总之，在任何情况下，对于新的偏移$s\u0026rsquo;$，无需把P的前k个字符与T中相应的字符进行比较，因为$P[q..k]=T[s\u0026rsquo;+1..s\u0026rsquo;+k]$已经保证它们肯定匹配。\n我们可以用模式与其自身进行比较来预先计算出这些必要的信息，如图所示： $\\pi$存储的是模式串P的最长公共前后缀。例如，$\\pi[5]=3,\\pi[3]=1,\\pi[1]=0$，通过迭代可以得到$\\pi[\\pi[\\pi[5]]]=0,\\pi[\\pi[5]]=1$，可以用$\\pi*[5]={3,1,0}$表示。那么，当k为3或1或0时，都会出现偏移$s\u0026rsquo;$满足P的某前缀与Pq的后缀匹配，实现更有效率的偏移。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 //伪代码KMP算法 n = T.length m = P.length π = COMPUTE-PREFIX-FUNCTION(p) q = 0 //number of characters matched for i = 1 to n //scan the text T while q \u0026gt; 0 and P[q+1] != T[i] q = π[q] if P[q+1] == T[i] q = q + 1 if q == m print \u0026#34;Pattern occurs with shift\u0026#34;i-m q = π[q] //COMPUTE-PREFIX-FUNCTION m = P.length let π[1..m] be a new array π[1] = 0 k = 0 for q = 2 to m while k \u0026gt; 0 and P[k+1] != P[q] k = π[k] if P[k+1] == P[q] k = k + 1 π[q] = k return π KMP算法通过运用$\\pi$而不是$\\delta$，可将对模式进行预处理的时间由$O(m \\sum)$减为$O(m)$，同时保持实际的匹配时间为$O(n)$（暂时省略证明，以后来补罢）。\n","date":"2024-12-07T14:49:21+08:00","image":"https://kaigezheng.github.io/p/algorithm1/img/cover_hu_a0aad241da7692fe.jpg","permalink":"https://kaigezheng.github.io/p/algorithm1/","title":"字符串匹配"},{"content":"前言 此篇博客系补档，写于2023/11/10至2023/12/07，最后更新于2023/12/07 21:53，曾发布于知乎。\n图论（Graph Theory）的基本知识 我们通常用$G=(V,E)$表示图（Graph，图；Vertices，顶点；Edge，边），一般用邻接矩阵或邻接（链）表来表示，二者都可以存储无向图和有向图。通常我们在稀疏图（$|E|\u0026laquo;|V|^2$）选择邻接表，在稠密图（$|E|$接近$|V|^2$）或需要快速判断任意两个结点之间是否有边相连时选择邻接矩阵。\n邻接表 对于图$G=(V,E)$的邻接表是一个长度为$|V|$的数组$Adj$，其中数组的每个元素都是一条链表，$Adj[u]$中存储有所有与结点$u$邻接的结点。\n接下来分别在有向图和无向图中讨论邻接表：\n若$G$是有向图，则对于边$(u,v)$，结点$v$将出现在链表$Adj[u]$里，因此所有邻接链表的长度之和为$|E|$ 若$G$是无向图，则对于边$(u,v)$，结点$v$将出现在链表$Adj[u]$中，结点$u$将出现在链表$Adj[v]$中，因此所有邻接链表的长度之和为$2|E|$ 不论有向图还是无向图，邻接表的存储空间需求均为$\\Theta(V+E)$。 优点：邻接表的robust很好，可以对其简单修改来支持许多其他的图变种。如权重图，将边$(u,v)\\in E$的权重值$\\omega(u,v)$存放在结点$u$的邻接链表中即可。 缺点：无法快速判断一条边$(u,v)$是否是图中的一条边，唯一的办法是在$Adj[u]$中搜索结点$v$，邻接矩阵克服了这一缺陷但付出的代价是更大的空间消耗。 邻接矩阵 对于图$G$的邻接矩阵表示一个$|V|\\times |V|$的矩阵$A=(a_{ij})$，该矩阵满足以下条件： $$a_{ij}=\\begin{cases} 1 ,若(i,j)\\in E\\\\ 0,其他 \\end{cases}$$ 不管一个图有多少边，邻接矩阵的空间需求皆为$\\Theta(V^2)$。在无向图中，邻接链表是一个堆成矩阵，即$A=A^T$，在某些应用中可以只存对角线及其以上部分而减少一半空间占用。同时，当$a_{ij}=\\omega(u,v)$也可以用于表示权重图。\n广度优先搜索（BFS） 算法分析 BFS是最简单的图搜索算法之一，Prim的最小生成树算法和Dijkstra的单源最短路径算法都使用了类似BFS的思想。\n在这里先抽象地描述一下广度优先搜索（BFS）和深度优先搜索（DFS）的区别。想象你身处迷宫之中，你的面前有n条路。 在BFS中，你会使用影分身之术，操纵分身走向每一条路，当其中一个分身抵达终点时，你就找到了抵达终点的最短路。 在DFS中，你选择其中一条路，一直走到尽头，如果是一条死路，那么你会回到之前的某一个分岔路口，选择另一条路，知道你走完所有能走的路，于是便找到了抵达终点的路。\nBFS能够计算从源节点$s$到每个可到达的结点的举例（最少的边数），同时生成一棵“广度优先搜索树”，该树以源结点$s$为根结点，包含所有可以从$s$到达的结点。该算法既可用于有向图，也可用于无向图。 广度优先搜索得名是因为该算法始终是将已发现结点和未发现结点之间的边界沿其广度方向向外扩展。也就是说，算法需要在发现所有距离源结点$s$为$k$的所有结点后才会发现距离源节点距离为$k+1$的结点。 在执行广度优先搜索的过程中将构造出一棵广度优先树，一开始，该树仅含有根结点（源结点$s$）作为已发现的结点。在扫描已发现结点$u$的邻接表时，每当发现一个未知结点$v$时，就将结点$u$和边$(u,v)$加入树中。由于每个结点最多被发现一次，因此最多只有一个父结点。\n在《算法导论》中，作者给图$G$中的每个结点定义了三种状态：$WHITE$（未发现结点），$BLACK$（已发现结点），$GRAY$（待发现结点，位于$white$结点与$black$结点之间）。 假定输入图$G=(V,E)$是以邻接表表示的： 1.将每个结点$u$的颜色存放在属性$u.color$中； 2.将$u$的前驱节点（父节点）存放在属性$u.\\pi$中（若$u=s$或$u$尚未被发现，则$u.\\pi=NULL$）； 3.将源结点到$u$的距离存放在属性$u.d$中。 下面给出BFS的伪代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 //$BFS(G,s)$ for each vertex $u\\in G.V-{s}$ $u.color=WHITE$ $u.d=\\infty$ $u.\\pi=NULL$ $s.color=GRAY$ $s.d=0$ $s.\\pi=NULL$ //初始化，源节点$s$为待发现结点，距离为0，无前驱结点 //源节点外的结点都是未发现结点 $Q=\\varnothing$ $ENQUEUE(Q,s)$ while $Q \\ne \\varnothing$ $u=DEQUEUE(Q)$ //出队，并对当前结点的邻接结点遍历，进入待发现状态并入队 for each $v \\in G.Adj[u]$ if $v.color == WHITE$ $v.color=GRAY$ $v.d=u.d+1$ $v.\\pi=u$ $ENQUEUE(Q,v)$ $u.color=BLACK$ //遍历结束后，当前结点从待发现结点变为已发现结点 时间复杂度分析 在初始化操作后，BFS不会再给任何点涂上白色，因此if(v.color==WHITE)保证了每个结点最多入队/出队一次，入队和出队的时间均为$\\Theta(1)$，因此对队列进行操作的总时间为$\\Theta(V)$。因为BFS只在一个结点出队时才对结点的邻接表进行扫描，因此每个邻接表最多只扫描一次，由于邻接表长度之和为$\\Theta(E)$，用于扫描邻接表的总时间为$\\Theta(E)$。综上，BFS的总运行时间为$\\Theta(V+E)$，是图$G$的邻接表大小的一个线性函数。\n深度优先搜索（DFS） 算法分析 深度优先搜索总是对最近才发现的结点$v$的出发边进行探索，直到该结点的所有出发边都被发现为止。一旦搜索过程发现的$v$的所有出发边都被发现，则搜索回溯到$v$的前驱结点，该过程一直持续到从源结点可以达到的所有结点都被发现为止。如果还存在尚未发现的结点，则深度优先搜索将从这些未被发现的结点中任选一个作为新的源结点，并重复同样的搜索过程，直到所有结点都被发现为止。 在DFS中，每当扫描已发现顶点$u$的邻接表，发现新的结点$v$时，令$v$的先辈域$\\pi[v]$为$u$（出于回溯的目的，可以通过递归的返回地址巧妙地完成）。与BFS不同的是，DFS生成的先辈子图可以由几棵树组成，因为搜索可能由多个源顶点开始重复进行。\n在《算法导论》中，类似于BFS，DFS也通过对结点着色表示结点的状态。开始时，每个结点都是$WHITE$，在搜索中被发现时置于$GRAY$，在当前搜索结束时（其邻接表被完全搜索后）置于$BALCK$，这可以保证每一顶点在搜索结束时只存在于一棵深度优先树中，因此这些树是不相交的。 除了创建深度优先森林，DFS同时为每个结点加盖时间戳。每个结点$v$由两个时间戳：当$v$第一次被发现（置于$GRAY$）时记录第一个时间戳$d[v]$，当结束检查$v$的邻接表（置于$BLACK$）时记录的第二个时间戳$f[v]$。显然，$d[v]\u0026lt;f[v]$。我们可以通过时间戳判断某一结点在DFS中的状态：$times\u0026lt;d[v]$时为$WHITE$，$d[v]\u0026lt;times\u0026lt;f[v]$时为$GRAY$，$times\u0026gt;f[v]$时为$BLACK$。 下面给出DFS的伪代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 //$DFS(G)$ for each vertex $u\\in V[G]$ do $color[u]=WHITE$ $\\pi[u]=NULL$ $times=0$ //初始化 for each vertex $u\\in V[G]$ do if $color[v]=WHITE$ then $DFS-VISIT(u)$//如果当前结点为被搜索过，那么开搜！ //$DFS-VISIT(u)$ $color[u]=GRAY$ $d[u]=time$ for each $v \\in Adj[u]$ do if $color[v]=WHITE$ then $\\pi[v]=u$ $DFS-VISIT(v)$ $color[u]=BLACK$ $f[u]=++time$ DFS的结果可能依赖于DFS的for each各个结点的访问顺序和DFS-VISIT的for each对邻接点的访问顺序，但在实践中这些不同的访问顺序往往不会引起什么问题，因为任何DFS结果通常都可以被有效利用，最终结果基本上等价。\n时间复杂度分析 初始化和遍历每个结点进行DFS-VISIT（不包括调用DFS-VISIT）的循环占用时间为$\\Theta(V)$。对于每个结点$v\\in V$，过程DFS-VISIT仅被调用一次，因为只有对$WHITE$顶点才会调用DFS-VISIT，且DFS-VISIT的第一件事就是将结点置为$GRAY$。在DFS-VISIT($v$)的一次执行过程中，循环被执行了$|Adj[v]|$次，由于$\\Sigma |Adj[v]| = \\Theta(E)$，故DFS0VUSUT的循环代价为$\\Theta(E)$。因此，DFS的运行时间为$\\Theta(V+E)$。\nDFS的性质 如果把发现结点$u$用左括号\u0026quot;$(u$\u0026ldquo;表示，完成用右括号\u0026rdquo;$u)$\u0026ldquo;表示，那么在各级括号正确嵌套的前提下，发现于完成时间的记载就是一个完善的表达式。\n推论（后裔区间的嵌套） 在一个（有向/无向）图$G$中的深度优先森林中，结点$v$是结点$u$的后裔，当且仅当$d[u]\u0026lt;d[v]\u0026lt;f[v]\u0026lt;f[u]$（由括号定理证明）。\n边的分类 树边（tree edge）深度优先森林中的边，如果结点$v$是在探寻边$(u,v)$时被首次发现，那么$(u,v)$就是一条树边。 反向边（$B$,back edge）深度优先树中结点$u$到某一祖先$v$的边（可能产生自环的边）。 正向边（$F$,forward edge）深度优先树中连接$u$到某后裔$v$的非树边$(u,v)$。 交叉边（$C$,cross edge）其他类型的边，存在于同一深度优先树种的两个结点（其中一个结点不是另一个结点的祖先）之间或不同深度优先树的结点之间。\n拓扑排序 算法分析 对一个有向无环图（$DAG$）$G=(V,E)$进行拓扑排序后，结果为该图所有结点的一个线性序列，满足若$G$包含边$(u,v)$，则在该序列种$u$出现在$v$的前面（若图有环路则不可能存在这样的线性序列）。简单来看，图的拓扑排序可以看成所有结点沿水平线排列而成的一个序列，其中所有有向边均从左指向右。\n1 2 3 4 //$TOPOLOGICAL-SORT(G)$ call $DPS(G)$ to compute finishing times $f[v]$ for rach vertex $v$ as each vertex is finished, insert it onto the front of a linked list **return** the linked list of vertices 因为DFS的运行时间为$\\Theta(V+E)$，而将$|V|$个结点中的每一结点插入链表所需的时间为$\\Theta(1)$，因此拓扑排序的运行时间为$\\Theta(V+E)$。\n应用 给任务排序（Ordering Tasks，UVa 10305)\n假设有$n$个变量，还有$m$个二元组$(u,v)$，分别表示变量$u$小于$v$。那么，所有变量从小到大排列起来应该是什么样子？例如，有4个变量$a,b,c,d$，若已知$a\u0026lt;b$，$c\u0026lt;b$，$d\u0026lt;c$，则这4个变量的排序可能是$a\u0026lt;d\u0026lt;c\u0026lt;b$，找出其中一个可能的答案即可。\nSolution：把每个变量看成一个点，“小于”关系看成有向边，则得到了一个有向图，任务转化为对这张图进行拓扑排序。 这里给出紫书（《算法竞赛入门经典-第二版》-刘汝佳）的代码： 其中，$c[u]=0$表示从未访问过，$c[u]=1$表示已经访问过且递归访问所有子孙（DFS已返回），$c[u]=-1$表示正在访问（递归调用dfs(u)或正在栈帧中，尚未返回）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 int c[maxn], topo[maxn], t; bool dfs(int u){ c[u] = -1;//访问标志 for(int v = 0; v \u0026lt; n; v++)if(G[u][v]){ if(c[v] == -1) return false;//存在环路，退出 else if(c[v] == 1 \u0026amp;\u0026amp; !dfs(v)) return false;//访问完所有子孙，退出 } c[u] = 1; topo[--t] = u; return true; } bool toposort(){ t = n; memset(c, 0, sizeof(c)); for(int u = 0; u \u0026lt; n; u++)if(!c[u]) if(!dfs(u)) return false; return true; } ","date":"2024-12-07T14:55:10+08:00","image":"https://kaigezheng.github.io/p/algorithm2/img/cover_hu_a0aad241da7692fe.jpg","permalink":"https://kaigezheng.github.io/p/algorithm2/","title":"基本的图算法"},{"content":"前言 前言写于2025/05/22 23:58，在一刻钟前我刚完成hot 100题单。突然感慨于身为计算机科班学生与算法（这里仅指传统算法，非AI/ML算法）的孽缘，从高考后向往ACM学习C/C++和紫书（被紫书摧残），大一开始为成为ACMer而日刷洛谷夜刷codeforces，到大二认清现实放弃ACM，却因离不开算法而浅读《算法导论》，到大三为找回感觉和准备机试而速通leetcode hot 100\u0026hellip;不知未来如何，起码本科的每个阶段都离不开算法（可惜咱算法水平一般）。本篇博文用于记录我从3月6日开始到今天写hot 100的过程。\nleetcode hop 100\n哈希 两数之和 难度：Easy\n1. 两数之和\n给定数组，求$x + y = target$的任意解。\n暴力枚举很容易实现，$\\Theta(n^2)$的时间复杂度和$\\Theta(1)$的空间复杂度。使用哈希表可以实现空间换时间，让时间和空间复杂度都为$\\Theta(n)$。\n遍历元素时判断target - x是否存在哈希表，若不存在则将(x, index)存入哈希表中，只用遍历一遍数组。\n1 2 3 4 5 6 7 8 9 10 11 12 class Solution { public: vector\u0026lt;int\u0026gt; twoSum(vector\u0026lt;int\u0026gt;\u0026amp; nums, int target) { unordered_map\u0026lt;int, int\u0026gt; hash; for(int i = 0; i \u0026lt; nums.size(); ++i) { auto it = hash.find(target - nums[i]); if(it != hash.end()) return {it-\u0026gt;second, i}; hash[nums[i]] = i; } return {}; } }; 字母异位词分组 难度：Medium\n49. 字母异位词分组\n给定字符串数组，将字母相同顺序不同的单词组合再一起按任意顺序返回列表。\n第一个思路是字符串哈希，应该是可以过掉大部分样例的（但也有被卡单哈希模数和溢出的风险）。这里将每个单词排序后插入哈希表，实现方法也很简单。拷贝结果的方法值得参考。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Solution { public: vector\u0026lt;vector\u0026lt;string\u0026gt;\u0026gt; groupAnagrams(vector\u0026lt;string\u0026gt;\u0026amp; strs) { unordered_map\u0026lt;string, vector\u0026lt;string\u0026gt;\u0026gt; hash; for(string s : strs) { string data = s; sort(s.begin(), s.end()); hash[s].emplace_back(data); } vector\u0026lt;vector\u0026lt;string\u0026gt;\u0026gt; ans; for(auto x : hash) ans.emplace_back(x.second); return ans; } }; 最长连续序列 难度：Medium\n128. 最长连续序列\n给定未排序的数组，找出数字连续的最长序列长度（不要求在原数组中连续）。\n使用std::set的自动排序可以轻松实现，虽然能过题，但是速度非常慢，只是样例没那么严格才能过的题，时间复杂度是$\\Theta(nlogn)$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public: int longestConsecutive(vector\u0026lt;int\u0026gt;\u0026amp; nums) { if(!nums.size()) return 0; set\u0026lt;int\u0026gt; s; for(int x : nums) { s.insert(x); } int cnt = 0; int max_cnt = 1; int last = -1; for(int x : s) { if(last + 1 == x) { max_cnt = max(++cnt, max_cnt); }else{ cnt = 1; } last = x; } return max_cnt; } }; 正解（哈希表）的构思很巧妙，首先遍历一遍数组去重，然后再遍历一遍哈希表。遍历时（假设当前元素为x），若x - 1在表中则跳过（因为这表示这个元素是最长序列的中间点或尾部，而非起点）；若x - 1不在表中，表明x只可能是起点，开始不断寻找x + 1是否在表内。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public: int longestConsecutive(vector\u0026lt;int\u0026gt;\u0026amp; nums) { unordered_set\u0026lt;int\u0026gt; hash; for(int num : nums) hash.insert(num); int max_cnt = 0; for(int num : hash) { if(!hash.count(num - 1)) { int cur = num; int cnt = 1; while(hash.count(cur + 1)) { ++cur; ++cnt; } max_cnt =max(cnt, max_cnt); } } return max_cnt; } }; 双指针 移动零 难度：Easy\n283. 移动零\n给定数字，将所有0移到末尾（要求原地操作）。\n使用双指针，左指针指向已处理好的序列尾部，右指针指向待处理序列头部。右指针不断移动，当指向非零元素时左右指针元素交换，且左指针右移。这样保证了左指针左边均非零，右指针到左指针之间均为0。\n盛最多水的容器 难度：Medium\n11. 盛最多水的容器\n给定数组height，求$(j - i) * min(height[i], height[j])$的最大值。\n$\\Theta(n^2)$的暴力解法显然会TLE，这题的双指针有些难想到。\n首先设置分别从头开始遍历和从尾开始遍历的双指针，考虑以下结论：\n若向内移动短板，$min(height[i], height[j])$可能增大，因此$S$可能增大\n若向内移动长板，$min(height[i], height[j])$可能不变或变小，又$j-i$一定变小，因此$S$一定变小\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { public: int maxArea(vector\u0026lt;int\u0026gt;\u0026amp; height) { int ans = 0, i = 0, j = height.size() - 1; while(i \u0026lt; j) { int result = (j - i) * min(height[i], height[j]); ans = max(ans, result); if(height[i] \u0026gt; height[j]) --j; else ++i; } return ans; } }; 三数之和 难度：Medium\n15. 三数之和\n与两数之和（哈希）类似。题目主要分为两个部分，寻找满足条件的解和去重。找到解再通过哈希表去重有些麻烦，在遍历前排序，直接保证解$(a, b, c)$满足$a \\leq b \\leq c$即可方便地去重。\n暴力的时间复杂度为$\\Theta(n^3)$（三重循环），可以改为一重循环+双指针，时间复杂度为$\\Theta(n * n)=\\ThetaO(n^3)$，在左指针元素递增时右指针元素递减。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Solution { public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; threeSum(vector\u0026lt;int\u0026gt;\u0026amp; nums) { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; ans; sort(nums.begin(), nums.end()); for(int i = 0; i \u0026lt; nums.size(); ++i) { if(i \u0026amp;\u0026amp; nums[i] == nums[i - 1]) continue; int l = i + 1; int r = nums.size() - 1; while(l \u0026lt; r) { while(l \u0026gt; i + 1 \u0026amp;\u0026amp; l \u0026lt; nums.size() \u0026amp;\u0026amp; nums[l] == nums[l - 1]) ++l; while(r \u0026lt; nums.size() - 1 \u0026amp;\u0026amp; nums[r] == nums[r + 1]) --r; if(l \u0026gt;= r) break; if(nums[i] + nums[l] + nums[r] \u0026gt; 0) --r; else if (nums[i] + nums[l] + nums[r] \u0026lt; 0) ++l; else{ ans.push_back({nums[i], nums[l], nums[r]}); ++l; --r; } } } return ans; } }; 接雨水 难度：Hard\n42. 接雨水\n给定数组表示每个宽度为1的柱子的高度图，计算按此排列的柱子下雨后能接多少雨水。\n对于下标$i$，能接的水等于下表$i$两边的最大高度的最小值减去$height[i]$。暴力法就是对于每个$i$都分别向左和向右遍历最大高度，时间复杂度是$\\Theta(n^3)$。\n构造动态规划 使用动态规划，可以在$\\Theta(n)$的时间内预处理得到每个位置两边的最大高度。维护两个长度为$n$的数组$leftMax$和$rightMax$，分别表示$i$及左边的最大值和$i$及右边的最大值。\n$leftMax[i] = max(leftMax[i - 1], height[i])$(正向遍历)\n$rightMax[i] = max(rightMax[i + 1], height[i])$(逆向遍历)\n于是下标$i$处能接的雨水量等于$min(leftMax[i], rightMax[i]) - height[i]$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Solution { public: int trap(vector\u0026lt;int\u0026gt;\u0026amp; height) { int n = height.size(); vector\u0026lt;int\u0026gt; leftMax, rightMax; leftMax.resize(n); rightMax.resize(n); /* 预处理:维护leftMax和rightMax */ leftMax[0] = height[0]; for(int i = 1; i \u0026lt; n; ++i) { leftMax[i] = max(leftMax[i - 1], height[i]); } rightMax[n - 1] = height[n - 1]; for(int i = n - 2; i \u0026gt;= 0; --i) { rightMax[i] = max(rightMax[i + 1], height[i]); } /* DP */ int ans = 0; for(int i = 0; i \u0026lt; n; ++i) { ans += min(leftMax[i], rightMax[i]) - height[i]; } return ans; } }; 双指针优化 使用双指针就不需要维护数组$leftMax$和$rightMax$了，可以将空间复杂度从$\\Theta(n)$降低到$\\Theta(1)$，但是很难想。\n维护两个指针$left$和$right$，以及两个变量$leftMax$和$rightMax$，$left$向右移动，$right$向左移动。\n使用$height[left]$和$height[right]$的值更新$leftMax$和$rightMax$\n如果$height[left] \\lt height[right]$，则必有$leftMax \\lt rightMax$，此时$left$处能接的雨水量等于$leftMax - height[left]$，$left$右移\n$right$的遍历基本同上，直到双指针相遇结束\n滑动窗口 涉及到子串（连续非空字符序列，并非子序列），就可以考虑一下滑动窗口了。\n无重复字符的最长子串 难度：Medium\n3. 无重复字符的最长子串\n给定一个字符串，找出不含重复字符的额最长子串长度。\n有点类似双指针，滑动窗口也需要$left$和$right$控制滑动窗口左右边界\n当$right + 1$元素存在且不重复时，向右扩大窗口\n不满足扩大窗口条件时，向右缩小窗口并将窗口外元素排除出哈希集合\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public: int lengthOfLongestSubstring(string s) { unordered_set\u0026lt;char\u0026gt; hash; int ans = 0, right = -1; for(int left = 0; left \u0026lt; s.size(); ++left) { /* 移动左边界直到子串不重复 */ if(left != 0) hash.erase(s[left - 1]); /* 右边界存在且不重复时扩大窗口 */ while(right + 1 \u0026lt; s.size() \u0026amp;\u0026amp; !hash.count(s[right + 1])) { hash.insert(s[right + 1]); ++right; } ans = max(ans, right - left + 1); } return ans; } }; 找到字符串中所有字母异位词 难度：Medium\n438. 找到字符串中所有字母异位词\n给定两个字符串s和p，找到s中所有p的异位词的子串，返回这些子串的起始索引。\n写了段又臭又长的代码，但反正思路是对的。这个滑动窗口比上一个简单，因为只要开始滑动，$right$和$left$是同时递增的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class Solution { public: vector\u0026lt;int\u0026gt; findAnagrams(string s, string p) { vector\u0026lt;int\u0026gt; ans; if(s.size() \u0026lt; p.size()) return ans; vector\u0026lt;int\u0026gt; hash; vector\u0026lt;int\u0026gt; hash_table(26, 0); vector\u0026lt;int\u0026gt; hash_table2(26, 0); for(int i = 0; i \u0026lt; p.size(); ++i) { ++hash_table[p[i] - \u0026#39;a\u0026#39;]; } int right = 0; for(int left = 0; left \u0026lt; s.size() - p.size() + 1; ++left) { if(left != 0) { --hash_table2[hash[0]]; hash.erase(hash.begin()); } while(right - left \u0026lt; p.size() \u0026amp;\u0026amp; right \u0026lt; s.size()) { hash.emplace_back(s[right] - \u0026#39;a\u0026#39;); ++hash_table2[s[right++] - \u0026#39;a\u0026#39;]; } bool ok = true; for(int i = 0; i \u0026lt; 26; ++i) { if(hash_table2[i] != hash_table[i]) { ok = false; break; } } if(ok) ans.emplace_back(left); } return ans; } }; 子串 和为K的子数组 难度：Medium\n560. 和为K的子数组\n给定数组和一个整数$k$，返回该数组中和为$k$的子数组的个数。\n写了一个非常之蠢的$\\Theta(n^2)$的前缀和（这和暴力有什么区别啊喂），竟然能AC。正解需要前缀和+哈希表优化。\n定义$s[i + 1]$为$[0..i]$里所有数的和，则$s[i]$可以由$s[i - 1]$递推而来，即$s[i] = s[i - 1] + nums[i - 1]$。\n设$i \\lt j$，如果$nums[i]$到$nums[j-1]$的元素和等于$k$，用前缀和表示就是$s[j] - s[i] == k$，移项得$s[i]==s[j]-k$。\n即，我们需要计算有多少个$s[i]$满足$i \\lt j$且$s[i] == s[j] - k$。既要求$s[i]$的个数又要求$s[j]$的个数，那么用哈希表优化。（已知$s[j]$和$k$，统计$s[0]$到$s[j-1]$长有多少个数等于$s[j] - k$）\n这个做法挺难理解的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Solution { public: int subarraySum(vector\u0026lt;int\u0026gt;\u0026amp; nums, int k) { int n = nums.size(); vector\u0026lt;int\u0026gt; s(n + 1); /* 维护前缀和数组 */ s[0] = 0; for(int i = 0; i \u0026lt; n; ++i) { s[i + 1] = s[i] + nums[i]; } int ans = 0; unordered_map\u0026lt;int, int\u0026gt; hash; for(int x : s) { /* 对于每一个j(右端), 寻找s[?] - k的个数, O(1) */ ans += hash.contains(x - k) ? hash[x - k] : 0; ++hash[x]; } return ans; } }; 滑动窗口最大值 难度：Hard\n239. 滑动窗口最大值\n给定数组和一个大小为k的滑动窗口（从左往右移动，每次移动一位），只能看到滑动窗口内的k个数字，返回滑动窗口中的最大值。\n优先队列（大根堆） 优先队列中的元素数量不一定等于滑动窗口大小（因为堆顶元素是最大值，但这个最大值不一定在滑动窗口中）。初始先将$k$个元素放入大根堆中。每次向右移动窗口就可以放一个心得元素到大根堆中，然后判断堆顶元素下标是否在滑动窗口范围内。\n将一个元素放入优先队列的时间复杂度为$\\Theta(log n)$，因此总体时间复杂度为$\\Theta(nlogn)$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public: vector\u0026lt;int\u0026gt; maxSlidingWindow(vector\u0026lt;int\u0026gt;\u0026amp; nums, int k) { int n = nums.size(); priority_queue\u0026lt;pair\u0026lt;int, int\u0026gt;\u0026gt; q; // 大顶堆 for(int i = 0; i \u0026lt; k; ++i) { q.emplace(nums[i], i); } vector\u0026lt;int\u0026gt; ans = {q.top().first}; for(int i = k; i \u0026lt; n; ++i) { q.emplace(nums[i], i); while(q.top().second \u0026lt;= i - k) { q.pop(); } ans.emplace_back(q.top().first); } return ans; } }; 双向队列（大根堆） 使用deque模拟滑动窗口（当然deque内的元素数量不一定等于滑动窗口大小），满足队首存储当前窗口的最大值，依据滑动窗口下标决定是否要弹出。由于不需要自动排序，时间复杂度可以优化为$\\Theta(n)$\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Solution { public: vector\u0026lt;int\u0026gt; maxSlidingWindow(vector\u0026lt;int\u0026gt;\u0026amp; nums, int k) { int n = nums.size(); deque\u0026lt;int\u0026gt; dq; vector\u0026lt;int\u0026gt; ans(n - k + 1); for(int i = 0; i \u0026lt; k; ++i) { /* 如果插入的元素更大，就可以把前面的元素去掉了 -\u0026gt; 队首是最大值*/ while(!dq.empty() \u0026amp;\u0026amp; dq.back() \u0026lt; nums[i]) dq.pop_back(); dq.push_back(nums[i]); } ans[0] = dq.front(); for(int i = k; i \u0026lt; n; ++i) { if(dq.front() == nums[i - k]) dq.pop_front(); while(!dq.empty() \u0026amp;\u0026amp; dq.back() \u0026lt; nums[i]) dq.pop_back(); dq.push_back(nums[i]); ans[i - k + 1] = dq.front(); } return ans; } }; 最小覆盖子串 难度：Hard\n76. 最小覆盖子串\n给定字符串s和t，返回s中涵盖t所有字符的最小子串或空。\n官解写得太难看懂了，终于找到了一个可以看懂的代码。使用滑动窗口+哈希表，在滑动时维护哈希表和计数器cnt，代码值得学习。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Solution { public: string minWindow(string s, string t) { int n = s.size(), m = t.size(); if(n \u0026lt; m) return \u0026#34;\u0026#34;; vector\u0026lt;int\u0026gt; cnt(128, 0); for(const char \u0026amp;x : t) cnt[x]++; int l = 0, cntC = m, ansL = 0, ansR = n + 1; for(int r = 0; r \u0026lt; n; r++) { // 如何出现过则计数器减一 if(cnt[s[r]] \u0026gt; 0) cntC--; // 当前字符在计数器中减一 cnt[s[r]]--; // 如果左指针元素可以不出现，则右移 while(cnt[s[l]] \u0026lt; 0) { cnt[s[l]]++; l++; } // 如果计数器为0说明找到了一个包含t的子串 if(cntC == 0) { if(r - l \u0026lt; ansR - ansL) ansL = l, ansR = r; // 左指针右移，继续滑动 cnt[s[l]]++; l++; cntC++; } } return ansR == n + 1 ? \u0026#34;\u0026#34; : s.substr(ansL, ansR - ansL + 1); } }; 普通数组 最大子数组和 难度：Medium\n53. 最大子数组和\n给定整数数组，求连续子数组的最大和。\n靠肌肉记忆秒了，贪心算法可解。\n1 2 3 4 5 6 7 8 9 10 11 12 class Solution { public: int maxSubArray(vector\u0026lt;int\u0026gt;\u0026amp; nums) { long long ans = -10005, cur = 0; for(const int\u0026amp; x: nums) { cur += x; ans = max(cur, ans); if(cur \u0026lt; 0) cur = 0; } return ans; } }; 合并区间 难度：Medium\n56. 合并区间\n给定数组pair\u0026lt;int, int\u0026gt;[]，合并所有重叠的区间并返回一个不重叠的区间数组。\n写了半天错误的解法，发现不能简单地扫一遍数组并维护一个一维数组。这样会导致类似[[1, 4], [5, 6]]这样的区间输出[1, 6]，而实际上中间并不连续，一开始就走上了不归路\u0026hellip;\n正解是按区间的左端点排序，那么可以合并的区间一定是连续的。用结果的最后一个区间判断是否重叠，如果重叠就更新最后一个区间的右端点，否则加入新区间。代码值得学习。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; merge(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; intervals) { sort(intervals.begin(), intervals.end(), [](auto\u0026amp; x, auto\u0026amp; y) { return x[0] \u0026lt; y[0]; }); /* 虽然默认也是按第一个元素排的 */ vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; result; result.push_back(intervals[0]); for(int i = 1; i \u0026lt; intervals.size(); ++i) { int start = intervals[i][0], end = intervals[i][1], n = result.size(); if(start \u0026lt;= result[n - 1][1]) { result[n - 1][1] = max(result[n - 1][1], end); } else { result.push_back(intervals[i]); } } return result; } }; 轮转数组 难度：Medium\n189. 轮转数组\n给定整数数组，将数组中的元素向右原地轮转k个位置。\n用最直接的方法来模拟即可，记得防止溢出(k %= nums.size())。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public: void rotate(vector\u0026lt;int\u0026gt;\u0026amp; nums, int k) { vector\u0026lt;int\u0026gt; tmp; k %= nums.size(); // 记得防溢出! for(int i = nums.size() - k; i \u0026lt; nums.size(); ++i) { tmp.push_back(nums[i]); } for(int i = nums.size() - 1; i \u0026gt;= k; --i) { nums[i] = nums[i - k]; } for(int i = 0; i \u0026lt; k; ++i) { nums[i] = tmp[i]; } } }; 除自身以外数组的乘积 难度：Medium\n238. 除自身以外数组的乘积\n给定整数数组nums，返回数组answer，其中answer[i]等于nums中除nums[i]外其余各元素乘积。（禁用除法，要求时间复杂度$\\Theta(n)$）\n初见无思路。题意等价于，只要知道i左边所有数的乘积和i右边所有数的乘积即可得到answer[i]-\u0026gt;前缀和！\n定义pre[i]表示从nums[0]到nums[i-1]的乘积；定义suf[i]表示从nums[i+1]到nums[n-1]的乘积。\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { public: vector\u0026lt;int\u0026gt; productExceptSelf(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int n = nums.size(); vector\u0026lt;int\u0026gt; pre(n, 1); for(int i = 1; i \u0026lt; n; ++i) pre[i] = pre[i - 1] * nums[i - 1]; vector\u0026lt;int\u0026gt; suf(n, 1); for(int i = n - 2; i \u0026gt;= 0; --i) suf[i] = suf[i + 1] * nums[i + 1]; vector\u0026lt;int\u0026gt; ans(n); for(int i = 0; i \u0026lt; n; ++i) ans[i] = pre[i] * suf[i]; return ans; } }; 缺失的第一个正数 难度：Hard\n41. 缺失的第一个正数\n给定一个未排序的整数数组，找出其中没有出现的最小的正整数（要求时间复杂度$\\Theta(n)$，空间复杂度$\\Theta(1)$）。\n初见无思路。很诡异的解法：1）将所有负数修改为$n+1$；2）遍历数组，如果绝对值在$[1, N]$中则将$|x| - 1$位置的数改为负数；3）遍历数组，如果每个数都是负数，则答案是$N + 1$，否则答案是第一个正数的位置加1（一种用位置index来映射真实数字的哈希表）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Solution { public: int firstMissingPositive(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int n = nums.size(); for(int\u0026amp; num: nums) { if(num \u0026lt;= 0) num = n + 1; } for(int i = 0; i \u0026lt; n; ++i) { int num = abs(nums[i]); if(num \u0026lt;= n) nums[num - 1] = -abs(nums[num - 1]); } for(int i = 0; i \u0026lt; n; ++i) { if(nums[i] \u0026gt; 0) return i + 1; } return n + 1; } }; 矩阵 矩阵置零 难度：Medium\n73. 矩阵置零\n给定mxn的矩阵，如果一个元素为0，使用原地算法将所在行列所有元素都设0。\nSolution 1（时间复杂度$\\Theta(2mn)$，空间复杂度$\\Theta(m+n)$）：用两个数组，分别记录每一行每一列是否有零出现，最后更新原矩阵\nSolution 2（时间复杂度$\\Theta(2mn)$，空间复杂度$\\Theta(2)$）：在Solution 1的基础上优化，用矩阵的第一行和第一列替代两个标记数组，并用两个标记变量分别记录第一行和第一列是否包含0。节省了空间，不过步骤麻烦了很多。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # Solution 2 class Solution { public: void setZeroes(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; matrix) { int m = matrix.size(); int n = matrix[0].size(); bool row_1 = false, col_1 = false; // 检查第一行和第一列 for(int i = 0; i \u0026lt; m; ++i) if(!matrix[i][0]) row_1 = true; for(int i = 0; i \u0026lt; n; ++i) if(!matrix[0][i]) col_1 = true; // 检查其余矩阵 for(int i = 1; i \u0026lt; m; ++i) { for(int j = 1; j \u0026lt; n; ++j) { if(!matrix[i][j]) matrix[i][0] = matrix[0][j] = 0; } } // 更新全局矩阵(注意是从index=1开始) for(int i = 1; i \u0026lt; m; ++i) if(!matrix[i][0]) for(int j = 0; j \u0026lt; n; ++j) matrix[i][j] = 0; for(int j = 1; j \u0026lt; n; ++j) if(!matrix[0][j]) for(int i = 0; i \u0026lt; m; ++i) matrix[i][j] = 0; // 更新第一行和第一列 if(row_1) for(int i = 0; i \u0026lt; m; ++i) matrix[i][0] = 0; if(col_1) for(int i = 0; i \u0026lt; n; ++i) matrix[0][i] = 0; } }; 螺旋矩阵 难度：Medium\n54. 螺旋矩阵\n给定mxn的矩阵，按照顺时针螺旋向里的顺序返回矩阵中的所有元素。\n暴力模拟，非常不优雅的代码，就不贴了。\n旋转图像 难度：Medium\n48. 旋转图像\n给定nxn的矩阵，原地顺时针旋转90度。\nSolution 1（时间复杂度$\\Theta(n^2)$，空间复杂度$\\Theta(n^2)$）：找规律可以发现$matrix[row][col]$在旋转后的位置为$matrix[col][n - row - 1]$。因此用一个二维辅助矩阵存储旋转后的矩阵，最后更新即可。\nSolution 2（时间复杂度$\\Theta(n^2)$，空间复杂度$\\Theta(1)$）：很巧妙，顺时针旋转90°等价于先水平翻转一次（$matrix[row][col] \\leftrightarrow matrix[n - row - 1][col]$），再主对角线翻转一次（$matrix[row][col] \\leftrightarrow matrix[col][row]$）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # Solution 2 class Solution { public: void rotate(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; matrix) { int n = matrix.size(); // 水平翻转 for(int i = 0; i \u0026lt; n / 2; ++i) { for(int j = 0; j \u0026lt; n; ++j) { swap(matrix[i][j], matrix[n - i - 1][j]); } } // 主对角线翻转 for(int i = 0; i \u0026lt; n; ++i) { for(int j = 0; j \u0026lt; i; ++j) { swap(matrix[i][j], matrix[j][i]); } } } }; 搜索二维矩阵II 难度：Medium\n240. 搜索二维矩阵II\n给定mxn的矩阵，每行的元素从左到右升序，每列的元素从上到下升序，搜索目标值target是否存在。\nSolution 1：跟搜索二维矩阵I不同，不是顺序严格递增，因此不能先对列二分，锁定某一行后再二分，但是可以对每一行都二分，代码简洁值得学习。\n1 2 3 4 5 6 7 8 9 10 11 # Solution 1 class Solution { public: bool searchMatrix(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; matrix, int target) { for(const auto\u0026amp; row: matrix) { auto it = lower_bound(row.begin(), row.end(), target); if(it != row.end() \u0026amp;\u0026amp; *it == target) return true; } return false; } }; Solution 2：很巧妙的方法，以右上角为二叉树的根节点，就得到了一个向左子树移动变小，向右子树移动变大的二叉搜索树！天才解法！\n1 2 3 4 5 6 7 8 9 10 11 12 class Solution { public: bool searchMatrix(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; matrix, int target) { int i = matrix.size() - 1, j = 0; while(i \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt; matrix[0].size()) { if(matrix[i][j] \u0026gt; target) --i; else if(matrix[i][j] \u0026lt; target) ++j; else return true; } return false; } }; 链表 相交链表(哈希、双指针) 难度：Easy\n160. 相交链表(Easy):给两个单链表的头节点，找出并返回两个单链表相交的起始节点。（Note:不是值相等，而是内存空间相等）\n哈希集合 时间复杂度：$\\Theta (m+n)$\n空间复杂度：$\\Theta (m)$\n遍历单链表A并把每个元素（内存地址）存储到集合(unordered_set)中，遍历单链表B并判断A中是否已经存在(unordered_set.count(..))。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public: ListNode *getIntersectionNode(ListNode *headA, ListNode *headB) { unordered_set\u0026lt;ListNode *\u0026gt; visited; ListNode *temp = headA; while(temp != nullptr) { visited.insert(temp); temp = temp-\u0026gt;next; } temp = headB; while(temp != nullptr) { if(visited.count(temp)) { return temp; } temp = temp-\u0026gt;next; } return nullptr; } }; 双指针 时间复杂度：$\\Theta (m+n)$\n空间复杂度：$\\Theta (1)$\n使用两个指针分别同时遍历单链表A和单链表B，并在各自遍历完后切换到单链表B和单链表A，如果有相交节点，一定会同时遇到。\n1 2 3 4 5 6 7 8 9 10 11 12 class Solution { public: ListNode *getIntersectionNode(ListNode *headA, ListNode *headB) { ListNode *A = headA, *B = headB; if(headA == nullptr || headB == nullptr) return nullptr; while(A != B) { A = (A == nullptr)? headB : A-\u0026gt;next; B = (B == nullptr)? headA : B-\u0026gt;next; } return A; } }; 反转链表（递归） 难度：Easy\n206. 反转链表\n给一个单链表的头节点head，反转链表并返回反转后的链表。\n递归 只有至少有两个元素时才有必要反转（因此递归出口是head \u0026amp;\u0026amp; head-\u0026gt;next时需要反转，递归出口是!head || !head-\u0026gt;next）。由于需要在链表尾部开始递归至链表头，因此先进入递归。简单画个图就明白反转的目的就是把当前节点的下一节点的next指向当前节点（head-\u0026gt;next-\u0026gt;next = head），同时把当前节点的下一节点的next节点置空（head-\u0026gt;next = nullptr）以避免环。最后返回这个节点。\n1 2 3 4 5 6 7 8 9 10 11 12 class Solution { public: ListNode* reverseList(ListNode* head) { if(head == nullptr || head-\u0026gt;next == nullptr) { return head; } ListNode* newHead = reverseList(head-\u0026gt;next); head-\u0026gt;next-\u0026gt;next = head; head-\u0026gt;next = nullptr; return newHead; } }; 迭代 迭代的思路更好理解，用双指针不断修改当前节点的next即可，用两个指针的目的是为了保护上一个节点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Solution { public: ListNode* reverseList(ListNode* head) { ListNode* pre = nullptr; ListNode* cur = head; while(cur) { ListNode* next = cur-\u0026gt;next; cur-\u0026gt;next = pre; // 移动指针 pre = cur; cur = next; } return pre; } }; 回文链表 难度：Easy\n234. 回文链表\n不难，跳过。\n环形链表 难度：Easy\n141. 环形链表\n判断链表中是否存在环。\nSolution 1：哈希表，遍历链表直到空，如果遇到曾访问过的节点表示有环\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Solution 1 class Solution { public: bool hasCycle(ListNode *head) { unordered_set\u0026lt;ListNode*\u0026gt; hash; while(head != nullptr) { if(hash.count(head)) { return true; } hash.insert(head); head = head-\u0026gt;next; } return false; } }; Solution 2：快慢指针，可以将空间优化到$\\Theta(1)$。Floyd判圈法的两个要点：1）通过快慢指针是否相遇判断是否存在环；2）判断存在环后，将慢指针放回起点，快慢指针同步移动，相遇点就是环起点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Solution 2 class Solution { public: bool hasCycle(ListNode *head) { if(!head || !head-\u0026gt;next) return false; ListNode* slow = head; ListNode* fast = head-\u0026gt;next; while(slow != fast) { if(!fast || !fast-\u0026gt;next) return false; slow = slow-\u0026gt;next; fast = fast-\u0026gt;next-\u0026gt;next; } return true; } }; 环形链表II 难度：Medium\n142. 环形链表II\n在判断是否有环的前提下，返回环起点或null。解法同上，哈希表或快慢指针，只是改成返回head。\n快慢指针的解法略有一些细节变化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public: ListNode *detectCycle(ListNode *head) { if(!head || !head-\u0026gt;next) return nullptr; /* 快慢指针务必初始化在同一起点 */ ListNode* slow = head; ListNode* fast = head; /* 判断条件是fast和fast-\u0026gt;next存在 */ while(fast \u0026amp;\u0026amp; fast-\u0026gt;next) { slow = slow-\u0026gt;next; fast = fast-\u0026gt;next-\u0026gt;next; if(slow == fast) break; } if(slow != fast) return nullptr; slow = head; while(slow != fast) { slow = slow-\u0026gt;next; fast = fast-\u0026gt;next; } return slow; } }; 合并两个有序链表 难度：Easy\n21. 合并两个有序链表\n双指针可解，写的时候注意要记录一下头节点的位置，同时ans不能直接初始化为list1或list2。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class Solution { public: ListNode* mergeTwoLists(ListNode* list1, ListNode* list2) { ListNode* ans = new ListNode(-1); ListNode* prev = ans; ListNode* p1 = list1, *p2 = list2; while(p1 || p2) { if(p1 \u0026amp;\u0026amp; p2) { if(p1-\u0026gt;val \u0026gt; p2-\u0026gt;val) { ans-\u0026gt;next = p2; p2 = p2-\u0026gt;next; } else { ans-\u0026gt;next = p1; p1 = p1-\u0026gt;next; } } else if(p1) { ans-\u0026gt;next = p1; p1 = p1-\u0026gt;next; } else if(p2) { ans-\u0026gt;next = p2; p2 = p2-\u0026gt;next; } ans = ans-\u0026gt;next; } return prev-\u0026gt;next; } }; 两数相加 难度：Medium\n2. 两数相加\n给定两个链表表示两个非负整数（逆序存储各数字），将两数相加并返回逆序链表。\n简单模拟一下加数和进位即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Solution { public: ListNode* addTwoNumbers(ListNode* l1, ListNode* l2) { int first = (l1-\u0026gt;val + l2-\u0026gt;val) % 10, next = (l1-\u0026gt;val + l2-\u0026gt;val) / 10; l1 = l1-\u0026gt;next, l2 = l2-\u0026gt;next; ListNode* result = new ListNode(first); ListNode* ans = result; while(l1 || l2) { ListNode* current = new ListNode(); if(l1 \u0026amp;\u0026amp; l2) { current-\u0026gt;val = (l1-\u0026gt;val + l2-\u0026gt;val + next) % 10; next = (l1-\u0026gt;val + l2-\u0026gt;val + next) / 10; } else if(l1) { current-\u0026gt;val = (l1-\u0026gt;val + next) % 10; next = (l1-\u0026gt;val + next) / 10; } else if(l2) { current-\u0026gt;val = (l2-\u0026gt;val + next) % 10; next = (l2-\u0026gt;val + next) / 10; } result-\u0026gt;next = current; result = result-\u0026gt;next; if(l1) l1 = l1-\u0026gt;next; if(l2) l2 = l2-\u0026gt;next; } if(next != 0) result-\u0026gt;next = new ListNode(next); return ans; } }; 删除链表的倒数第N个结点 难度：Medium\n19. 删除链表的倒数第N个结点\n给定链表，删除倒数第n个节点并返回头结点。\nSolution 1:先跑一遍得到链表长度再删除，挣扎了半天加上if(step == 0)的情况就AC了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public: ListNode* removeNthFromEnd(ListNode* head, int n) { int N = 0; ListNode* start = head; while(head) { head = head-\u0026gt;next; ++N; } int step = N - n; if(step == 0) { return start-\u0026gt;next; } ListNode* remove = start; for(int i = 0; i \u0026lt; step - 1; ++i) remove = remove-\u0026gt;next; remove-\u0026gt;next = remove-\u0026gt;next-\u0026gt;next; return start; } }; Solution 2：居然没有想到，栈可太适合这题了，出栈的第n个节点就是需要删除的节点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public: ListNode* removeNthFromEnd(ListNode* head, int n) { ListNode* dummy = new ListNode(0, head); stack\u0026lt;ListNode*\u0026gt; stk; ListNode* cur = dummy; while(cur) { stk.push(cur); cur = cur-\u0026gt;next; } for(int i = 0; i \u0026lt; n; ++i) { stk.pop(); } ListNode* prev = stk.top(); // 待删除节点的前一个节点 prev-\u0026gt;next = prev-\u0026gt;next-\u0026gt;next; return dummy-\u0026gt;next; } }; Solution 3：双指针，可以将时间复杂度优化到常数级。first指针比second指针快n个节点，那么当first遍历到链表尾时，second就恰好处于倒数第n个节点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Solution { public: ListNode* removeNthFromEnd(ListNode* head, int n) { ListNode* dummy = new ListNode(0, head); ListNode* first = head; ListNode* second = dummy; for(int i = 0; i \u0026lt; n; ++i) first = first-\u0026gt;next; while(first) { first = first-\u0026gt;next; second = second-\u0026gt;next; } second-\u0026gt;next = second-\u0026gt;next-\u0026gt;next; return dummy-\u0026gt;next; } }; 两两交换链表中的节点 难度：Medium\n24. 两两交换链表中的节点\n给定链表，两两交换其中相邻的节点。\n有点绕，画个图会清晰很多。同时记得用prev维护前驱节点的连接。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public: ListNode* swapPairs(ListNode* head) { ListNode* dummy = new ListNode(0, head); ListNode* A; ListNode* B; ListNode* C; ListNode* prev = dummy; while(head \u0026amp;\u0026amp; head-\u0026gt;next) { A = head, B = head-\u0026gt;next, C = head-\u0026gt;next-\u0026gt;next, head = C; prev-\u0026gt;next = B; B-\u0026gt;next = A; A-\u0026gt;next = C; prev = A; } return dummy-\u0026gt;next; } }; K个一组翻转链表 难度：Hard\n25. K个一组翻转链表\n每k个节点一组进行翻转（将上一题的k=2扩展）。\n比上一题更复杂一些的模拟，还得稍微看一看代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Solution { pair\u0026lt;ListNode*, ListNode*\u0026gt; Reverse(ListNode* head, ListNode* tail) { ListNode* prev = tail-\u0026gt;next; ListNode* p = head; while(prev != tail) { ListNode* next = p-\u0026gt;next; p-\u0026gt;next = prev; prev = p; p = next; } return {tail, head}; } public: ListNode* reverseKGroup(ListNode* head, int k) { ListNode* dummy = new ListNode(0, head); ListNode *pre = dummy; while(head) { ListNode* tail = pre; /* 判断剩余部分长度是否\u0026gt;=k */ for(int i = 0; i \u0026lt; k; ++i) { tail = tail-\u0026gt;next; if(!tail) return dummy-\u0026gt;next; } /* 寻找要翻转的链表尾 */ ListNode* next = tail-\u0026gt;next; pair\u0026lt;ListNode*, ListNode*\u0026gt; result = Reverse(head, tail); head = result.first; tail = result.second; pre-\u0026gt;next = head; tail-\u0026gt;next = next; pre = tail; // 当前链表尾 head = tail-\u0026gt;next; // 更新head } return dummy-\u0026gt;next; } }; 随机链表的复制 难度：Medium\n138. 随机链表的复制\n实现链表（比正常链表多一个随机节点）的深拷贝。\n毫无思路，正解是哈希表。由于next和random实际都是指向自己的（某个节点），因此深拷贝只需要申请一次链表空间。1）先建立拷贝前节点-\u0026gt;拷贝后节点的映射，申请空间；2）参考拷贝前链表，还原拷贝后链表的next和random。值得学习。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 class Solution { public: Node* copyRandomList(Node* head) { if(head == nullptr) return nullptr; Node* cur = head; unordered_map\u0026lt;Node*, Node*\u0026gt; map; /* 建立拷贝前节点-\u0026gt;拷贝后节点的映射，申请空间 */ while(cur != nullptr) { map[cur] = new Node(cur-\u0026gt;val); cur = cur-\u0026gt;next; } cur = head; /* 参考拷贝前链表，还原next和random */ while(cur != nullptr) { map[cur]-\u0026gt;next = map[cur-\u0026gt;next]; map[cur]-\u0026gt;random = map[cur-\u0026gt;random]; cur = cur-\u0026gt;next; } return map[head]; } }; 排序链表 难度：Medium\n148. 排序链表\n给定链表，升序排序后并返回排序后的链表。\n数组暂存一下然后快排，跳过。但是要$\\Theta(1)$的空间复杂度还是有点难度的。\n合并K个升序链表 难度：Hard\n23. 合并K个升序链表\n给定一个链表数组，每个链表都按升序排序，将所有链表合并到一个升序链表。\n别看到Hard就怕了，大问题拆小问题，用一个变量ans来维护已经合并的链表，第i次循环把第i个链表和ans合并并保存。那就只需要写一个合并两个链表的函数，时间复杂度$\\Theta(k^2n)$，空间复杂度$\\Theta(1)$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Solution { ListNode* merge(ListNode* a, ListNode* b) { if(!a || !b) return a ? a : b; ListNode head, *tail = \u0026amp;head; ListNode *aPtr = a, *bPtr = b; while(aPtr \u0026amp;\u0026amp; bPtr) { if(aPtr-\u0026gt;val \u0026lt; bPtr-\u0026gt;val) { tail-\u0026gt;next = aPtr; aPtr = aPtr-\u0026gt;next; } else { tail-\u0026gt;next = bPtr; bPtr = bPtr-\u0026gt;next; } tail = tail-\u0026gt;next; } /* 最后接一次即可，后面的链表继续 */ tail-\u0026gt;next = (aPtr ? aPtr : bPtr); return head.next; } public: ListNode* mergeKLists(vector\u0026lt;ListNode*\u0026gt;\u0026amp; lists) { ListNode *ans = nullptr; for(size_t i = 0; i \u0026lt; lists.size(); ++i) { ans = merge(ans, lists[i]); } return ans; } }; LRU缓存 难度：Medium\n146. LRU缓存\n设计并实现一个满足LRU（最近最少使用）缓存\nLRUCache(int capacity)以正整数作为容量capacity初始化LRU缓存\nint get(int key)如果关键字key存在于缓存中，则返回关键字的值，否则返回-1\nvoid put(int key, int value)如果关键字key已存在则变更其数据值value；如果不存在则向缓存中插入该组key-value。如果插入操作导致关键字数量超过capacity，则应该逐出最久未使用的关键字。\n函数get和pu必须以\\Theta(1)的平均时间复杂度运行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # template class LRUCache { public: LRUCache(int capacity) { } int get(int key) { } void put(int key, int value) { } }; /** * Your LRUCache object will be instantiated and called as such: * LRUCache* obj = new LRUCache(capacity); * int param_1 = obj-\u0026gt;get(key); * obj-\u0026gt;put(key,value); */ 如果只要简单的实现的话确实不难，但是要\\Theta(1)的平均时间复杂度。使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 class LRUCache { struct CacheLine{ int key, value; CacheLine *prev, *next; CacheLine(): key(0), value(0), prev(nullptr), next(nullptr) {} CacheLine(int _key, int _value): key(_key), value(_value), prev(nullptr), next(nullptr) {} }; unordered_map\u0026lt;int, CacheLine*\u0026gt; cache_hash; CacheLine *head, *tail; int size; int capacity; public: LRUCache(int capacity) { /* 使用伪头节点和尾节点 */ head = new CacheLine(); tail = new CacheLine(); head-\u0026gt;next = tail; tail-\u0026gt;prev = head; size = 0; this-\u0026gt;capacity = capacity; } int get(int key) { if(!cache_hash.count(key)) { return -1; } CacheLine* line = cache_hash[key]; moveToHead(line); return line-\u0026gt;value; } void put(int key, int value) { if(!cache_hash.count(key)) { // 不在缓存中 CacheLine* line = new CacheLine(key, value); cache_hash[key] = line; addToHead(line); ++size; if(size \u0026gt; capacity) { CacheLine* removed = removeTail(); cache_hash.erase(removed-\u0026gt;key); delete removed; --size; } } else { // 在缓存中 CacheLine* line = cache_hash[key]; line-\u0026gt;value = value; moveToHead(line); } } void addToHead(CacheLine* line) { /* 将line添加到链表头 */ line-\u0026gt;prev = head; line-\u0026gt;next = head-\u0026gt;next; head-\u0026gt;next-\u0026gt;prev = line; head-\u0026gt;next = line; } void removeLine(CacheLine *line) { /* 将line移除(暂时) */ line-\u0026gt;prev-\u0026gt;next = line-\u0026gt;next; line-\u0026gt;next-\u0026gt;prev = line-\u0026gt;prev; } void moveToHead(CacheLine *line) { /* 将line移动到链表头 */ removeLine(line); addToHead(line); } CacheLine* removeTail() { /* 将链表尾元素删除 */ CacheLine* line = tail-\u0026gt;prev; removeLine(line); return line; } }; 二叉树 二叉树的中序遍历 难度：Easy\n94. 二叉树的中序遍历\n递归 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Solution { public: void inorder(TreeNode* root, vector\u0026lt;int\u0026gt;\u0026amp; ans) { if(!root) { return; } inorder(root-\u0026gt;left, ans); ans.emplace_back(root-\u0026gt;val); inorder(root-\u0026gt;right, ans); } vector\u0026lt;int\u0026gt; inorderTraversal(TreeNode* root) { vector\u0026lt;int\u0026gt; ans; inorder(root, ans); return ans; } }; 迭代 和递归是等价的，用stack模拟函数栈\n二叉树的最大深度 难度：Easy\n104. 二叉树的最大深度\n给定二叉树返回最大深度。\n递归，非常简单，秒了。\n1 2 3 4 5 6 7 class Solution { public: int maxDepth(TreeNode* root) { if(root == nullptr) return 0; else return max(maxDepth(root-\u0026gt;left), maxDepth(root-\u0026gt;right)) + 1; } }; 翻转二叉树（递归） 难度：Easy\n226. 翻转二叉树\n给一棵二叉树，翻转每一个左右节点，很简单的递归，秒了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public: void reverse(TreeNode* root) { if(root == nullptr) return; // 递归放上面or下面都无所谓，不影响结果 reverse(root-\u0026gt;left); reverse(root-\u0026gt;right); TreeNode* newLeft = root-\u0026gt;right; TreeNode* newRight = root-\u0026gt;left; root-\u0026gt;left = newLeft; root-\u0026gt;right = newRight; return; } TreeNode* invertTree(TreeNode* root) { reverse(root); return root; } }; 对称二叉树 难度：Easy\n101. 对称二叉树\n给定二叉树的根节点，检查是否轴对称。\n递归 想了一会没想出来，这个递归的构造有点巧妙。开局直接让左右子树进去递归没问题，如何在后续检查对称呢？答案是之后分别把(leftTree-\u0026gt;right, rightTree-\u0026gt;left)和(leftTree-\u0026gt;right, rightTree-\u0026gt;left)丢进去递归。\n1 2 3 4 5 6 7 8 9 10 11 12 class Solution { bool check(TreeNode* p, TreeNode* q) { if(p == nullptr || q == nullptr) { return p == q; } return p-\u0026gt;val == q-\u0026gt;val \u0026amp;\u0026amp; check(p-\u0026gt;left, q-\u0026gt;right) \u0026amp;\u0026amp; check(p-\u0026gt;right, q-\u0026gt;left); } public: bool isSymmetric(TreeNode* root) { return check(root-\u0026gt;left, root-\u0026gt;right); } }; 迭代 递归改迭代常引用队列，初始化时将根节点入队两次，每次提取两个节点比较是否相等，同时将左右子节点按相反顺序入队。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { bool check(TreeNode *l, TreeNode *r) { queue\u0026lt;TreeNode*\u0026gt;q; q.push(l); q.push(r); while(!q.empty()) { l = q.front(); q.pop(); r = q.front(); q.pop(); if(!l \u0026amp;\u0026amp; !r) continue; if((!l || !r) || (l-\u0026gt;val != r-\u0026gt;val)) return false; q.push(l-\u0026gt;left); q.push(r-\u0026gt;right); q.push(l-\u0026gt;right); q.push(r-\u0026gt;left); } return true; } public: bool isSymmetric(TreeNode* root) { return check(root, root); } }; 二叉树的直径 难度：Easy\n543. 二叉树的直径\n给定二叉树，求任意两节点之间最长路径的长度。递归注意返回max(L, R) + 1。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { int ans; int check(TreeNode* root) { if(!root) return 0; int L = check(root-\u0026gt;left); int R = check(root-\u0026gt;right); ans = max(ans, L + R + 1); return max(L, R) + 1; } public: int diameterOfBinaryTree(TreeNode* root) { ans = 0; check(root); return ans - 1; } }; 二叉树的层序遍历 难度：Medium\n102. 二叉树的层序遍历\n遇到层次遍历和最短路径应该想到BFS。将根节点入队后，每次将队列当前所有元素清空，并将所有左节点和右节点入队。时间复杂度和空间复杂度都为$\\Theta(n)$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Solution { public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; levelOrder(TreeNode* root) { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; result; if(!root) return result; queue\u0026lt;TreeNode*\u0026gt; q; q.push(root); while(!q.empty()) { int n = q.size(); result.push_back(vector\u0026lt;int\u0026gt;()); for(int i = 0; i \u0026lt; n; ++i) { TreeNode* node = q.front(); q.pop(); result.back().push_back(node-\u0026gt;val); if(node-\u0026gt;left) q.push(node-\u0026gt;left); if(node-\u0026gt;right) q.push(node-\u0026gt;right); } } return result; } }; 将有序数组转换为二叉搜索树 难度：Easy\n108. 将有序数组转换为二叉搜索树\n给定升序数组，转换为一棵平衡（左右子树高度相差\u0026lt;=1）二叉搜索树。\n递归解法如下。每次将中间节点作为根节点，然后将左升序区间和右升序区间再丢进去递归（直接写在申请TreeNode中，有点妙）。\n1 2 3 4 5 6 7 8 9 10 11 class Solution { TreeNode* dfs(vector\u0026lt;int\u0026gt;\u0026amp; nums, int left, int right) { if(left == right) return nullptr; int m = left + (right - left) / 2; return new TreeNode(nums[m], dfs(nums, left, m), dfs(nums, m + 1, right)); } public: TreeNode* sortedArrayToBST(vector\u0026lt;int\u0026gt;\u0026amp; nums) { return dfs(nums, 0, nums.size()); } }; 验证二叉搜索树 难度：Medium\n98. 验证二叉搜索树\n给定二叉树，判断是否是合法的二叉搜索树（左子树只包含小于当前节点的数，右子树只包含大于当前节点的数）。\n递归，初始范围区间是$(-inf, +inf)$，当前值在区间内时，将左子树和左区间$(-inf, root-\u0026gt;val)$，右子树和右区间$(root-\u0026gt;val, +inf)$丢进去递归。\n（题目样例用INT_MAX恶心人\u0026hellip;）\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { bool check(TreeNode* root, long long lower, long long upper) { if(root == nullptr) return true; if(root-\u0026gt;val \u0026lt;= lower || root-\u0026gt;val \u0026gt;= upper) { return false; } return check(root-\u0026gt;left, lower, root-\u0026gt;val) \u0026amp;\u0026amp; check(root-\u0026gt;right, root-\u0026gt;val, upper); } public: bool isValidBST(TreeNode* root) { return check(root, LONG_MIN, LONG_MAX); } }; 二叉搜索树中第K小的元素 难度：Medium\n230. 二叉搜索树中第K小的元素\n给定二叉搜索树，返回第k小的元素。\n在二叉搜索树中，任意子节点都满足$left \u0026lt; root \u0026lt; right$，因此有一个重要性质：BST的中序遍历为递增序列。问题转化为求中序遍历的第k个节点。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { int ans, k; void dfs(TreeNode* root) { if(root == nullptr) return; /* 中序遍历 */ dfs(root-\u0026gt;left); if(--k == 0) ans = root-\u0026gt;val; dfs(root-\u0026gt;right); } public: int kthSmallest(TreeNode* root, int k) { this-\u0026gt;k = k; dfs(root); return ans; } }; 二叉树的右视图 难度：Medium\n199. 二叉树的右视图\n给定二叉树，返回从右侧看到的节点值。\nBFS 解法和二叉树的层序遍历差不多，由于不知道二叉树的形状，因此是需要遍历每个节点的。每次迭代将所有节点出队（队首就是需要的右视图），并将所有出队节点的子节点放入队列（优先放入右节点，保证下次迭代出队时，队首节点是需要的右视图）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public: vector\u0026lt;int\u0026gt; rightSideView(TreeNode* root) { vector\u0026lt;int\u0026gt; ans; queue\u0026lt;TreeNode*\u0026gt; q; if(root) q.push(root); while(!q.empty()) { int n = q.size(); for(int i = 0; i \u0026lt; n; ++i) { TreeNode* node = q.front(); if(i == 0) ans.emplace_back(node-\u0026gt;val); if(node-\u0026gt;right) q.push(node-\u0026gt;right); if(node-\u0026gt;left) q.push(node-\u0026gt;left); q.pop(); } } return ans; } }; DFS 补充一个间接的DFS解法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { vector\u0026lt;int\u0026gt; ans; void dfs(TreeNode* node, int u) { if(u == ans.size()) ans.emplace_back(node-\u0026gt;val); if(node-\u0026gt;right) dfs(node-\u0026gt;right, u + 1); if(node-\u0026gt;left) dfs(node-\u0026gt;left, u + 1); } public: vector\u0026lt;int\u0026gt; rightSideView(TreeNode* root) { if(root) dfs(root, 0); return ans; } }; 二叉树展开为链表 难度：Medium\n114. 二叉树展开为链表\n给定二叉树，按前序遍历展平为链表。\n前序遍历塞进队列，然后遍历一遍的队列。时间复杂度和空间复杂度都是$\\Thata(n)$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Solution { queue\u0026lt;TreeNode*\u0026gt;q; void preorder(TreeNode* node) { if(node == nullptr) { return; } q.push(node); preorder(node-\u0026gt;left); preorder(node-\u0026gt;right); } public: void flatten(TreeNode* root) { if(root) preorder(root); else return; TreeNode* last = q.front(); q.pop(); while(!q.empty()) { last-\u0026gt;left = nullptr; TreeNode* current = q.front(); q.pop(); last-\u0026gt;right = current; last = current; } } }; 还有一种空间复杂度降为$\\Theta(1)$的做法，对于当前节点，如果左节点非空，则在左子树找到最右的节点作为前驱节点，将右节点（右子树）赋这个节点的右节点（反正这个空间暂时用不到），并将当前节点的左节点赋给右节点，左节点置空。这个做法得思考一下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public: void flatten(TreeNode* root) { TreeNode *cur = root; while(cur != nullptr) { if(cur-\u0026gt;left != nullptr) { TreeNode* next = cur-\u0026gt;left; TreeNode* pre = next; while(pre-\u0026gt;right != nullptr) { pre = pre-\u0026gt;right; } pre-\u0026gt;right = cur-\u0026gt;right; cur-\u0026gt;left = nullptr; cur-\u0026gt;right = next; } cur = cur-\u0026gt;right; } } }; 从前序与中序遍历序列构造二叉树 难度：Medium\n105. 从前序与中序遍历序列构造二叉树\n给定二叉树的前序遍历和中序遍历，构造二叉树并返回根节点。\n不会写，只能看题解了，给出一种递归解法：\n前序遍历（$root, [left],[right]$）和中序遍历（$[left],root,[right]$）的长度是一致的。首先遍历一遍中序遍历，将$(inorder[i], i)$塞进哈希表，来快速找某个元素在中序遍历的位置（$\\Theta(1)$），用来找根节点。\n在递归中，前序遍历的第一个节点就是当前根节点，然后在中序遍历中寻找，就能分割左右子树了。当要递归构造左子树时，中序遍历的范围是清晰的（当前根节点左边的区间，即$[in_l, in_root - 1]$）；前序遍历的范围则是第一个节点（当前根节点）后的左子树个数的区间。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Solution { unordered_map\u0026lt;int, int\u0026gt; index; public: TreeNode* build(const vector\u0026lt;int\u0026gt;\u0026amp; preorder, const vector\u0026lt;int\u0026gt;\u0026amp; inorder, int pre_l, int pre_r, int in_l, int in_r) { if(pre_l \u0026gt; pre_r) { return nullptr; } /* 前序遍历的第一个节点是根节点 */ int pre_root = pre_l; /* 在中序遍历中定位根节点 */ int in_root = index[preorder[pre_root]]; TreeNode* root = new TreeNode(preorder[pre_root]); /* (通过中序遍历)左子树的子节点数目 */ int size_l = in_root - in_l; /* 递归构造左子树 */ root-\u0026gt;left = build(preorder, inorder, pre_l + 1, pre_l + size_l, in_l, in_root - 1); /* 递归构造右子树 */ root-\u0026gt;right = build(preorder, inorder, pre_l + size_l + 1, pre_r, in_root + 1, in_r); return root; } TreeNode* buildTree(vector\u0026lt;int\u0026gt;\u0026amp; preorder, vector\u0026lt;int\u0026gt;\u0026amp; inorder) { int n = preorder.size(); for(int i = 0; i \u0026lt; n; ++i) { index[inorder[i]] = i; } return build(preorder, inorder, 0, n - 1, 0, n - 1); } }; 路径总和III 难度：Medium\n437. 路径总和III\n给定二叉树和整数$targetSum$，求该二叉树里节点值之和等于$targetSum$的路径的数目。\nDFS 每次递归时都传$targetSum - node-\u0026gt;val$，可以少传一个$sum$变量。要记得开long long，不然过不去阴间测例。这种解法遍历了每个节点出发到叶子节点，复杂度是$\\Theta(n^2)$，有大量的重复遍历。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { int dfs(TreeNode* node, long long targetSum) { if(!node) return 0; int result = 0; if(node-\u0026gt;val == targetSum) ++result; result += dfs(node-\u0026gt;left, targetSum - node-\u0026gt;val); result += dfs(node-\u0026gt;right, targetSum - node-\u0026gt;val); return result; } public: int pathSum(TreeNode* root, int targetSum) { if(!root) return 0; int ans = dfs(root, targetSum); ans += pathSum(root-\u0026gt;left, targetSum); ans += pathSum(root-\u0026gt;right, targetSum); return ans; } }; 前缀和优化 求路径上的和是否等于目标值，很容易想到前缀和。问题转化为将二叉树构造为前缀和。\n从根节点$root$开始遍历到$node$，此时的路径是$root-\u0026gt;p_1-\u0026gt;p_2-\u0026gt;\u0026hellip;-\u0026gt;node$，而$sum$就是前缀和，在遍历的过程中，将前缀和记录到前缀和数组中($++prefix[sum]$)。**注意退出节点时还需要状态回溯($\u0026ndash;prefix[sum]$)。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Solution { unordered_map\u0026lt;long long, int\u0026gt; prefix; int dfs(TreeNode *node, long long sum, int targetSum) { if(!node) return 0; int result = 0; sum += node-\u0026gt;val; if(prefix.count(sum - targetSum)) result = prefix[sum - targetSum]; ++prefix[sum]; result += dfs(node-\u0026gt;left, sum, targetSum); result += dfs(node-\u0026gt;right, sum, targetSum); --prefix[sum]; return result; } public: int pathSum(TreeNode* root, int targetSum) { prefix[0] = 1; return dfs(root, 0, targetSum); } }; 二叉树的最近公共祖先（LCA） 难度：Medium\n236. 二叉树的最近公共祖先\n给定一个二叉树，找到该树中两个指定节点p和q的最近公共祖先。\n参考题解，p和q的情况分为两种：1）p和q在相同子树中；2）p和q在不同子树中。从根节点向左右节点递归（递归出口：空/p/q，返回当前节点）。\n递归遍历左右子树，如果都不为空，那么p和q分别在左右子树中，因此当前节点为LCA\n如果左右子树其中一个不为空，则返回非空节点\n代码很简洁，得再思考思考。\n1 2 3 4 5 6 7 8 9 10 11 class Solution { public: TreeNode* lowestCommonAncestor(TreeNode* root, TreeNode* p, TreeNode* q) { if(!root || root == p || root == q) return root; TreeNode *left = lowestCommonAncestor(root-\u0026gt;left, p, q); TreeNode *right = lowestCommonAncestor(root-\u0026gt;right, p, q); // 在不同子树时返回root if(left \u0026amp;\u0026amp; right) return root; return left ? left : right; } }; 二叉树中的最大路径和 难度：Hard\n124. 二叉树中的最大路径和\n给定二叉树，返回最大路径和。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Solution { int ans = INT_MIN; int solve(TreeNode* node) { if(!node) return 0; /* 递归计算左右子节点的最大贡献值 */ /* 贡献值\u0026gt;0时才选取 */ int l = max(solve(node-\u0026gt;left), 0); int r = max(solve(node-\u0026gt;right), 0); /* 节点的最大路径和取决于该节点值与左右子节点的最大贡献值 */ int sum = node-\u0026gt;val + l + r; ans = max(ans, sum); /* 返回节点的最大贡献值 */ return node-\u0026gt;val + max(l, r); } public: int maxPathSum(TreeNode* root) { solve(root); return ans; } }; 图论 岛屿数量 难度：Medium\n200. 岛屿数量\n给定'0\u0026rsquo;和'1\u0026rsquo;组成的二维网格，求成片的'1\u0026rsquo;的数量。\ndfs过题代码如下，bfs、并查集也可以写。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Solution { public: bool visited[305][305]; bool dfs(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; grid, int x, int y) { if(grid[x][y] == \u0026#39;0\u0026#39; || visited[x][y]) return false; visited[x][y] = true; if(x - 1 \u0026gt;= 0) dfs(grid, x - 1, y); if(x + 1 \u0026lt; grid.size()) dfs(grid, x + 1, y); if(y - 1 \u0026gt;= 0) dfs(grid, x, y - 1); if(y + 1 \u0026lt; grid[0].size()) dfs(grid, x, y + 1); return true; } int numIslands(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; grid) { for(int i = 0; i \u0026lt; grid.size(); ++i) { for(int j = 0; j \u0026lt; grid[0].size(); ++j) { visited[i][j] = false; } } int ans = 0; for(int i = 0; i \u0026lt; grid.size(); ++i) { for(int j = 0; j \u0026lt; grid[0].size(); ++j) { if(dfs(grid, i, j)) ++ans; } } return ans; } }; 腐烂的橘子 难度：Medium\n994. 腐烂的橘子\n给定含橘子、烂橘子和空的网格，求多少时间单位后全部腐烂。\n因为烂橘子的腐烂是向外扩散的，因此必须用BFS（实际上是多源BFS）。由于题目的特殊性，可以不使用队列来BFS（因为下一次扩散的一定是上一次被扩散的，可以直接用新的队列存储，直接用move替换）。\n题解代码写得太精致了，得学习一下。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Solution { int DIRECTIONS[4][2] = {{-1, 0}, {1, 0}, {0, -1}, {0, 1}}; public: int orangesRotting(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid) { int n = grid.size(), m = grid[0].size(); int fresh = 0; vector\u0026lt;pair\u0026lt;int, int\u0026gt;\u0026gt; q; for(int i = 0; i \u0026lt; n; ++i) { for(int j = 0; j \u0026lt; m; ++j) { if(grid[i][j] == 1) { ++fresh; } else if(grid[i][j] == 2) { q.emplace_back(i, j); } } } int ans = 0; /* BFS */ while(fresh \u0026amp;\u0026amp; !q.empty()) { ++ans; // 经过一分钟 vector\u0026lt;pair\u0026lt;int, int\u0026gt;\u0026gt; next; for(auto\u0026amp; [x, y] : q) { for(auto d : DIRECTIONS) { int i = x + d[0], j = y + d[1]; if(0 \u0026lt;= i \u0026amp;\u0026amp; i \u0026lt; n \u0026amp;\u0026amp; 0 \u0026lt;= j \u0026amp;\u0026amp; j \u0026lt; m \u0026amp;\u0026amp; grid[i][j] == 1) { --fresh; grid[i][j] = 2; next.emplace_back(i, j); } } } q = move(next); // 优化: 下一次出发点一定是刚腐烂的 } return fresh ? -1 : ans; } }; 课程表 难度：Medium\n207. 课程表\n给出必修课程$0~n-1$和先修课程$pre[i] = [a_i, b_i]$（$a_i$的先决条件是$b_i$），判断是否可能完成所有课程的学习。\n少见的拓扑排序（topo sort），很重要的题。给定$n$，一个先决条件表，判断是否能完成所有课程。\n先用先决条件表构造入边表和有向图。使用BFS，将没有先决条件的课程加入队列。对于队列中的每个课程，可以在有向图中遍历，将先决条件消除，并将没有先决条件的课程继续加入队列直到空。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 class Solution { public: bool canFinish(int numCourses, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; prerequisites) { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; g(numCourses); vector\u0026lt;int\u0026gt; indegree(numCourses); /* 遍历先决条件表，构造“入边”表 */ for(int i = 0; i \u0026lt; prerequisites.size(); ++i) { int x = prerequisites[i][0], y = prerequisites[i][1]; ++indegree[x]; // x有?个先决条件 g[y].push_back(x); } queue\u0026lt;int\u0026gt; q; for(int i = 0; i \u0026lt; numCourses; ++i) { if(indegree[i] == 0) q.push(i); // 没有先决条件的课程(没有先修课程的课程) } /* BFS */ while(!q.empty()) { int u = q.front(); q.pop(); for(int v : g[u]) { --indegree[v]; if(indegree[v] == 0) { q.push(v); } } } for(int x : indegree) { if(x != 0) return false; } return true; } }; 实现Trie（前缀树）（多叉树） 难度：Medium\n208. 实现Trie（前缀树）\n思路就是多叉树，每个节点映射到26个字母（26叉）。\n插入单词时，按照映射关系遍历，若为空则申请空间并将结尾标记为isEnd=true。\nsearch和startwith的区别仅仅在于返回isEnd还是true。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Trie { private: bool isEnd; Trie* next[26]; // 每个节点至多映射26个节点 public: Trie() { isEnd = false; memset(next, 0, sizeof(next)); } void insert(string word) { Trie* node = this; for(char c : word) { if(node-\u0026gt;next[c - \u0026#39;a\u0026#39;] == NULL) { node-\u0026gt;next[c - \u0026#39;a\u0026#39;] = new Trie(); } node = node-\u0026gt;next[c - \u0026#39;a\u0026#39;]; } node-\u0026gt;isEnd = true; } bool search(string word) { Trie* node = this; for(char c : word) { node = node-\u0026gt;next[c - \u0026#39;a\u0026#39;]; if(node == NULL) { return false; } } return node-\u0026gt;isEnd; } bool startsWith(string prefix) { Trie* node = this; for(char c : prefix) { node = node-\u0026gt;next[c - \u0026#39;a\u0026#39;]; if(node == NULL) { return false; } } return true; } }; 回溯 全排列 难度：Medium\n46. 全排列\n给定不含重复数字的数组，按任意顺序返回其所有可能的全排列。\n邪门歪道之stl可秒，和31. 下一个排列类似。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Solution { public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; permute(vector\u0026lt;int\u0026gt;\u0026amp; nums) { sort(nums.begin(), nums.end()); vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; ans; do{ vector\u0026lt;int\u0026gt; tmp; for(int x : nums) { tmp.push_back(x); } ans.push_back(tmp); }while(next_permutation(nums.begin(), nums.end())); return ans; } }; 但还是认真写回溯解法吧。可以将问题视为在长度为$n$的数组中填充数字，每个数字仅能使用一次。题解是优化后的解法，去掉了$vis$标记数组。当已经填充到第$n$个位置，那么$[0, n - 1]$是已填的集合，$[n, len - 1]$是待填的集合，将$nums[n]$和$nums[x],x \\in [n, len - 1]$区间的数字交换可以达到同样的效果。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { void solve(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; ans, vector\u0026lt;int\u0026gt;\u0026amp; output, int n, int len) { if(n == len) { ans.emplace_back(output); return; } for(int i = n; i \u0026lt; len; ++i) { swap(output[i], output[n]); solve(ans, output, n + 1, len); swap(output[i], output[n]); } } public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; permute(vector\u0026lt;int\u0026gt;\u0026amp; nums) { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; ans; solve(ans, nums, 0, nums.size()); return ans; } }; 子集 难度：Medium\n78. 子集\n给定不含重复数字的数组，返回不重复的所有子集。\n看到这题就想到大一时学习紫书的时候，被子集生成的增量构造法、二进制法、位向量法折磨地死去活来。Anyway，现在我都忘了。\n二进制法 二进制法是最快，最节省空间，最简单易懂的方法。可以将形如$XXX, X \\in {0, 1}$的二进制串的每一位看作是否选取这个元素作为子集的一部分。从全0遍历到全1即可覆盖从空集到最长子集的所有情况。共1 \u0026lt;\u0026lt; n种情况，实际从0遍历到1 \u0026lt;\u0026lt; n - 1。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; ans; vector\u0026lt;int\u0026gt; tmp; public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; subsets(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int n = nums.size(); for(int mask = 0; mask \u0026lt; (1 \u0026lt;\u0026lt; n); ++mask) { tmp.clear(); for(int i = 0; i \u0026lt; n; ++i) { if(mask \u0026amp; (1 \u0026lt;\u0026lt; i)) tmp.push_back(nums[i]); } ans.push_back(tmp); } return ans; } }; 回溯法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; ans; vector\u0026lt;int\u0026gt; tmp; void dfs(int cur, vector\u0026lt;int\u0026gt;\u0026amp; nums) { if(cur == nums.size()) { ans.push_back(tmp); return; } /* 将选择这个元素的情况进入递归栈 */ tmp.push_back(nums[cur]); dfs(cur + 1, nums); /* 将不选择这个元素的情况进入递归栈 */ tmp.pop_back(); // 回溯：将刚插入的元素删除 dfs(cur + 1, nums); } public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; subsets(vector\u0026lt;int\u0026gt;\u0026amp; nums) { dfs(0, nums); return ans; } }; 电话号码的字母组合 难度：Medium\n17. 电话号码的字母组合\n给定一个仅包含数字2-9的字符串，返回所有能表示的字母组合。\n参考上一题的回溯法解法，很容易写出AC代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Solution { vector\u0026lt;string\u0026gt; ans; vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt; dict; string tmp; void init() { dict.resize(10); dict[2].push_back(\u0026#39;a\u0026#39;); dict[2].push_back(\u0026#39;b\u0026#39;); dict[2].push_back(\u0026#39;c\u0026#39;); dict[3].push_back(\u0026#39;d\u0026#39;); dict[3].push_back(\u0026#39;e\u0026#39;); dict[3].push_back(\u0026#39;f\u0026#39;); dict[4].push_back(\u0026#39;g\u0026#39;); dict[4].push_back(\u0026#39;h\u0026#39;); dict[4].push_back(\u0026#39;i\u0026#39;); dict[5].push_back(\u0026#39;j\u0026#39;); dict[5].push_back(\u0026#39;k\u0026#39;); dict[5].push_back(\u0026#39;l\u0026#39;); dict[6].push_back(\u0026#39;m\u0026#39;); dict[6].push_back(\u0026#39;n\u0026#39;); dict[6].push_back(\u0026#39;o\u0026#39;); dict[7].push_back(\u0026#39;p\u0026#39;); dict[7].push_back(\u0026#39;q\u0026#39;); dict[7].push_back(\u0026#39;r\u0026#39;); dict[7].push_back(\u0026#39;s\u0026#39;); dict[8].push_back(\u0026#39;t\u0026#39;); dict[8].push_back(\u0026#39;u\u0026#39;); dict[8].push_back(\u0026#39;v\u0026#39;); dict[9].push_back(\u0026#39;w\u0026#39;); dict[9].push_back(\u0026#39;x\u0026#39;); dict[9].push_back(\u0026#39;y\u0026#39;); dict[9].push_back(\u0026#39;z\u0026#39;); } void dfs(int cur, string \u0026amp; digits) { if(cur == digits.size()) { ans.push_back(tmp); return; } int num = digits[cur] - \u0026#39;0\u0026#39;; int n = dict[num].size(); for(int i = 0; i \u0026lt; n; ++i) { tmp.push_back(dict[num][i]); dfs(cur + 1, digits); tmp.pop_back(); } } public: vector\u0026lt;string\u0026gt; letterCombinations(string digits) { init(); if(digits.size())dfs(0, digits); return ans; } }; 不过哈希表有更优雅的写法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 unordered_map\u0026lt;char, string\u0026gt; map{ {\u0026#39;2\u0026#39;, \u0026#34;abc\u0026#34;}, {\u0026#39;3\u0026#39;, \u0026#34;def\u0026#34;}, {\u0026#39;4\u0026#39;, \u0026#34;ghi\u0026#34;}, {\u0026#39;5\u0026#39;, \u0026#34;jkl\u0026#34;}, {\u0026#39;6\u0026#39;, \u0026#34;mno\u0026#34;}, {\u0026#39;7\u0026#39;, \u0026#34;pqrs\u0026#34;}, {\u0026#39;8\u0026#39;, \u0026#34;tuv\u0026#34;}, {\u0026#39;9\u0026#39;, \u0026#34;wxyz\u0026#34;} }; const string\u0026amp; letters = map.at(digit[i]); for(const char\u0026amp; letter : letters) {...} 组合总数 难度：Medium\n39. 组合总数\n给定一个无重复元素($\u0026gt; 0$)的整数数组和一个目标整数，找出数组种可以使数组和为目标的所有不同组合。\n对于这类寻找所有可行解的题，可以尝试用搜索回溯来解决。实际上还是遍历每一种可能的组合（搜索树），但只有和为0时才被视为有效解。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; ans; vector\u0026lt;int\u0026gt; combine; void dfs(vector\u0026lt;int\u0026gt;\u0026amp; candidates, int target, int idx) { if(idx == candidates.size()) return; if(target == 0) { ans.emplace_back(combine); return; } dfs(candidates, target, idx + 1); if(target - candidates[idx] \u0026gt;= 0) { combine.emplace_back(candidates[idx]); dfs(candidates, target - candidates[idx], idx); combine.pop_back(); } } public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; combinationSum(vector\u0026lt;int\u0026gt;\u0026amp; candidates, int target) { dfs(candidates, target, 0); return ans; } }; 括号生成 难度：Medium\n22. 括号生成\n生成$n$对括号，返回所有可能且有效的括号组合。\n思路很好想，但是第一次写的时候发现实现有点困难。因为括号的有效性限制，得控制好(的入栈次数总是比)要多。因此这里还需要两个变量记录(和)的个数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class Solution { vector\u0026lt;string\u0026gt; ans; string tmp; void dfs(int n,int l, int r) { if(tmp.size() == n * 2) { ans.emplace_back(tmp); return; } if(l \u0026lt; n) { tmp.push_back(\u0026#39;(\u0026#39;); dfs(n, l + 1, r); tmp.pop_back(); } if(r \u0026lt; l) { // 注意是r \u0026lt; l tmp.push_back(\u0026#39;)\u0026#39;); dfs(n, l, r + 1); tmp.pop_back(); } } public: vector\u0026lt;string\u0026gt; generateParenthesis(int n) { dfs(n, 0, 0); return ans; } }; 单词搜索 难度：Medium\n79. 单词搜索4\n给定二维字符网格和一个字符串单词，如果可以通过水平/垂直移动找到这个单词就返回true；否则返回false。\n很容易写出DFS代码，但怎么debug都不能AC。最后发现是同一个单元格内的字母不允许被重复使用这个条件导致的。必须要在继续DFS前排除这个单元格(board[x][y] = '?')，并在DFS结束后回溯（board[x][y] = word[idx]）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Solution { bool dfs(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; board, const string\u0026amp; word, int x, int y, int idx) { if(x \u0026lt; 0 || y \u0026lt; 0 || x \u0026gt;= board.size() || y \u0026gt;= board[0].size() || board[x][y] != word[idx]) { return false; } if(idx == word.size() - 1) return true; bool result = false; board[x][y] = \u0026#39;?\u0026#39;; result = dfs(board, word, x + 1, y, idx + 1) || dfs(board, word, x, y + 1, idx + 1) || dfs(board, word, x - 1, y, idx + 1) || dfs(board, word, x, y - 1, idx + 1); board[x][y] = word[idx]; return result; } public: bool exist(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; board, string word) { for(int i = 0; i \u0026lt; board.size(); ++i) { for(int j = 0; j \u0026lt; board[0].size(); ++j) { if(dfs(board, word, i, j, 0)) return true; } } return false; } }; 分割回文串 难度：Medium\n131. 分割回文串\n给定字符串，将其分割成一些子串，使每个子串都是回文串，返回所有可能的分割方案。初见没有思路！\n回溯+DP 要求所有分割方案，使用搜索+回溯枚举所有可能的分割方法。假设当前搜索到第$i$个字符，且$s[0..i-1]$位置的所有字符已被分割成若干个回文串，那么就需要枚举下一个回文串的右边界$j$，使$s[i..j]$是一个回文串。因此回溯方法是：如果$s[i..j]$是回文串，那么加入$ans$并以$j+1$作为新的$i$进行下一层搜索，并在未来的回溯时将$s[i..j]$从ans移除。(注意string::substr(start, len))\n除此之外，每次判断回文串需要用双指针，时间复杂度$\\Theta(n)$。多次判断显然有重复计算，因此用DP把判断任意$s[i..j]$是否为回文串降低到$\\Theta(1)$。\n设$f(i, j)$表示$s[i..j]$是否为回文串，有状态转移方程：\n$$f(i, j) = \\begin{cases} True, i\\geq j \\\\ f(i+1, j-1)\\\u0026\u0026(s[i]==s[j]),i\\lt j \\end{cases}$$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class Solution { vector\u0026lt;vector\u0026lt;string\u0026gt;\u0026gt; ans; vector\u0026lt;string\u0026gt; tmp; int n; vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; f; void dfs(const string\u0026amp; s, int i) { if(i == n) { ans.emplace_back(tmp); return; } for(int j = i; j \u0026lt; n; ++j) { if(f[i][j]) { tmp.push_back(s.substr(i, j - i + 1)); dfs(s, j + 1); tmp.pop_back(); } } } public: vector\u0026lt;vector\u0026lt;string\u0026gt;\u0026gt; partition(string s) { n = s.size(); f.resize(n); for(auto\u0026amp; row : f) row.resize(n, true); for(int i = n - 1; i \u0026gt;= 0; --i) { for(int j = i + 1; j \u0026lt; n; ++j) { f[i][j] = (s[i] == s[j]) \u0026amp;\u0026amp; f[i + 1][j - 1]; } } dfs(s, 0); return ans; } }; 回溯+记忆化搜索 众所周知，DP和记搜是可以转换的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 class Solution { vector\u0026lt;vector\u0026lt;string\u0026gt;\u0026gt; ans; vector\u0026lt;string\u0026gt; tmp; int n; vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; f; void dfs(const string\u0026amp; s, int i) { if(i == n) { ans.push_back(tmp); return; } for(int j = i; j \u0026lt; n; ++j) { if(check(s, i, j) == 1) { tmp.push_back(s.substr(i, j - i + 1)); dfs(s, j + 1); tmp.pop_back(); } } } int check(const string\u0026amp; s, int i, int j) { if(f[i][j]) { return f[i][j]; } if(i \u0026gt;= j) { return f[i][j] = 1; } return f[i][j] = (s[i] == s[j] ? check(s, i + 1, j - 1) : -1); } public: vector\u0026lt;vector\u0026lt;string\u0026gt;\u0026gt; partition(string s) { n = s.size(); f.resize(n); for(auto\u0026amp; row : f) row.resize(n); dfs(s, 0); return ans; } }; N皇后 难度：Hard\n51. N皇后\n又想起了被紫书支配的恐惧\u0026hellip;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class Solution { vector\u0026lt;vector\u0026lt;string\u0026gt;\u0026gt; ans; void dfs(vector\u0026lt;string\u0026gt;\u0026amp; cur, int level, int n) { if(level == n) { ans.push_back(cur); return; } // 遍历第level行的每一列放置皇后的情况，并回溯 for(int j = 0; j \u0026lt; n; ++j) { if(check(cur, level, j)) { cur[level][j] = \u0026#39;Q\u0026#39;; dfs(cur, level + 1, n); cur[level][j] = \u0026#39;.\u0026#39;; } } } bool check(const vector\u0026lt;string\u0026gt;\u0026amp; cur, int i, int j) { int n = cur.size(); for(int a = 0; a \u0026lt; n; ++a) { // 上 if(cur[a][j] == \u0026#39;Q\u0026#39;) return false; } for(int a = i, b = j; a \u0026gt;= 0 \u0026amp;\u0026amp; b \u0026gt;= 0; --a, --b) { // 左上 if(cur[a][b] == \u0026#39;Q\u0026#39;) return false; } for(int a = i, b = j; a \u0026gt;= 0 \u0026amp;\u0026amp; b \u0026lt; n; --a, ++b) { // 右上 if(cur[a][b] == \u0026#39;Q\u0026#39;) return false; } return true; } public: vector\u0026lt;vector\u0026lt;string\u0026gt;\u0026gt; solveNQueens(int n) { vector\u0026lt;string\u0026gt; cur(n, string(n, \u0026#39;.\u0026#39;)); dfs(cur, 0, n); return ans; } }; 二分查找 搜索插入位置 难度：Easy\n35. 搜索插入位置\n二分查找板题。\nSTL return lower_bound(nums.begin(), nums.end(), target) - nums.begin();\n二分查找\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Solution { public: int searchInsert(vector\u0026lt;int\u0026gt;\u0026amp; nums, int target) { int n = nums.size(); int l = 0, r = n - 1, ans = n; while(l \u0026lt;= r) { int mid = ((r - l) \u0026gt;\u0026gt; 1) + l; if(target \u0026lt;= nums[mid]) { ans = mid; r = mid - 1; } else { l = mid + 1; } } return ans; } }; 搜索二维矩阵 难度：Medium\n74. 搜索二维矩阵\n给定严格递增顺序填充的二维矩阵，查找$target$是否在矩阵中。很奇怪的数据，$\\Theta(n^2)$的暴力法也能AC，甚至0ms击败100.00%\u0026hellip;\n官解的写法有点高级，总之就是先在列维度二分查找，锁定行后在行维度二分查找。\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { public: bool searchMatrix(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; matrix, int target) { auto row = upper_bound(matrix.begin(), matrix.end(), target, [](const int b, const vector\u0026lt;int\u0026gt; \u0026amp;a) { return b \u0026lt; a[0]; }); if(row == matrix.begin()) { return false; } --row; return binary_search(row-\u0026gt;begin(), row-\u0026gt;end(), target); } }; 在排序数组中查找元素的第一个和最后一个位置 难度：Medium\n34. 在排序数组中查找元素的第一个和最后一个位置\n给定非递减整数数组，返回目标值在数组的开始位置和结束位置。\n用两次二分查找。\n1 2 3 4 5 6 7 8 9 10 11 12 class Solution { public: vector\u0026lt;int\u0026gt; searchRange(vector\u0026lt;int\u0026gt;\u0026amp; nums, int target) { vector\u0026lt;int\u0026gt; ans(2); ans[0] = lower_bound(nums.begin(), nums.end(), target) - nums.begin(); ans[1] = upper_bound(nums.begin() + ans[0], nums.end(), target) - nums.begin() - 1; if(ans[0] \u0026lt;= ans[1] \u0026amp;\u0026amp; ans[1] \u0026lt; nums.size() \u0026amp;\u0026amp; nums[ans[0]] == target \u0026amp;\u0026amp; nums[ans[1]] == target) { return ans; } return vector\u0026lt;int\u0026gt;{-1, -1}; } }; 搜索旋转排序数组 难度：Medium\n33. 搜索旋转排序数组\n给定变形后的数组$[nums[k], nums[k+1],\u0026hellip;,nums[n-1], nums[0], nums[1],\u0026hellip;, nums[k-1]]$($k$未知)，要求以$\\Theta(log n)$的复杂度查找目标值的下标。\n旋转后的数组只保证了数组的局部是有序的，但是依然可以进行二分查找。\n如果$[l, mid - 1]$是有序数组，且$target$的大小满足$[nums[l], nums[mid])$，那么可以将搜索范围缩小至$[l, mid - 1]$，否则在$[mid + 1, r]$中搜索。\n如果$[mid, r]$是有序数组，且$target$的大小满足$(nums[mid + 1], nums[r]]$，那么可以将搜索范围缩小至$[mid + 1, r]$，否则在$[l, mid - 1]$中搜索。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Solution { public: int search(vector\u0026lt;int\u0026gt;\u0026amp; nums, int target) { int n = nums.size(); int l = 0, r = n - 1; while(l \u0026lt;= r) { int mid = (l + r) \u0026gt;\u0026gt; 1; if(nums[mid] == target) return mid; if(nums[0] \u0026lt;= nums[mid]) { if(nums[0] \u0026lt;= target \u0026amp;\u0026amp; target \u0026lt; nums[mid]) { r = mid - 1; } else { l = mid + 1; } } else { if(nums[mid] \u0026lt; target \u0026amp;\u0026amp; target \u0026lt;= nums[n - 1]) { l = mid + 1; } else { r = mid - 1; } } } return -1; } }; 寻找旋转排序数组中的最小值 难度：Medium\n153. 寻找旋转排序数组中的最小值\n给定两段局部升序的合并数组，以$\\Theta(log n)$查找最小值。跟上一题33. 搜索旋转排序数组很像，但是二分时的情况少了一些，因为只需要找最小值，画个图会比较好。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Solution { public: int findMin(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int l = 0, r = nums.size() - 1; while(l \u0026lt; r) { int mid = (l + r) \u0026gt;\u0026gt; 1; if(nums[mid] \u0026lt; nums[r]) { // 最小值在[l, mid]中 r = mid; } else { // 最小值在(mid, r]中 l = mid + 1; } } return nums[r]; // or nums[l] } }; 寻找两个正序数组的中位数 难度：Hard\n4. 寻找两个正序数组的中位数\n给定两个大小分别为$m$和$n$的正序数组，以$\\Theta(log(m+n))$返回两个正序数组的中位数。（要求时间复杂度为$\\Theta(log(m+n))$）\n难，不会写\u0026hellip;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 class Solution { public: int getKthElement(const vector\u0026lt;int\u0026gt;\u0026amp; nums1, const vector\u0026lt;int\u0026gt;\u0026amp; nums2, int k) { int m = nums1.size(); int n = nums2.size(); int index1 = 0, index2 = 0; while (true) { // 边界情况 if (index1 == m) { return nums2[index2 + k - 1]; } if (index2 == n) { return nums1[index1 + k - 1]; } if (k == 1) { return min(nums1[index1], nums2[index2]); } // 正常情况 int newIndex1 = min(index1 + k / 2 - 1, m - 1); int newIndex2 = min(index2 + k / 2 - 1, n - 1); int pivot1 = nums1[newIndex1]; int pivot2 = nums2[newIndex2]; if (pivot1 \u0026lt;= pivot2) { k -= newIndex1 - index1 + 1; index1 = newIndex1 + 1; } else { k -= newIndex2 - index2 + 1; index2 = newIndex2 + 1; } } } double findMedianSortedArrays(vector\u0026lt;int\u0026gt;\u0026amp; nums1, vector\u0026lt;int\u0026gt;\u0026amp; nums2) { int totalLength = nums1.size() + nums2.size(); if (totalLength % 2 == 1) { return getKthElement(nums1, nums2, (totalLength + 1) / 2); } else { return (getKthElement(nums1, nums2, totalLength / 2) + getKthElement(nums1, nums2, totalLength / 2 + 1)) / 2.0; } } }; 栈 有效的括号 难度：Easy\n20. 有效的括号\n给定只含(){}[]的字符串，判断是否有效。\n使用栈，边塞边判断，当前如果是)}]时就和栈顶元素判断是否匹配。代码值得学习。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Solution { public: bool isValid(string s) { int n = s.size(); if(n % 2 == 1) return false; unordered_map\u0026lt;char, char\u0026gt; pairs = { {\u0026#39;)\u0026#39;, \u0026#39;(\u0026#39;}, {\u0026#39;}\u0026#39;, \u0026#39;{\u0026#39;}, {\u0026#39;]\u0026#39;, \u0026#39;[\u0026#39;} }; stack\u0026lt;char\u0026gt; str; for(const char \u0026amp;ch : s) { if(pairs.count(ch)) { if(str.empty() || str.top() != pairs[ch]) return false; str.pop(); } else { str.push(ch); } } return str.empty(); } }; 最小栈 难度：Medium\n155. 最小栈\n设计一个支持push，pop，top操作，并能在常熟时间内检索到最小元素的栈。\n使用一个辅助栈，当元素入栈时将当前栈的最小值存储起来。很新颖的题，看完题解后发现很简单。核心在于元素入栈的时候，让辅助栈入栈当前最小值（min(min_stack.top(), val)。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 class MinStack { stack\u0026lt;int\u0026gt; x_stack; stack\u0026lt;int\u0026gt; min_stack; public: MinStack() { min_stack.push(INT_MAX); } void push(int val) { x_stack.push(val); min_stack.push(min(min_stack.top(), val)); } void pop() { x_stack.pop(); min_stack.pop(); } int top() { return x_stack.top(); } int getMin() { return min_stack.top(); } }; 字符串解码 难度：Medium\n394. 字符串解码\n对给定字符串进行解码，编码规则为k[encoded_string]，表示方括号内部的字符串正好重复k次（k==0时省略k）。\n如果不存在嵌套编码的话，简单模拟一下即可，但是只能通过大概一半的样例。由于存在内嵌套括号，需要从内向外生成拼接字符串（类似乘法分配律），因此需要用到栈。正解是使用两个辅助栈，遇到[时，将当前的数字和字符串都入栈并清零，遇到]时将当前的数字栈栈顶元素个数的字符串加入字符串栈顶（有点绕），然后同时出栈一个元素。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Solution { public: string decodeString(string s) { string ans = \u0026#34;\u0026#34;; stack\u0026lt;int\u0026gt; nums; stack\u0026lt;string\u0026gt; strs; int num = 0; for(int i = 0; i \u0026lt; s.size(); ++i) { if(s[i] \u0026gt;= \u0026#39;0\u0026#39; \u0026amp;\u0026amp; s[i] \u0026lt;= \u0026#39;9\u0026#39;) { num = num * 10 + s[i] - \u0026#39;0\u0026#39;; } else if((s[i] \u0026gt;= \u0026#39;a\u0026#39; \u0026amp;\u0026amp; s[i] \u0026lt;= \u0026#39;z\u0026#39;) || (s[i] \u0026gt;= \u0026#39;A\u0026#39; \u0026amp;\u0026amp; s[i] \u0026lt;= \u0026#39;Z\u0026#39;)) { ans = ans + s[i]; } else if(s[i] == \u0026#39;[\u0026#39;) { // 将[前的数字压入nums栈内，字母压入strs栈内-\u0026gt;暂存 nums.push(num); num = 0; strs.push(ans); ans = \u0026#34;\u0026#34;; } else if(s[i] == \u0026#39;]\u0026#39;) { // 与前一个[匹配 for(int j = 0; j \u0026lt; nums.top(); ++j) strs.top() += ans; // 很妙的一行 ans = strs.top(); nums.pop(); strs.pop(); } } return ans; } }; 每日温度（栈） 难度：Medium\n739. 每日温度\n给定一个气温数组，求每个气温遇到下一个更高气温的距离。暴力解法是$\\Theta(n^2)$，会TLE，明显会大量重复遍历，考虑一些“记忆化”手段。\n递减栈 用一个stack（存储索引），如果栈空则直接入栈，若栈非空，且大于栈顶索引的元素时（说明找到了下一个更高的气温），就可以通过索引差计算距离并stack.pop()。\n只需要遍历一次数组，$\\Theta(n)$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Solution { public: vector\u0026lt;int\u0026gt; dailyTemperatures(vector\u0026lt;int\u0026gt;\u0026amp; temperatures) { vector\u0026lt;int\u0026gt; ans(temperatures.size(), 0); stack\u0026lt;int\u0026gt; st; for(int i = 0; i \u0026lt; temperatures.size(); ++i) { while(!st.empty() \u0026amp;\u0026amp; temperatures[i] \u0026gt; temperatures[st.top()]) { auto t = st.top(); st.pop(); ans[t] = i - t; } st.push(i); } return ans; } }; 柱状图中最大的矩形 难度：Hard\n84. 柱状图中最大的矩形\n给定n个非负整数作为柱状图高度，宽度为1，求最大矩形面积。（题面感觉和接雨水很像）\n单调（递减）栈，\n性质：栈内的元素是递增的，即1）当元素出栈时，说明新元素是出栈元素向后找的第一个更小的元素；2）当元素出栈后，说明栈顶元素是出栈元素向前找的第一个更小元素\n模板：\n1 2 3 4 5 stack\u0026lt;int\u0026gt; st; for(int i = 0; i \u0026lt; nums.size(); ++i) { while(!st.empty() \u0026amp;\u0026amp; st.top() \u0026gt; nums[i]) st.pop(); st.push(nums[i]); } 对于一个高度，如果能得到向左和向右的边界，那么就能对每个高度求一次面积，遍历所有高度即可得出最大面积。使用单调栈，在出栈时得到前后（自己）边界并计算面积。代码值得学习与深思。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public: int largestRectangleArea(vector\u0026lt;int\u0026gt;\u0026amp; heights) { stack\u0026lt;int\u0026gt; st; // 存储索引 int ans = heights[0]; heights.push_back(0); // 避免输入递增的情况 for(int i = 0; i \u0026lt; heights.size(); ++i) { // 遇到了更矮的柱子，就一直出栈直到碰到了较高的 while(!st.empty() \u0026amp;\u0026amp; heights[st.top()] \u0026gt;= heights[i]) { int h = heights[st.top()]; st.pop(); if(st.empty()) ans = max(ans, i * h); // 前面所有柱子都更高 else ans = max(ans, (i - st.top() - 1) * h); // 宽度为和栈顶索引值的距离 } st.push(i); } return ans; } }; 堆 数组中的第K个最大元素（排序） 难度：Medium\n215. 数组中的第K个最大元素\n顾名思义，用algorithm库的快排，两行代码秒了\u0026hellip;\n1 2 3 4 5 6 7 class Solution { public: int findKthLargest(vector\u0026lt;int\u0026gt;\u0026amp; nums, int k) { sort(nums.begin(), nums.end()); return nums[nums.size() - k]; } }; 手搓快排（重要）：\n1 2 3 4 5 6 7 8 9 10 11 int quickselect(vector\u0026lt;int\u0026gt; \u0026amp;nums, int l, int r, int k) { if (l == r) return nums[k]; int partition = nums[l], i = l - 1, j = r + 1; while (i \u0026lt; j) { do i++; while (nums[i] \u0026lt; partition); do j--; while (nums[j] \u0026gt; partition); if (i \u0026lt; j) swap(nums[i], nums[j]); } if (k \u0026lt;= j)return quickselect(nums, l, j, k); else return quickselect(nums, j + 1, r, k); } 前K个高频元素 难度：Medium\n347. 前K个高频元素\n给定整数数组和整数k，按任意顺序返回出现频率前k高的元素（要求时间复杂度优于$\\Theta(nlogn)$）。\n用数组记录出现频次然后排序，但是复杂度会达到$\\Theta(nlogn)$。因此可以设计一个小顶堆（用优先队列实现，代码值得学习），然后遍历出现频次数组：\n如果堆的元素小于k，则直接插入堆中\n如果堆的元素大于k，则检查堆顶与当前出现次数的大小。如果堆顶更大，说明至少有k个数字的出现次数比当前值大，故舍弃；否则就弹出堆顶，将当前值插入。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Solution { public: vector\u0026lt;int\u0026gt; topKFrequent(vector\u0026lt;int\u0026gt;\u0026amp; nums, int k) { unordered_map\u0026lt;int, int\u0026gt; freq; for(const int\u0026amp; x : nums) ++freq[x]; /* 自定义比较方式 */ struct cmp{ bool operator()(pair\u0026lt;int, int\u0026gt;\u0026amp; x, pair\u0026lt;int, int\u0026gt;\u0026amp; y) { return x.second \u0026gt; y.second; } }; priority_queue\u0026lt;pair\u0026lt;int, int\u0026gt;, vector\u0026lt;pair\u0026lt;int, int\u0026gt;\u0026gt;, cmp\u0026gt; q; for(auto \u0026amp; x : freq) { q.push(x); if(q.size() \u0026gt; k) q.pop(); // 小顶堆: 栈顶元素一定是最小的\u0026gt; } vector\u0026lt;int\u0026gt; ans; while(!q.empty()) { ans.emplace_back(q.top().first); q.pop(); } return ans; } }; 数据流的中位数 难度：Hard\n实现MedianFinder类：\nMedianFinder()初始化MedianFinder对象\nvoid addNum(int num)将数据流中的整数num添加到数据结构中\ndouble findMedian()返回到目前位置所有元素的中位数。与实际答案相差$10^{-5}$以内的答案将被接受。\n初见能想到（部分）正解思路（用一个大顶堆和一个小顶堆），不过没敢写。用两个优先队列maxHeap和minHeap分别记录小于中位数的数和大于等于中位数的数。当累计添加的数的数量为奇数时，minHeap中的数比maxHeap多一个，此时中位数为minHeap的堆顶。代码还得再看看。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 class MedianFinder { priority_queue\u0026lt;int, vector\u0026lt;int\u0026gt;, less\u0026lt;int\u0026gt;\u0026gt; minHeap; priority_queue\u0026lt;int, vector\u0026lt;int\u0026gt;, greater\u0026lt;int\u0026gt;\u0026gt; maxHeap; public: MedianFinder() { } void addNum(int num) { // 小于中位数，中位数将变小 if(minHeap.empty() || num \u0026lt;= minHeap.top()) { minHeap.push(num); if(maxHeap.size() + 1 \u0026lt; minHeap.size()) { maxHeap.push(minHeap.top()); minHeap.pop(); } } else { // 大于中位数，中位数将变大 maxHeap.push(num); if(maxHeap.size() \u0026gt; minHeap.size()) { minHeap.push(maxHeap.top()); maxHeap.pop(); } } } double findMedian() { if(minHeap.size() \u0026gt; maxHeap.size()) return minHeap.top(); return (minHeap.top() + maxHeap.top()) / 2.0; } }; 贪心算法 买卖股票的最佳时机 难度：Easy\n121. 买卖股票的最佳时机\n给定一段时间的股价，选择某一天购买并在某一天卖出，求最大利润。\n可以把问题转换成对于$(i, j)$，求$max(prices[j] - prices[i]), j \u0026gt; i$。\n暴力$\\Theta(n^2)$显然是不行的，需要只遍历一轮的解法。从第一天遍历到最后一天，如果遇到了更低的股价就更新_min（记录最低点），假设在这一天买入，那么往后遍历的时候，第i填的利润就是prices[i] - _min。\n1 2 3 4 5 6 7 8 9 10 11 12 class Solution { public: int maxProfit(vector\u0026lt;int\u0026gt;\u0026amp; prices) { int inf = 0x3f3f3f3f; int _min = inf, ans = 0; for(int price : prices) { ans = max(ans, price - _min); _min = min(price, _min); } return ans; } }; 跳跃游戏 难度：Medium\n55. 跳跃游戏\n给定非负整数数组，你位于数组的第一个下标，每个元素代表在该下标可以移动的最大长度，判断是否能到达最后一个下标。\n太久没写题了，看了一点题解就豁然开朗了。显然要找出一种$\\Theta(n)$的解法。可以在遍历时维护一个变量max_len表示目前可达的最远位置，那么对于每个元素要么不更新，要么用更大的index + step[i]更新max_len，只要max_len能覆盖到数组尾即通过。\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { public: bool canJump(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int max_len = 0; bool ans = false; for(int i = 0; i \u0026lt; nums.size(); ++i) { if(i \u0026gt; max_len) break; if(i == nums.size() - 1) ans = true; max_len = max(max_len, i + nums[i]); } return ans; } }; 跳跃游戏 II 难度：Medium\n45. 跳跃游戏II\n在上一题的基础上，最远不会跳过最后一个下标，求到达最后一个下标的最小跳跃次数。不难，维护一个数组ans[]表示移动到每个下标的最小跳跃次数（ans[0] = 0, ans[1:] = inf），对于每个元素，将能移动到的下标要么不更新，要么更新为更小的ans[i] + 1。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Solution { public: int jump(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int n = nums.size(); vector\u0026lt;int\u0026gt; ans(n, 0x3f3f3f3f); ans[0] = 0; for(int i = 0; i \u0026lt; n; ++i) { for(int j = 1; j \u0026lt;= nums[i] \u0026amp;\u0026amp; i + j \u0026lt; n; ++j) { ans[i + j] = min(ans[i + j], ans[i] + 1); } } return ans[n - 1]; } }; 划分字母区间 763. 划分字母区间\n给定字符串，顺序切分成多个字符串，每个字符串内出现过的字母不允许出现在其他字符串。 （如\u0026quot;ababcc\u0026quot;切分成[\u0026ldquo;abab\u0026rdquo;, \u0026ldquo;cc\u0026rdquo;]）\n值得学习一下题解。给出一种时间复杂度为$\\Theta(n)$，空间复杂度为$\\Theta(1)$的解法。预处理很重要，需要遍历字符串，得到每个字母最后一次出现的下标位置（last[s[i] - 'a'] = i）。接下来用start和end两个变量限定分割字符串的范围，对于当前的字符串，start不动，end为s[start]字母最后一次出现的下标，同时需要保证中间的字母都在[start, end]内，否则继续扩大end（贪心）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public: vector\u0026lt;int\u0026gt; partitionLabels(string s) { int last[26]; int n = s.size(); vector\u0026lt;int\u0026gt; ans; for(int i = 0; i \u0026lt; n; ++i) last[s[i] - \u0026#39;a\u0026#39;] = i; int start = 0, end = 0; for(int i = 0; i \u0026lt; n; ++i) { end = max(end, last[s[i] - \u0026#39;a\u0026#39;]); if(i == end) { ans.push_back(end - start + 1); start = end + 1; } } return ans; } }; 动态规划 最大正方形（DP） 难度：Medium\n221. 最大正方形（此题不在当前hot 100中）\nDP 显然暴力法会重复遍历很多元素，即使是dfs也是如此。\n以dp(i, j)表示以(i, j)为右下角且只包含1的正方形的边长最大值，接下来考虑转移方程。matrix[i][j] == 0时的转移方程显然是dp[i][j] = 0；matrix[i][j] == 1时且边界安全时，则dp[i][j]的值由左、上、左上元素的最小值决定，简单来说是=min(左, 上, 左上)+1。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Solution { public: int maximalSquare(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; matrix) { if(matrix.size() == 0 || matrix[0].size() == 0) return 0; vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp; dp.resize(matrix.size()); for(int i = 0; i \u0026lt; matrix.size(); ++i) { dp[i].resize(matrix[0].size(), 0); } int ans = 0; for(int i = 0; i \u0026lt; matrix.size(); ++i) { for(int j = 0; j \u0026lt; matrix[0].size(); ++j) { if(matrix[i][j] == \u0026#39;1\u0026#39;) { if(i == 0 || j == 0) { dp[i][j] = 1; } else { dp[i][j] = min(min(dp[i - 1][j], dp[i][j - 1]), dp[i - 1][j - 1]) + 1; } } ans = max(ans, dp[i][j]); } } return ans * ans; // return square } }; 爬楼梯 难度：Easy\n70. 爬楼梯\n入门板子题。\n1 2 3 4 5 6 7 8 9 10 11 12 class Solution { public: int climbStairs(int n) { int a[50]; a[1] = 1; a[2] = 2; for(int i = 3; i \u0026lt;= n; ++i) { a[i] = a[i - 1] + a[i - 2]; } return a[n]; } }; 杨辉三角 难度：Easy\n118. 杨辉三角\n杨辉三角，把样例写成左对齐就很容易发现规律：\n1 2 3 4 5 [1] [1, 1] [1, 2, 1] [1, 3, 3, 1] [1, 4, 6, 4, 1] 即状态转移公式$ans[i][j] = ans[i - 1][j - 1] + ans[i - 1][j]$\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; generate(int numRows) { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; ans(numRows); for(int i = 0; i \u0026lt; numRows; ++i) { ans[i].resize(i + 1, 1); for(int j = 1; j \u0026lt; i; ++j) { ans[i][j] = ans[i - 1][j - 1] + ans[i - 1][j]; } } return ans; } }; 打家劫舍 难度：Medium\n208. 打家劫舍\n偷n个房子，但是不能偷相邻的，求最高金额。\ndp解题步骤：\n定义子问题 可以将问题规模缩小，“从前$k$个房子中偷到的最大金额”，表示为$f(k)$。\n子问题的递推关系（最优子结构） $f(k)$可以由$f(k-1)$和$f(k-2)$递推而来，偷$k$个房子有两种方法：\n在前$k-1$个房子得到了最大值，那么第$k$个房子不偷\n偷前$k-2$个房子和第$k$个房子得到了最大值\n得到递推关系——$dp[k] = max(dp[k - 1], dp[k - 2] + nums[k])$。这个情况覆盖了可能让$f(k)$达到最大值的所有情况。\n确定dp数组的计算顺序\n空间优化（optional）\n计算$f(k)$时只用到了$f(k-1)$和$f(k-2)$，那么不需要用一维数组存储，用两个变量即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Solution { public: int rob(vector\u0026lt;int\u0026gt;\u0026amp; nums) { if(!nums.size()) return 0; vector\u0026lt;int\u0026gt; dp(nums.size() + 1, 0); dp[0] = 0; dp[1] = nums[0]; for(int k = 2; k \u0026lt;= nums.size(); ++k) { dp[k] = max(dp[k - 1], nums[k - 1] + dp[k - 2]); } return dp[nums.size()]; } }; 完全平方数 难度：Medium\n279. 完全平方数\n给定整数n，返回和为n的完全平方数的最少数量（如13=4+9）\n状态转移方程比较特别，用$f[i]$表示最少需要多少个数的平方和来表示$i$。枚举这些数（$[1, \\sqrt(i)]$），假设当前枚举到$j$，那么还需要取若干数的平方，构成$i-j^2$。（其实就是cost=1的完全背包问题）\n$$f[i] = 1 + \\Sigma^{\\sqrt(i)}_{j=1}min(f[i-j^2])$$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Solution { public: int numSquares(int n) { vector\u0026lt;int\u0026gt; f(n + 1); for(int i = 1; i \u0026lt;= n; ++i) { int temp = 0x3f3f3f; for(int j = 1; j * j \u0026lt;= i; ++j) { temp = min(temp, f[i - j * j]); } f[i] = temp + 1; } return f[n]; } }; 零钱兑换 难度：Medium\n322. 零钱兑换\n给定整数硬币数组（不限量），求凑出给定整数的最少个数。\n跟上一题（完全平方数）很像，用$f[i]$表示凑成$i$所需的最少个数，只不过状态转移量从$j^2$变成了$coins[j]$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Solution { public: int coinChange(vector\u0026lt;int\u0026gt;\u0026amp; coins, int amount) { if(amount == 0) return 0; vector\u0026lt;int\u0026gt; dp(amount + 1, 0x3f3f3f); dp[0] = 0; for(int i = 1; i \u0026lt;= amount; ++i) { for(int j = 0; j \u0026lt; coins.size(); ++j) { if(i - coins[j] \u0026gt;= 0) dp[i] = min(1 + dp[i - coins[j]], dp[i]); } } return dp[amount] == 0x3f3f3f? -1 : dp[amount]; } }; 单词拆分 难度：Medium\n139. 单词拆分\n给定字符串和字符串列表作为字典，如果可以利用字典中出现的一个或多个单词拼接出字符串则返回true。\n$dp[i] (i \\in [0, n])$表示前$i$位是否可以用wordDict中的单词表示，并初始化$dp[0] = true$。遍历字符串的所有子串，对于区间$[l, r]$，如果$dp[l] = true$即$l$左边的部分可以用字典标识，且s.substr(l, r - l)的部分出现在字典中时，那么$r$左边的部分都可以用字典表示，因此$dp[r] = true$，即状态转移方程$dp[r] = dp[l] \u0026amp;\u0026amp; check(l, r)$的\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public: bool wordBreak(string s, vector\u0026lt;string\u0026gt;\u0026amp; wordDict) { int n = s.size(); unordered_set\u0026lt;string\u0026gt; words(wordDict.begin(), wordDict.end()); vector\u0026lt;bool\u0026gt; dp(n + 1, false); dp[0] = true; for(int r = 1; r \u0026lt;= n; ++r) { for(int l = 0; l \u0026lt; r; ++l) { if(dp[l] \u0026amp;\u0026amp; words.find(s.substr(l, r - l)) != words.end()) { dp[r] = true; break; } } } return dp[n]; } }; 最长递增子序列（LIS） 难度：Medium\n300. 最长递增子序列\n给定整数数组，找到最长严格递增子序列的长度。\n$dp[i]$表示只考虑前$i$个元素，以第$i$个数字结尾的LIS长度（**nums[i]**必须被选取）。状态转移方程为$dp[i] = max(dp[j]) + 1$。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Solution { public: int lengthOfLIS(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int n = nums.size(); if(!n) return 0; vector\u0026lt;int\u0026gt; dp(n, 0); for(int i = 0; i \u0026lt; n; ++i) { dp[i] = 1; for(int j = 0; j \u0026lt; i; ++j) { if(nums[j] \u0026lt; nums[i]) dp[i] = max(dp[i], dp[j] + 1); } } return *max_element(dp.begin(), dp.end()); } }; 乘积最大子数组 难度：Medium\n152. 乘积最大子数组\n给定整数数组，找出数组中乘积最大的非空连续子数组，并返回乘积。\n跟上一题类似地能写出来，但是有bug，因为没有正确地考虑序列的整体正负情况！\n1 2 3 4 5 6 7 8 9 10 11 class Solution { public: int maxProduct(vector\u0026lt;int\u0026gt;\u0026amp; nums) { vector\u0026lt;long\u0026gt; maxF(nums.begin(), nums.end()), minF(nums.begin(), nums.end()); for(int i = 1; i \u0026lt; nums.size(); ++i) { maxF[i] = max(maxF[i - 1] * nums[i], max((long)nums[i], minF[i - 1] * nums[i])); minF[i] = min(minF[i - 1] * nums[i], min((long)nums[i], maxF[i - 1] * nums[i])); } return *max_element(maxF.begin(), maxF.end()); } }; 分割等和子集 难度：Medium\n416. 分割等和子集\n给定一个只包含正整数的非空数组，判断是否将这个数组分割成两个子集，使得两个子集的元素和相等。\n完全没有思路的一题。题面等价于“给定数组判断是否可以选出一些数字使和等于整个数组元素和的一半”（0-1背包问题）。限制条件不在于重量不超背包总容量，而是和恰好等于元素和的一半。\n有三种情况可以直接排除：1）n % 2 == 1；2）sum % 2 == 1；3）$maxElement \u0026gt; \\frac{sum}{2}$。\n创建$n$行$\\frac{sum}{2} + 1$列的二维数组，$dp[i][j]$表示从下标$[0, i]$范围内选取若干个正整数是否存在和等于$j$。初始化：$dp[i][0] = true$；$dp[0][nums[0]] = true$（当$i == 0$时，只有$nums[0]$可选）。\n接下来考虑状态转移方程\n如果$j \\ge nums[i]$，那么$nums[i]$可选可不选，考虑以下两种情况：1）如果不选，那么$dp[i][j] = dp[i - 1][j]$；如果选取，那么$dp[i][j] = dp[i - 1][j - nums[i]]$。\n如果$j \\lt nums[i]$，那么无法选取，$dp[i][j] = dp[i - 1][j]$。\n综上，得到状态转移方程：\n$$dp[i][j]=\\begin{cases}dp[i-1][j] || dp[i-1][j-nums[i]]\\ (j \\ge nums[i])\\\\dp[i-1][j]\\ (j \\lt nums[i])\\end{cases}$$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class Solution { public: bool canPartition(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int n = nums.size(); /* 排除三种情况 */ if(n \u0026lt; 2) return false; // 1.1 int sum = accumulate(nums.begin(), nums.end(), 0); int maxNum = *max_element(nums.begin(), nums.end()); if(sum \u0026amp; 1) return false; // 1.2 int target = sum / 2; if(maxNum \u0026gt; target) return false; // 1.3 vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(n, vector\u0026lt;int\u0026gt;(target + 1, 0)); /* 初始化 */ for(int i = 0; i \u0026lt; n; ++i) dp[i][0];// 2.1 dp[0][nums[0]] = true; // 2.2 /* 动态规划 */ for(int i = 1; i \u0026lt; n; ++i) { int num = nums[i]; for(int j = 1; j \u0026lt;= target; ++j) { if(j \u0026gt;= num) { dp[i][j] = dp[i - 1][j] || dp[i - 1][j - num]; } else { dp[i][j] = dp[i - 1][j]; } } } return dp[n - 1][target]; } }; 最长有效括号 难度：Hard\n32. 最长有效括号\n给定值包含(和)的字符串，找出最长有效括号子串的长度。\nSolution 1（栈）：始终保持栈底元素为当前已遍历过的元素中最后一个没有被匹配的右括号的下标，栈里其他元素维护左括号下标：\n遇到(则将下标放入栈中\n遇到)则需要出栈并考虑两种情况：1）如果栈空，那么当前)为没有被匹配的)，将下标入栈；2）如果栈不空，i - stk.top()表示以该右括号结尾的最长有效括号长度\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public: int longestValidParentheses(string s) { int ans = 0; stack\u0026lt;int\u0026gt; stk; stk.push(-1); for(int i = 0; i \u0026lt; s.size(); ++i) { if(s[i] == \u0026#39;(\u0026#39;) { stk.push(i); } else if(s[i] == \u0026#39;)\u0026#39;) { stk.pop(); if(stk.empty()) stk.push(i); else ans = max(ans, i - stk.top()); } } return ans; } }; Solution 2（DP）：定义$dp[i]$表示以下标$i$结尾的最长有效括号长度。显然有效的子串一定以)结尾，因此以(结尾的子串对应$dp$值为0。从前往后遍历字符串求解$dp$，每两字符检查一次：\n形如...()，那么$dp[i] = dp[i - 2] + 2$\n形如...))，那么$dp[i] = dp[i - 1] + dp[i - dp[i - 1] - 2] + 2$（$i$位置匹配的左括号在$i - dp[i - 1] - 1$，这里有点懵\u0026hellip;）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # 不会但是抄一下官解 class Solution { public: int longestValidParentheses(string s) { int ans = 0, n = s.size(); vector\u0026lt;int\u0026gt; dp(n, 0); for(int i = 1; i \u0026lt; n; ++i) { if(s[i] == \u0026#39;)\u0026#39;) { if(s[i - 1] == \u0026#39;(\u0026#39;) { dp[i] = (i \u0026gt;= 2 ? dp[i - 2] : 0) + 2; } else if (i - dp[i - 1] \u0026gt; 0 \u0026amp;\u0026amp; s[i - dp[i - 1] - 1] == \u0026#39;(\u0026#39;) { dp[i] = dp[i - 1] + ((i - dp[i - 1]) \u0026gt;= 2 ? dp[i - dp[i - 1] - 2] : 0) + 2; } ans = max(ans, dp[i]); } } return ans; } }; 多维动态规划 不同路径 难度：Medium\n62. 不同路径\n从网格的左上移动到坐下共有多少条不同路径。\ndp板子题，秒了。可以优化为一维数组（完全背包）：计算$dp[1][1]$时，会使用到$dp[0][1]$和$dp[1][0]$，但是$dp[0][1]$之后就不用了，那就干脆直接把$dp[1][1]$记到$dp[0][1]$中。\n排列组合也能算，但还是算了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Solution { public: int uniquePaths(int m, int n) { int dp[105][105]; for(int i = 0; i \u0026lt; m; ++i) dp[i][0] = 1; for(int i = 1; i \u0026lt; n; ++i) dp[0][i] = 1; for(int i = 1; i \u0026lt; m; ++i) { for(int j = 1; j \u0026lt; n; ++j) { dp[i][j] = dp[i - 1][j] + dp[i][j - 1]; } } return dp[m - 1][n - 1]; } }; 最小路径和 难度：Medium\n64. 最小路径和\n给定包含非负整数的mxn网格，找到一条从左上角到右下角的路径使路径上的数字综合为最小（只能向下或向右移动一步）。\n很标准的写法，注意一下细节即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public: int minPathSum(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid) { int m = grid.size(), n = grid[0].size(); vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(m, vector\u0026lt;int\u0026gt;(n, 0)); dp[0][0] = grid[0][0]; for(int i = 1; i \u0026lt; m; ++i) dp[i][0] = dp[i - 1][0] + grid[i][0]; for(int i = 1; i \u0026lt; n; ++i) dp[0][i] = dp[0][i - 1] + grid[0][i]; for(int i = 1; i \u0026lt; m; ++i) { for(int j = 1; j \u0026lt; n; ++j) { dp[i][j] = grid[i][j] + min(dp[i - 1][j], dp[i][j - 1]); } } return dp[m - 1][n - 1]; } }; 最长回文子串（LPS） 难度：Medium\n5. 最长回文子串\n给定字符串，求最长回文子串。\n定义$P(i, j) = true\\ if\\ S_{i}\u0026hellip;S_{j}是回文串\\ else\\ false$，且如果一个子串是回文串，那么去掉首尾字母仍是回文串。那么得到状态转移方程$P(i, j) = P(i + 1, j - 1) \\wedge (S_{i} == S_{j})$（向外扩展）。同时注意两个初始化条件：1）$P(i, i) = true$；2）$P(i, i + 1) = (S_{i} == S_{j})$。有点难\u0026hellip;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 class Solution { public: string longestPalindrome(string s) { int n = s.size(); if(n \u0026lt; 2) return s; int maxLen = 1, begin = 0; // 记录子串 vector\u0026lt;vector\u0026lt;bool\u0026gt;\u0026gt; dp(n, vector\u0026lt;bool\u0026gt;(n)); for(int i = 0; i \u0026lt; n; ++i) dp[i][i] = true; for(int L = 2; L \u0026lt;= n; ++L) { for(int i = 0; i \u0026lt; n; ++i) { // 长度 int j = L + i - 1; // 右边界 if(j \u0026gt;= n) break; // 越界 if(s[i] != s[j]) dp[i][j] = false; else { if(j - i \u0026lt; 3) dp[i][j] = true; else dp[i][j] = dp[i + 1][j - 1]; } if(dp[i][j] \u0026amp;\u0026amp; j - i + 1 \u0026gt; maxLen) maxLen = j - i + 1, begin = i; } } return s.substr(begin, maxLen); } }; 最长公共子序列（LCS） 给定两个字符串，求最长公共子序列的长度。\n死去的记忆又在攻击我\u0026hellip;2023/06/02 21:58在OneNote写下的LCS笔记依旧。\n定义$dp[i][j]$表示序列$S1[0:i]$和序列$S2[0:j]$的LCS。\n考虑两种情况：1）如果序列S1与S2的最后一个元素相等，则S1与S2的LCS就是S1去尾与S2去尾的LCS加上最后一个元素；2）如果序列S1与S2的最后一个元素不等，那么S1与S2的LCS就是$max(S1去尾与L2的LCS， S2去尾与S1的LCS)$。\n那么状态转移方程如下：\n$$dp[i][j]=\\begin{cases}dp[i-1][j-1] + 1, 尾部相等\\\\max(dp[i-1][j], dp[i][j-1]), 尾部不相等\\end{cases}$$ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Solution { public: int longestCommonSubsequence(string text1, string text2) { int m = text1.size(), n = text2.size(); vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(m + 1, vector\u0026lt;int\u0026gt;(n + 1)); for(int i = 1; i \u0026lt;= m; ++i) { char c1 = text1.at(i - 1); for(int j = 1; j \u0026lt;= n; ++j) { char c2 = text2.at(j - 1); if(c1 == c2) { dp[i][j] = dp[i - 1][j - 1] + 1; } else { dp[i][j] = max(dp[i - 1][j], dp[i][j - 1]); } } } return dp[m][n]; } }; 编辑距离 难度：Medium\n72. 编辑距离\n给定两个单词，求将字符串A转换为字符串B使用的最少操作数（插入、删除、替换）。\n如果A为空，那么编辑距离为字符串B的长度；如果B为空，那么编辑距离为字符串A的长度。因此定义$dp[i][j]$表示A的前$i$个字母和$B$的前$j$个字母之间的编辑距离，具体的状态转移建议直接看代码。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Solution { public: int minDistance(string word1, string word2) { int n = word1.size(), m = word2.size(); if(n * m == 0) return n + m; // 其中一个字符串为空 vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; dp(n + 1, vector\u0026lt;int\u0026gt;(m + 1)); // 边界状态初始化 for(int i = 0; i \u0026lt; n + 1; ++i) dp[i][0] = i; for(int j = 0; j \u0026lt; m + 1; ++j) dp[0][j] = j; for(int i = 1; i \u0026lt; n + 1; ++i) { for(int j = 1; j \u0026lt; m + 1; ++j) { // 三种状态转移方式 int left = dp[i - 1][j] + 1; int down = dp[i][j - 1] + 1; int left_down = dp[i - 1][j - 1]; if(word1[i - 1] != word2[j - 1]) left_down += 1; dp[i][j] = min(left, min(down, left_down)); } } return dp[n][m]; } }; 技巧 只出现一次的数字 难度：Easy\n136. 只出现一次的数字\n给定非空整数数组，除了某个元素只出现一次外，其余每个元素均出现两次，返回只出现一次的元素。（要求时间复杂度$\\Theta(N)$，空间复杂度$\\Theta(1)$）\n利用位运算的异或，对于任意整数有$x\\oplus x = 0$。因此，最后留下的结果就是出现一次的数字。\n$$a\\oplus a\\oplus b\\oplus b\\oplus c=c$$ 1 2 3 4 5 6 7 8 class Solution { public: int singleNumber(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int result = 0; for(auto \u0026amp; x : nums) result ^= x; return result; } }; 多数元素 难度：Easy\n169. 多数元素\n给定数组，返回其中的多数元素（出现次数大于$\\lfloor n\\rfloor$）。\n很简单，排完序肯定在中间。题解里有各式奇奇怪怪的解法，属于是小题大做了。\n1 2 3 4 5 6 7 class Solution { public: int majorityElement(vector\u0026lt;int\u0026gt;\u0026amp; nums) { sort(nums.begin(), nums.end()); return nums[nums.size() / 2]; } }; 颜色分类 难度：Medium\n75. 颜色分类\n排序但不能sort。桶一下就好了。\n1 2 3 4 5 6 7 8 9 10 class Solution { public: void sortColors(vector\u0026lt;int\u0026gt;\u0026amp; nums) { vector\u0026lt;int\u0026gt; color(3); for(int \u0026amp; x : nums) ++color[x]; for(int i = 0; i \u0026lt; color[0]; ++i) nums[i] = 0; for(int i = color[0]; i \u0026lt; color[0] + color[1]; ++i) nums[i] = 1; for(int i = color[0] + color[1]; i \u0026lt; color[0] + color[1] + color[2]; ++i) nums[i] = 2; } }; 下一个排列 难度：Medium\n31. 下一个排列\n将数组原地修改到下一个排列，用STL可秒。字节一面题，还是了解一下正攻解法吧。\nSTL： 1 2 3 4 5 6 class Solution { public: void nextPermutation(vector\u0026lt;int\u0026gt;\u0026amp; nums) { next_permutation(nums.begin(), nums.end()); } }; 正攻： 分为三步：\n从右向左找第一个数字$x$满足右边有$\\lt x$的数。\n找$x$右边最小的大于$x$的数$y$（注意一个性质，$x$右边是单调递减的），交换$x$和$y$。\n交换后，$y$右边是单调递减的，需要转成单调递增（reverse即可，不需要排序）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 class Solution { public: void nextPermutation(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int n = nums.size(); // 从右向左找第一个小于右侧相邻数字的元素 int i = n - 2; while (i \u0026gt;= 0 \u0026amp;\u0026amp; nums[i] \u0026gt;= nums[i + 1]) --i; // 如果没找到就跳过 if (i \u0026gt;= 0) { // 从右向左找第一个 int j = n - 1; while(nums[j] \u0026lt;= nums[i]) --j; swap(nums[i], nums[j]); } // 反转nums[i + 1:] reverse(nums.begin() + i + 1, nums.end()); } }; 寻找重复数 难度：Medium\n287. 寻找重复数\n给定长度为$n+1$的数组，数字范围在$[1,n]$内，有且只有一个重复的整数，返回这个重复的整数。（要求不修改原数组，空间复杂度为$\\Theta(1)$）\n很难，前两种题解都看不懂\u0026hellip;解法是快慢指针（Floyd判圈法），对nums数组建图$i \\rightarrow nums[i]$，由于有重复的nums[i]，因此必然有两个i指向相同的nums[i]值（存在环路）。设置快慢指针，慢指针走一步，快指针走两步（fast = nums[nums[fast]]），一定会相遇。此时将慢指针放置起点0，快慢指针每次同时移动一步就一定会相遇。（还不太理解）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 class Solution { public: int findDuplicate(vector\u0026lt;int\u0026gt;\u0026amp; nums) { int slow = 0, fast = 0; do{ slow = nums[slow]; fast = nums[nums[fast]]; } while(slow != fast); slow = 0; while(slow != fast) { slow = nums[slow]; fast = nums[fast]; } return slow; } }; ","date":"2025-05-22T23:58:00+08:00","image":"https://kaigezheng.github.io/p/leetcode/img/cover_hu_faad6b6a11992ab4.png","permalink":"https://kaigezheng.github.io/p/leetcode/","title":"LeetCode刷题记录"}]